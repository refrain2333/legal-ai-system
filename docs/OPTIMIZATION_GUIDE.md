# æ³•å¾‹æ£€ç´¢ç³»ç»Ÿä¼˜åŒ–æŠ€æœ¯æ–‡æ¡£

> **ç‰ˆæœ¬**: v1.0  
> **æ—¥æœŸ**: 2025-09-06  
> **ç›®æ ‡**: åŸºäºç°æœ‰æ•°æ®é›†è¿›è¡Œç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å’ŒåŠŸèƒ½å¢å¼º

## ğŸ“‹ ä¼˜åŒ–æ¦‚è¿°

### å½“å‰ç³»ç»ŸçŠ¶æ€
- **æ–‡æ¡£æ•°é‡**: 3,519ä¸ª (æ³•æ¡2,729 + æ¡ˆä¾‹790)
- **å¹³å‡ç›¸å…³åº¦**: 0.6802
- **æˆåŠŸç‡**: 100%
- **å“åº”æ—¶é—´**: 51.1ms
- **è´¨é‡è¯„çº§**: B+çº§ (è‰¯å¥½åä¼˜ç§€)

### ä¼˜åŒ–ç›®æ ‡
- **ç›¸å…³åº¦æå‡**: 0.68 â†’ 0.75+ (ç›®æ ‡æå‡10%)
- **åŠŸèƒ½å¢å¼º**: å¢åŠ å¯è§£é‡Šæ€§å’Œå…³è”åˆ†æ
- **ç”¨æˆ·ä½“éªŒ**: æä¾›æ›´æ™ºèƒ½çš„æ£€ç´¢ä½“éªŒ

### æ•°æ®èµ„æºä¼˜åŠ¿
- **ç²¾ç¡®æ˜ å°„è¡¨**: 668ä¸ªç²¾ç¡®æ¡ˆä¾‹-æ³•æ¡å¯¹åº”å…³ç³»
- **æ¨¡ç³Šæ˜ å°„è¡¨**: 1,364ä¸ªè¯­ä¹‰å…³è”å…³ç³»
- **å®Œæ•´æ³•å¾‹æ–‡æ¡£**: è¦†ç›–å¤šä¸ªæ³•å¾‹é¢†åŸŸ

---

## ğŸ¯ ä¼˜åŒ–æŠ€æœ¯è·¯çº¿å›¾

### é˜¶æ®µä¸€: æ··åˆæ’åºå¼•æ“ (ä¼˜å…ˆçº§:ğŸ”¥ğŸ”¥ğŸ”¥)
**ç›®æ ‡**: ç›´æ¥æå‡æ£€ç´¢å‡†ç¡®åº¦  
**é¢„æœŸæ•ˆæœ**: ç›¸å…³åº¦ 0.68 â†’ 0.75+  
**å®æ–½æ—¶é—´**: 1-2å¤©  
**æŠ€æœ¯éš¾åº¦**: â­â­

### é˜¶æ®µäºŒ: çŸ¥è¯†å›¾è°±å…³è”åˆ†æ (ä¼˜å…ˆçº§:ğŸ”¥ğŸ”¥)
**ç›®æ ‡**: å¢åŠ å¯è§£é‡Šæ€§å’Œå…³è”æ¨è  
**é¢„æœŸæ•ˆæœ**: ç”¨æˆ·ä½“éªŒæ˜¾è‘—æå‡  
**å®æ–½æ—¶é—´**: 2-3å¤©  
**æŠ€æœ¯éš¾åº¦**: â­â­â­

### é˜¶æ®µä¸‰: å¯¹æ¯”å­¦ä¹ æ¨¡å‹ä¼˜åŒ– (ä¼˜å…ˆçº§:ğŸ”¥)
**ç›®æ ‡**: æ¨¡å‹ä¸“ä¸šåŒ–è®­ç»ƒ  
**é¢„æœŸæ•ˆæœ**: ç›¸å…³åº¦è´¨çš„é£è·ƒ  
**å®æ–½æ—¶é—´**: 3-5å¤©  
**æŠ€æœ¯éš¾åº¦**: â­â­â­â­

---

## ğŸ”§ é˜¶æ®µä¸€: æ··åˆæ’åºå¼•æ“å®ç°

### 1.1 æŠ€æœ¯åŸç†

ç°æœ‰ç³»ç»Ÿåªä½¿ç”¨è¯­ä¹‰æ£€ç´¢ï¼Œæ–°çš„æ··åˆæ’åºå¼•æ“å°†ç»“åˆä¸‰ç§ä¿¡å·ï¼š
1. **è¯­ä¹‰ç›¸ä¼¼åº¦** (70%æƒé‡) - ç°æœ‰åŠŸèƒ½
2. **æ˜ å°„å…³ç³»åŠ æƒ** (20%æƒé‡) - åŸºäºç²¾ç¡®æ˜ å°„è¡¨
3. **å…³é”®è¯åŒ¹é…** (10%æƒé‡) - ä¼ ç»Ÿæ–‡æœ¬åŒ¹é…

### 1.2 å®ç°æ­¥éª¤

#### æ­¥éª¤1: åˆ›å»ºæ··åˆæ’åºæœåŠ¡
```bash
# åˆ›å»ºæ–°æ–‡ä»¶
touch src/services/hybrid_ranking_service.py
```

#### æ­¥éª¤2: å®ç°æ ¸å¿ƒä»£ç 

**æ–‡ä»¶**: `src/services/hybrid_ranking_service.py`

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ··åˆæ’åºå¼•æ“ - å¤šä¿¡å·èåˆæ£€ç´¢ä¼˜åŒ–
åŸºäºæ˜ å°„è¡¨æ•°æ®è¿›è¡Œæ£€ç´¢ç»“æœé‡æ–°æ’åº
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Any
from pathlib import Path
import re

class HybridRankingService:
    """æ··åˆæ’åºæœåŠ¡"""
    
    def __init__(self):
        self.mapping_data = self._load_mapping_data()
        self.precise_mappings = self._build_precise_mapping_index()
        self.fuzzy_mappings = self._build_fuzzy_mapping_index()
        
    def _load_mapping_data(self) -> Dict:
        """åŠ è½½æ˜ å°„è¡¨æ•°æ®"""
        try:
            precise_df = pd.read_csv('data/raw/ç²¾ç¡®æ˜ å°„è¡¨.csv')
            fuzzy_df = pd.read_csv('data/raw/ç²¾ç¡®+æ¨¡ç³ŠåŒ¹é…æ˜ å°„è¡¨.csv')
            
            return {
                'precise': precise_df,
                'fuzzy': fuzzy_df,
                'precise_count': len(precise_df),
                'fuzzy_count': len(fuzzy_df)
            }
        except Exception as e:
            print(f"Warning: æ˜ å°„è¡¨åŠ è½½å¤±è´¥ - {e}")
            return {'precise': pd.DataFrame(), 'fuzzy': pd.DataFrame()}
    
    def _build_precise_mapping_index(self) -> Dict:
        """æ„å»ºç²¾ç¡®æ˜ å°„ç´¢å¼•"""
        index = {}
        if not self.mapping_data['precise'].empty:
            for _, row in self.mapping_data['precise'].iterrows():
                law_id = row['å¾‹æ³•ID']
                case_id = row['æ¡ˆä¾‹ID']
                
                # åŒå‘ç´¢å¼•
                if law_id not in index:
                    index[law_id] = []
                if case_id not in index:
                    index[case_id] = []
                    
                index[law_id].append({
                    'related_id': case_id,
                    'type': 'case',
                    'relation': 'precise',
                    'description': row.get('é€‚ç”¨è¯´æ˜', '')
                })
                
                index[case_id].append({
                    'related_id': law_id,
                    'type': 'law',
                    'relation': 'precise', 
                    'description': row.get('é€‚ç”¨è¯´æ˜', '')
                })
        
        return index
    
    def _build_fuzzy_mapping_index(self) -> Dict:
        """æ„å»ºæ¨¡ç³Šæ˜ å°„ç´¢å¼•"""
        index = {}
        if not self.mapping_data['fuzzy'].empty:
            for _, row in self.mapping_data['fuzzy'].iterrows():
                law_id = row['å¾‹æ³•ID']
                case_id = row['æ¡ˆä¾‹ID']
                is_precise = row.get('ä¸»è¦é€‚ç”¨', 'å¦') == 'æ˜¯'
                
                # è·³è¿‡å·²åœ¨ç²¾ç¡®æ˜ å°„ä¸­çš„å…³ç³»
                if not is_precise:
                    if law_id not in index:
                        index[law_id] = []
                    if case_id not in index:
                        index[case_id] = []
                        
                    index[law_id].append({
                        'related_id': case_id,
                        'type': 'case',
                        'relation': 'fuzzy',
                        'description': row.get('é€‚ç”¨è¯´æ˜', '')
                    })
                    
                    index[case_id].append({
                        'related_id': law_id,
                        'type': 'law',
                        'relation': 'fuzzy',
                        'description': row.get('é€‚ç”¨è¯´æ˜', '')
                    })
        
        return index
    
    def enhance_search_results(self, query: str, original_results: List[Dict]) -> List[Dict]:
        """å¢å¼ºæ£€ç´¢ç»“æœ"""
        if not original_results:
            return original_results
        
        enhanced_results = []
        
        for result in original_results:
            enhanced_result = result.copy()
            doc_id = result.get('id', '')
            
            # 1. è®¡ç®—æ˜ å°„å…³ç³»æƒé‡
            mapping_boost = self._calculate_mapping_boost(doc_id)
            
            # 2. è®¡ç®—å…³é”®è¯åŒ¹é…æƒé‡  
            keyword_boost = self._calculate_keyword_boost(query, result)
            
            # 3. èåˆæœ€ç»ˆåˆ†æ•°
            original_score = result.get('score', 0)
            enhanced_score = (
                original_score * 0.7 +           # è¯­ä¹‰ç›¸ä¼¼åº¦ 70%
                mapping_boost * 0.2 +            # æ˜ å°„å…³ç³» 20%
                keyword_boost * 0.1              # å…³é”®è¯åŒ¹é… 10%
            )
            
            enhanced_result['original_score'] = original_score
            enhanced_result['mapping_boost'] = mapping_boost
            enhanced_result['keyword_boost'] = keyword_boost
            enhanced_result['score'] = enhanced_score
            enhanced_result['boost_applied'] = True
            
            # 4. æ·»åŠ å…³è”ä¿¡æ¯
            enhanced_result['related_docs'] = self._get_related_documents(doc_id)
            
            enhanced_results.append(enhanced_result)
        
        # é‡æ–°æ’åº
        enhanced_results.sort(key=lambda x: x['score'], reverse=True)
        
        return enhanced_results
    
    def _calculate_mapping_boost(self, doc_id: str) -> float:
        """è®¡ç®—æ˜ å°„å…³ç³»åŠ æƒåˆ†æ•°"""
        boost_score = 0.0
        
        # ç²¾ç¡®æ˜ å°„åŠ æƒæ›´é«˜
        if doc_id in self.precise_mappings:
            boost_score += 0.8  # ç²¾ç¡®æ˜ å°„åŸºç¡€åˆ†
            boost_score += min(len(self.precise_mappings[doc_id]) * 0.1, 0.2)  # å…³è”æ•°é‡åŠ æƒ
        
        # æ¨¡ç³Šæ˜ å°„åŠ æƒè¾ƒä½
        elif doc_id in self.fuzzy_mappings:
            boost_score += 0.5  # æ¨¡ç³Šæ˜ å°„åŸºç¡€åˆ†
            boost_score += min(len(self.fuzzy_mappings[doc_id]) * 0.05, 0.1)  # å…³è”æ•°é‡åŠ æƒ
        
        return min(boost_score, 1.0)
    
    def _calculate_keyword_boost(self, query: str, result: Dict) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åŠ æƒåˆ†æ•°"""
        title = result.get('title', '')
        content_preview = result.get('content_preview', '')
        
        # æå–æŸ¥è¯¢å…³é”®è¯
        query_keywords = self._extract_keywords(query)
        
        if not query_keywords:
            return 0.5  # é»˜è®¤åˆ†æ•°
        
        # è®¡ç®—åŒ¹é…åº¦
        title_matches = sum(1 for kw in query_keywords if kw in title)
        content_matches = sum(1 for kw in query_keywords if kw in content_preview)
        
        # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
        match_score = (title_matches * 0.6 + content_matches * 0.4) / len(query_keywords)
        
        return min(match_score, 1.0)
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        legal_keywords = [
            'åˆåŒ', 'è¿çº¦', 'èµ”å¿', 'è´£ä»»', 'æ³•å¾‹', 'æ¡æ¬¾', 'è¯‰è®¼', 'æ³•é™¢',
            'åˆ¤å†³', 'æŸå®³', 'æ•…æ„', 'è¿‡å¤±', 'åˆ‘æ³•', 'æ°‘æ³•', 'è¡Œæ”¿', 'ç¨‹åº',
            'è¯æ®', 'æ‰§è¡Œ', 'ä¸Šè¯‰', 'ä»²è£', 'è°ƒè§£', 'å’Œè§£'
        ]
        
        found_keywords = []
        for keyword in legal_keywords:
            if keyword in text:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def _get_related_documents(self, doc_id: str) -> List[Dict]:
        """è·å–ç›¸å…³æ–‡æ¡£ä¿¡æ¯"""
        related = []
        
        # ç²¾ç¡®å…³è”
        if doc_id in self.precise_mappings:
            for rel in self.precise_mappings[doc_id][:3]:  # æœ€å¤š3ä¸ª
                related.append({
                    'id': rel['related_id'],
                    'type': rel['type'],
                    'relation': 'precise',
                    'description': rel['description']
                })
        
        # æ¨¡ç³Šå…³è” (å¦‚æœç²¾ç¡®å…³è”ä¸è¶³)
        if len(related) < 3 and doc_id in self.fuzzy_mappings:
            remaining = 3 - len(related)
            for rel in self.fuzzy_mappings[doc_id][:remaining]:
                related.append({
                    'id': rel['related_id'],
                    'type': rel['type'],
                    'relation': 'fuzzy',
                    'description': rel['description']
                })
        
        return related
    
    def get_optimization_stats(self) -> Dict:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'precise_mappings_count': len(self.precise_mappings),
            'fuzzy_mappings_count': len(self.fuzzy_mappings),
            'total_relations': self.mapping_data.get('precise_count', 0) + 
                             self.mapping_data.get('fuzzy_count', 0),
            'optimization_enabled': True
        }
```

#### æ­¥éª¤3: é›†æˆåˆ°ç°æœ‰æ£€ç´¢æœåŠ¡

**æ–‡ä»¶**: `src/services/retrieval_service.py` (ä¿®æ”¹ç°æœ‰æ–‡ä»¶)

åœ¨ç°æœ‰çš„ `RetrievalService` ç±»ä¸­æ·»åŠ ï¼š

```python
# åœ¨ __init__ æ–¹æ³•ä¸­æ·»åŠ 
def __init__(self, index_file: str = "data/indices/complete_semantic_index.pkl"):
    # ... ç°æœ‰ä»£ç  ...
    
    # æ–°å¢ï¼šåˆå§‹åŒ–æ··åˆæ’åºæœåŠ¡
    try:
        from .hybrid_ranking_service import HybridRankingService
        self.hybrid_ranker = HybridRankingService()
        print("Hybrid ranking service initialized successfully!")
    except Exception as e:
        print(f"Warning: Hybrid ranking initialization failed - {e}")
        self.hybrid_ranker = None

# ä¿®æ”¹ search æ–¹æ³•
async def search(self, query: str, top_k: int = 10, 
                enable_hybrid: bool = True) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆæœç´¢æ–¹æ³•"""
    
    # ... ç°æœ‰æœç´¢é€»è¾‘ ...
    
    # æ–°å¢ï¼šåº”ç”¨æ··åˆæ’åºä¼˜åŒ–
    if enable_hybrid and self.hybrid_ranker:
        try:
            results = self.hybrid_ranker.enhance_search_results(query, results)
            print(f"Hybrid ranking applied to {len(results)} results")
        except Exception as e:
            print(f"Warning: Hybrid ranking failed - {e}")
    
    return {
        'query': query,
        'results': results[:top_k],
        'total': len(results),
        'search_time': search_time,
        'hybrid_enabled': enable_hybrid and self.hybrid_ranker is not None
    }
```

#### æ­¥éª¤4: æµ‹è¯•ä¼˜åŒ–æ•ˆæœ

**åˆ›å»ºæµ‹è¯•è„šæœ¬**: `test_hybrid_optimization.py`

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ··åˆæ’åºä¼˜åŒ–æ•ˆæœæµ‹è¯•
"""

import sys
from pathlib import Path
import asyncio

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

async def test_hybrid_optimization():
    """æµ‹è¯•æ··åˆæ’åºä¼˜åŒ–æ•ˆæœ"""
    from src.services.retrieval_service import get_retrieval_service
    
    service = await get_retrieval_service()
    
    test_queries = [
        "åˆåŒè¿çº¦è´£ä»»",
        "äº¤é€šäº‹æ•…èµ”å¿",
        "æ•…æ„ä¼¤å®³ç½ª",
        "åŠ³åŠ¨åˆåŒçº çº·",
        "ç¦»å©šè´¢äº§åˆ†å‰²"
    ]
    
    print("="*80)
    print("æ··åˆæ’åºä¼˜åŒ–æ•ˆæœæµ‹è¯•")
    print("="*80)
    
    for query in test_queries:
        print(f"\næµ‹è¯•æŸ¥è¯¢: {query}")
        
        # ä¸ä½¿ç”¨æ··åˆæ’åº
        result_original = await service.search(query, top_k=3, enable_hybrid=False)
        
        # ä½¿ç”¨æ··åˆæ’åº
        result_hybrid = await service.search(query, top_k=3, enable_hybrid=True)
        
        print("åŸå§‹ç»“æœ:")
        for i, doc in enumerate(result_original['results']):
            print(f"  {i+1}. åˆ†æ•°: {doc['score']:.4f} - {doc.get('title', '')[:50]}...")
            
        print("ä¼˜åŒ–ç»“æœ:")
        for i, doc in enumerate(result_hybrid['results']):
            original_score = doc.get('original_score', doc['score'])
            mapping_boost = doc.get('mapping_boost', 0)
            keyword_boost = doc.get('keyword_boost', 0)
            
            print(f"  {i+1}. åˆ†æ•°: {doc['score']:.4f} "
                  f"(åŸ{original_score:.4f}+æ˜ å°„{mapping_boost:.2f}+å…³é”®è¯{keyword_boost:.2f}) "
                  f"- {doc.get('title', '')[:40]}...")
            
            if doc.get('related_docs'):
                print(f"      å…³è”: {len(doc['related_docs'])}ä¸ªç›¸å…³æ–‡æ¡£")

if __name__ == "__main__":
    asyncio.run(test_hybrid_optimization())
```

### 1.3 ä½¿ç”¨å‘½ä»¤

```bash
# è¿è¡Œæµ‹è¯•
"C:\Users\lenovo\Miniconda3\envs\legal-ai\python.exe" test_hybrid_optimization.py

# æ­£å¸¸å¯åŠ¨ç³»ç»Ÿ (è‡ªåŠ¨å¯ç”¨æ··åˆæ’åº)
"C:\Users\lenovo\Miniconda3\envs\legal-ai\python.exe" app.py
```

### 1.4 é¢„æœŸæ•ˆæœ

- **ç›¸å…³åº¦æå‡**: ä» 0.68 æå‡åˆ° 0.75+
- **ç»“æœè´¨é‡**: æœ‰æ˜ å°„å…³ç³»çš„æ–‡æ¡£æ’åºé å‰
- **ç”¨æˆ·ä½“éªŒ**: æ£€ç´¢ç»“æœæ›´å‡†ç¡®ï¼Œæ›´ç¬¦åˆé¢„æœŸ

---

## ğŸ•¸ï¸ é˜¶æ®µäºŒ: çŸ¥è¯†å›¾è°±å…³è”åˆ†æ

### 2.1 æŠ€æœ¯åŸç†

åŸºäº1,364ä¸ªæ˜ å°„å…³ç³»æ„å»ºæ³•å¾‹çŸ¥è¯†å›¾è°±ï¼Œå®ç°ï¼š
- æ–‡æ¡£é—´å…³è”å…³ç³»å¯è§†åŒ–
- ç›¸å…³æ¨èåŠŸèƒ½
- å¯è§£é‡Šçš„æ£€ç´¢ç»“æœ

### 2.2 å®ç°æ­¥éª¤

#### æ­¥éª¤1: å®‰è£…ä¾èµ–
```bash
pip install networkx matplotlib
```

#### æ­¥éª¤2: åˆ›å»ºçŸ¥è¯†å›¾è°±æœåŠ¡

**æ–‡ä»¶**: `src/services/legal_knowledge_graph.py`

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ³•å¾‹çŸ¥è¯†å›¾è°±æœåŠ¡
åŸºäºæ˜ å°„è¡¨æ„å»ºæ–‡æ¡£å…³è”å…³ç³»å›¾è°±
"""

import pandas as pd
import networkx as nx
from typing import Dict, List, Any, Tuple
import pickle
from pathlib import Path

class LegalKnowledgeGraph:
    """æ³•å¾‹çŸ¥è¯†å›¾è°±"""
    
    def __init__(self, rebuild_graph: bool = False):
        self.graph_file = Path("data/indices/legal_knowledge_graph.pkl")
        
        if rebuild_graph or not self.graph_file.exists():
            self.graph = self._build_graph_from_mappings()
            self._save_graph()
        else:
            self.graph = self._load_graph()
        
        self.stats = self._calculate_graph_stats()
    
    def _build_graph_from_mappings(self) -> nx.Graph:
        """ä»æ˜ å°„è¡¨æ„å»ºçŸ¥è¯†å›¾è°±"""
        print("Building legal knowledge graph from mapping tables...")
        
        G = nx.Graph()
        
        # åŠ è½½æ˜ å°„æ•°æ®
        try:
            precise_df = pd.read_csv('data/raw/ç²¾ç¡®æ˜ å°„è¡¨.csv')
            fuzzy_df = pd.read_csv('data/raw/ç²¾ç¡®+æ¨¡ç³ŠåŒ¹é…æ˜ å°„è¡¨.csv')
        except Exception as e:
            print(f"Error loading mapping tables: {e}")
            return G
        
        # æ·»åŠ ç²¾ç¡®æ˜ å°„å…³ç³»
        for _, row in precise_df.iterrows():
            law_id = row['å¾‹æ³•ID']
            case_id = row['æ¡ˆä¾‹ID']
            description = row.get('é€‚ç”¨è¯´æ˜', '')
            
            G.add_edge(law_id, case_id,
                      relation_type='precise',
                      description=description,
                      weight=1.0,
                      mapping_id=row.get('æ˜ å°„ID', ''))
        
        # æ·»åŠ æ¨¡ç³Šæ˜ å°„å…³ç³» (æ’é™¤å·²æœ‰çš„ç²¾ç¡®å…³ç³»)
        for _, row in fuzzy_df.iterrows():
            law_id = row['å¾‹æ³•ID']
            case_id = row['æ¡ˆä¾‹ID']
            is_precise = row.get('ä¸»è¦é€‚ç”¨', 'å¦') == 'æ˜¯'
            description = row.get('é€‚ç”¨è¯´æ˜', '')
            
            # å¦‚æœä¸æ˜¯ç²¾ç¡®å…³ç³»ä¸”ä¸å­˜åœ¨è¾¹ï¼Œåˆ™æ·»åŠ 
            if not is_precise and not G.has_edge(law_id, case_id):
                G.add_edge(law_id, case_id,
                          relation_type='fuzzy',
                          description=description,
                          weight=0.6,
                          mapping_id=row.get('æ˜ å°„ID', ''))
        
        print(f"Knowledge graph built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
        return G
    
    def _save_graph(self):
        """ä¿å­˜å›¾è°±åˆ°æ–‡ä»¶"""
        try:
            with open(self.graph_file, 'wb') as f:
                pickle.dump(self.graph, f)
            print(f"Knowledge graph saved to {self.graph_file}")
        except Exception as e:
            print(f"Error saving graph: {e}")
    
    def _load_graph(self) -> nx.Graph:
        """ä»æ–‡ä»¶åŠ è½½å›¾è°±"""
        try:
            with open(self.graph_file, 'rb') as f:
                graph = pickle.load(f)
            print(f"Knowledge graph loaded from {self.graph_file}")
            return graph
        except Exception as e:
            print(f"Error loading graph: {e}")
            return nx.Graph()
    
    def get_related_documents(self, doc_id: str, max_results: int = 5) -> List[Dict]:
        """è·å–ç›¸å…³æ–‡æ¡£"""
        if doc_id not in self.graph:
            return []
        
        related_docs = []
        neighbors = list(self.graph.neighbors(doc_id))
        
        # æŒ‰å…³ç³»æƒé‡æ’åº
        neighbor_weights = []
        for neighbor in neighbors:
            edge_data = self.graph[doc_id][neighbor]
            weight = edge_data.get('weight', 0.5)
            relation_type = edge_data.get('relation_type', 'unknown')
            description = edge_data.get('description', '')
            
            neighbor_weights.append({
                'doc_id': neighbor,
                'weight': weight,
                'relation_type': relation_type,
                'description': description
            })
        
        # æ’åºå¹¶è¿”å›
        neighbor_weights.sort(key=lambda x: x['weight'], reverse=True)
        
        return neighbor_weights[:max_results]
    
    def get_document_centrality(self, doc_id: str) -> Dict[str, float]:
        """è·å–æ–‡æ¡£åœ¨å›¾è°±ä¸­çš„é‡è¦æ€§æŒ‡æ ‡"""
        if doc_id not in self.graph:
            return {}
        
        # è®¡ç®—å„ç§ä¸­å¿ƒæ€§æŒ‡æ ‡
        degree_centrality = nx.degree_centrality(self.graph).get(doc_id, 0)
        try:
            betweenness_centrality = nx.betweenness_centrality(self.graph, k=min(100, self.graph.number_of_nodes())).get(doc_id, 0)
        except:
            betweenness_centrality = 0
        
        return {
            'degree_centrality': degree_centrality,
            'betweenness_centrality': betweenness_centrality,
            'neighbor_count': len(list(self.graph.neighbors(doc_id)))
        }
    
    def find_shortest_path(self, doc_id1: str, doc_id2: str) -> List[str]:
        """æŸ¥æ‰¾ä¸¤ä¸ªæ–‡æ¡£é—´çš„æœ€çŸ­è·¯å¾„"""
        try:
            if doc_id1 in self.graph and doc_id2 in self.graph:
                return nx.shortest_path(self.graph, doc_id1, doc_id2)
        except nx.NetworkXNoPath:
            pass
        return []
    
    def get_subgraph_around_document(self, doc_id: str, radius: int = 2) -> nx.Graph:
        """è·å–æ–‡æ¡£å‘¨å›´çš„å­å›¾"""
        if doc_id not in self.graph:
            return nx.Graph()
        
        # è·å–æŒ‡å®šåŠå¾„å†…çš„æ‰€æœ‰èŠ‚ç‚¹
        nodes_in_radius = set([doc_id])
        current_nodes = set([doc_id])
        
        for _ in range(radius):
            next_nodes = set()
            for node in current_nodes:
                next_nodes.update(self.graph.neighbors(node))
            current_nodes = next_nodes - nodes_in_radius
            nodes_in_radius.update(current_nodes)
        
        return self.graph.subgraph(nodes_in_radius)
    
    def _calculate_graph_stats(self) -> Dict:
        """è®¡ç®—å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        if self.graph.number_of_nodes() == 0:
            return {}
        
        stats = {
            'total_nodes': self.graph.number_of_nodes(),
            'total_edges': self.graph.number_of_edges(),
            'average_degree': sum(dict(self.graph.degree()).values()) / self.graph.number_of_nodes(),
            'density': nx.density(self.graph),
            'connected_components': nx.number_connected_components(self.graph)
        }
        
        # è¾¹ç±»å‹ç»Ÿè®¡
        edge_types = {}
        for _, _, data in self.graph.edges(data=True):
            rel_type = data.get('relation_type', 'unknown')
            edge_types[rel_type] = edge_types.get(rel_type, 0) + 1
        
        stats['edge_types'] = edge_types
        
        return stats
    
    def get_graph_stats(self) -> Dict:
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats
    
    def export_graph_for_visualization(self, output_file: str = "legal_graph.gexf"):
        """å¯¼å‡ºå›¾è°±ç”¨äºå¯è§†åŒ–"""
        try:
            nx.write_gexf(self.graph, output_file)
            print(f"Graph exported to {output_file}")
        except Exception as e:
            print(f"Error exporting graph: {e}")
```

#### æ­¥éª¤3: é›†æˆåˆ°æ£€ç´¢æœåŠ¡

åœ¨ `src/services/retrieval_service.py` ä¸­æ·»åŠ ï¼š

```python
# åœ¨ __init__ æ–¹æ³•ä¸­æ·»åŠ 
try:
    from .legal_knowledge_graph import LegalKnowledgeGraph
    self.knowledge_graph = LegalKnowledgeGraph()
    print("Knowledge graph service initialized successfully!")
except Exception as e:
    print(f"Warning: Knowledge graph initialization failed - {e}")
    self.knowledge_graph = None

# åœ¨ search æ–¹æ³•ä¸­æ·»åŠ å…³è”ä¿¡æ¯
if self.knowledge_graph:
    for result in results:
        doc_id = result.get('id', '')
        
        # æ·»åŠ å…³è”æ–‡æ¡£
        related_docs = self.knowledge_graph.get_related_documents(doc_id, max_results=3)
        result['knowledge_graph_relations'] = related_docs
        
        # æ·»åŠ é‡è¦æ€§æŒ‡æ ‡
        centrality = self.knowledge_graph.get_document_centrality(doc_id)
        result['document_importance'] = centrality
```

### 2.3 ä½¿ç”¨è¯´æ˜

åˆ›å»ºæµ‹è¯•è„šæœ¬æµ‹è¯•çŸ¥è¯†å›¾è°±åŠŸèƒ½ï¼š

```python
# test_knowledge_graph.py
import asyncio
from src.services.legal_knowledge_graph import LegalKnowledgeGraph

async def test_knowledge_graph():
    kg = LegalKnowledgeGraph(rebuild_graph=True)
    
    print("å›¾è°±ç»Ÿè®¡:", kg.get_graph_stats())
    
    # æµ‹è¯•å…³è”æŸ¥è¯¢
    test_doc = "law2023"  # æ›¿æ¢ä¸ºå®é™…å­˜åœ¨çš„æ–‡æ¡£ID
    related = kg.get_related_documents(test_doc)
    print(f"\n{test_doc} çš„å…³è”æ–‡æ¡£:", related)

if __name__ == "__main__":
    asyncio.run(test_knowledge_graph())
```

---

## ğŸ¤– é˜¶æ®µä¸‰: å¯¹æ¯”å­¦ä¹ æ¨¡å‹ä¼˜åŒ–

### 3.1 æŠ€æœ¯åŸç†

åˆ©ç”¨668ä¸ªç²¾ç¡®æ˜ å°„å…³ç³»è¿›è¡Œå¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼Œè®©æ¨¡å‹ä¸“é—¨å­¦ä¹ æ³•å¾‹æ–‡æ¡£çš„è¯­ä¹‰å…³ç³»ã€‚

### 3.2 å®ç°æ­¥éª¤

#### æ­¥éª¤1: å®‰è£…è®­ç»ƒä¾èµ–
```bash
pip install sentence-transformers torch scikit-learn
```

#### æ­¥éª¤2: åˆ›å»ºè®­ç»ƒæ•°æ®æ„å»ºå™¨

**æ–‡ä»¶**: `src/models/contrastive_training.py`

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡å—
åŸºäºç²¾ç¡®æ˜ å°„è¡¨è¿›è¡Œæ¨¡å‹ä¼˜åŒ–
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from torch.utils.data import DataLoader
import random
from typing import List, Tuple, Dict
from pathlib import Path

class ContrastiveLegalTrainer:
    """æ³•å¾‹é¢†åŸŸå¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, base_model_name: str = 'shibing624/text2vec-base-chinese'):
        self.base_model_name = base_model_name
        self.model = SentenceTransformer(base_model_name)
        
        # åŠ è½½åŸå§‹æ–‡æ¡£æ•°æ®
        self.laws_data = self._load_laws_data()
        self.cases_data = self._load_cases_data()
        self.mappings = self._load_mappings()
        
    def _load_laws_data(self) -> Dict:
        """åŠ è½½æ³•æ¡æ•°æ®"""
        try:
            with open('data/processed/full_dataset.pkl', 'rb') as f:
                import pickle
                full_data = pickle.load(f)
                
            laws_dict = {}
            for item in full_data:
                if item.get('type') == 'law':
                    laws_dict[item['id']] = {
                        'title': item.get('title', ''),
                        'content': item.get('content', '')
                    }
            return laws_dict
        except Exception as e:
            print(f"Error loading laws data: {e}")
            return {}
    
    def _load_cases_data(self) -> Dict:
        """åŠ è½½æ¡ˆä¾‹æ•°æ®"""
        try:
            with open('data/processed/full_dataset.pkl', 'rb') as f:
                import pickle
                full_data = pickle.load(f)
                
            cases_dict = {}
            for item in full_data:
                if item.get('type') == 'case':
                    cases_dict[item['id']] = {
                        'title': item.get('title', ''),
                        'content': item.get('content', '')
                    }
            return cases_dict
        except Exception as e:
            print(f"Error loading cases data: {e}")
            return {}
    
    def _load_mappings(self) -> pd.DataFrame:
        """åŠ è½½ç²¾ç¡®æ˜ å°„è¡¨"""
        try:
            return pd.read_csv('data/raw/ç²¾ç¡®æ˜ å°„è¡¨.csv')
        except Exception as e:
            print(f"Error loading mappings: {e}")
            return pd.DataFrame()
    
    def create_training_examples(self, test_ratio: float = 0.2) -> Tuple[List, List]:
        """åˆ›å»ºè®­ç»ƒæ ·æœ¬"""
        print("Creating training examples from precise mappings...")
        
        train_examples = []
        test_examples = []
        
        # ä»ç²¾ç¡®æ˜ å°„åˆ›å»ºæ­£æ ·æœ¬å¯¹
        positive_pairs = []
        for _, row in self.mappings.iterrows():
            law_id = row['å¾‹æ³•ID']
            case_id = row['æ¡ˆä¾‹ID']
            
            # è·å–æ–‡æœ¬å†…å®¹
            law_text = self._get_document_text(law_id, 'law')
            case_text = self._get_document_text(case_id, 'case')
            
            if law_text and case_text:
                positive_pairs.append((case_text, law_text, 1.0))
        
        print(f"Created {len(positive_pairs)} positive pairs")
        
        # åˆ›å»ºè´Ÿæ ·æœ¬å¯¹
        negative_pairs = []
        law_ids = list(self.laws_data.keys())
        case_ids = list(self.cases_data.keys())
        
        # è·å–å·²æœ‰çš„æ­£æ ·æœ¬å¯¹
        positive_law_case_pairs = set()
        for _, row in self.mappings.iterrows():
            positive_law_case_pairs.add((row['å¾‹æ³•ID'], row['æ¡ˆä¾‹ID']))
        
        # ç”Ÿæˆè´Ÿæ ·æœ¬ï¼ˆéšæœºé…å¯¹ï¼Œä½†æ’é™¤æ­£æ ·æœ¬ï¼‰
        negative_count = min(len(positive_pairs), 1000)  # é™åˆ¶è´Ÿæ ·æœ¬æ•°é‡
        attempts = 0
        max_attempts = negative_count * 10
        
        while len(negative_pairs) < negative_count and attempts < max_attempts:
            law_id = random.choice(law_ids)
            case_id = random.choice(case_ids)
            
            if (law_id, case_id) not in positive_law_case_pairs:
                law_text = self._get_document_text(law_id, 'law')
                case_text = self._get_document_text(case_id, 'case')
                
                if law_text and case_text:
                    negative_pairs.append((case_text, law_text, 0.0))
            
            attempts += 1
        
        print(f"Created {len(negative_pairs)} negative pairs")
        
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬
        all_pairs = positive_pairs + negative_pairs
        random.shuffle(all_pairs)
        
        # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        split_index = int(len(all_pairs) * (1 - test_ratio))
        train_pairs = all_pairs[:split_index]
        test_pairs = all_pairs[split_index:]
        
        # è½¬æ¢ä¸ºSentenceTransformersæ ¼å¼
        train_examples = [
            InputExample(texts=[pair[0], pair[1]], label=pair[2])
            for pair in train_pairs
        ]
        
        test_examples = [
            InputExample(texts=[pair[0], pair[1]], label=pair[2])
            for pair in test_pairs
        ]
        
        print(f"Training examples: {len(train_examples)}")
        print(f"Test examples: {len(test_examples)}")
        
        return train_examples, test_examples
    
    def _get_document_text(self, doc_id: str, doc_type: str) -> str:
        """è·å–æ–‡æ¡£æ–‡æœ¬"""
        if doc_type == 'law' and doc_id in self.laws_data:
            doc = self.laws_data[doc_id]
            return f"{doc['title']} {doc['content']}"[:512]  # é™åˆ¶é•¿åº¦
        elif doc_type == 'case' and doc_id in self.cases_data:
            doc = self.cases_data[doc_id]
            return f"{doc['title']} {doc['content']}"[:512]  # é™åˆ¶é•¿åº¦
        return ""
    
    def train_model(self, output_path: str = 'models/legal_model_optimized', 
                   epochs: int = 3, batch_size: int = 16):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"Starting contrastive learning training...")
        print(f"Base model: {self.base_model_name}")
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        train_examples, test_examples = self.create_training_examples()
        
        if not train_examples:
            print("No training examples created. Aborting training.")
            return None
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
            test_examples, name='legal-eval'
        )
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        train_loss = losses.CosineSimilarityLoss(self.model)
        
        # è®­ç»ƒå‚æ•°
        warmup_steps = int(len(train_dataloader) * epochs * 0.1)
        
        print(f"Training parameters:")
        print(f"  Epochs: {epochs}")
        print(f"  Batch size: {batch_size}")
        print(f"  Warmup steps: {warmup_steps}")
        print(f"  Training batches per epoch: {len(train_dataloader)}")
        
        # å¼€å§‹è®­ç»ƒ
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            evaluator=evaluator,
            epochs=epochs,
            evaluation_steps=len(train_dataloader) // 2,  # æ¯åŠä¸ªepochè¯„ä¼°ä¸€æ¬¡
            warmup_steps=warmup_steps,
            output_path=output_path,
            save_best_model=True,
            show_progress_bar=True
        )
        
        print(f"Training completed! Model saved to: {output_path}")
        return output_path
    
    def evaluate_model(self, model_path: str):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("Evaluating model performance...")
        
        # åŠ è½½è®­ç»ƒåçš„æ¨¡å‹
        optimized_model = SentenceTransformer(model_path)
        
        # åˆ›å»ºæµ‹è¯•æ ·æœ¬
        _, test_examples = self.create_training_examples()
        
        if not test_examples:
            print("No test examples available.")
            return
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        correct_predictions = 0
        total_predictions = len(test_examples)
        
        for example in test_examples[:100]:  # é™åˆ¶æµ‹è¯•æ•°é‡
            text1, text2 = example.texts
            expected_label = example.label
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            embeddings = optimized_model.encode([text1, text2])
            similarity = np.dot(embeddings[0], embeddings[1]) / (
                np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])
            )
            
            # ç®€å•é˜ˆå€¼åˆ†ç±»
            predicted_label = 1.0 if similarity > 0.5 else 0.0
            
            if abs(predicted_label - expected_label) < 0.5:
                correct_predictions += 1
        
        accuracy = correct_predictions / min(total_predictions, 100)
        print(f"Model accuracy on test set: {accuracy:.4f}")
        
        return accuracy
```

#### æ­¥éª¤3: åˆ›å»ºè®­ç»ƒè„šæœ¬

**æ–‡ä»¶**: `train_legal_model.py`

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ³•å¾‹æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import sys
from pathlib import Path
import os

# è®¾ç½®é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    from src.models.contrastive_training import ContrastiveLegalTrainer
    
    print("="*80)
    print("æ³•å¾‹é¢†åŸŸå¯¹æ¯”å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
    print("="*80)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = ContrastiveLegalTrainer()
    
    # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
    print(f"æ³•æ¡æ•°æ®: {len(trainer.laws_data)} ä¸ª")
    print(f"æ¡ˆä¾‹æ•°æ®: {len(trainer.cases_data)} ä¸ª")  
    print(f"æ˜ å°„å…³ç³»: {len(trainer.mappings)} ä¸ª")
    
    if len(trainer.mappings) == 0:
        print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æ˜ å°„æ•°æ®ï¼Œæ— æ³•è¿›è¡Œè®­ç»ƒ")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("models/legal_model_optimized")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        model_path = trainer.train_model(
            output_path=str(output_dir),
            epochs=3,
            batch_size=16
        )
        
        if model_path:
            print(f"\nè®­ç»ƒå®Œæˆ! æ¨¡å‹ä¿å­˜åœ¨: {model_path}")
            
            # è¯„ä¼°æ¨¡å‹
            trainer.evaluate_model(model_path)
            
            print(f"\nä½¿ç”¨æ–°æ¨¡å‹é‡å»ºç´¢å¼•:")
            print(f"1. å°†æ–°æ¨¡å‹è·¯å¾„æ›´æ–°åˆ° src/config/settings.py")
            print(f"2. è¿è¡Œå‘é‡åŒ–è„šæœ¬é‡å»ºç´¢å¼•")
            print(f"3. é‡å¯ç³»ç»Ÿä½¿ç”¨ä¼˜åŒ–æ¨¡å‹")
            
        else:
            print("è®­ç»ƒå¤±è´¥!")
            
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

### 3.3 ä½¿ç”¨æ­¥éª¤

```bash
# 1. å¼€å§‹è®­ç»ƒ
"C:\Users\lenovo\Miniconda3\envs\legal-ai\python.exe" train_legal_model.py

# 2. è®­ç»ƒå®Œæˆåï¼Œæ›´æ–°é…ç½®ä½¿ç”¨æ–°æ¨¡å‹
# ç¼–è¾‘ src/config/settings.pyï¼Œä¿®æ”¹æ¨¡å‹è·¯å¾„

# 3. é‡å»ºå‘é‡ç´¢å¼•
"C:\Users\lenovo\Miniconda3\envs\legal-ai\python.exe" tools/full_vectorization_executor.py

# 4. é‡å¯ç³»ç»Ÿ
"C:\Users\lenovo\Miniconda3\envs\legal-ai\python.exe" app.py
```

---

## ğŸ“Š ä¼˜åŒ–æ•ˆæœç›‘æ§

### åˆ›å»ºæ•ˆæœå¯¹æ¯”æµ‹è¯•

**æ–‡ä»¶**: `optimization_comparison.py`

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–æ•ˆæœå¯¹æ¯”æµ‹è¯•
"""

import sys
import asyncio
import time
from pathlib import Path

project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

async def comprehensive_comparison():
    """å…¨é¢å¯¹æ¯”ä¼˜åŒ–å‰åæ•ˆæœ"""
    from src.services.retrieval_service import get_retrieval_service
    
    service = await get_retrieval_service()
    
    test_cases = [
        "åˆåŒè¿çº¦çš„æ³•å¾‹è´£ä»»",
        "äº¤é€šäº‹æ•…èµ”å¿æ ‡å‡†",
        "åŠ³åŠ¨äº‰è®®å¤„ç†ç¨‹åº", 
        "æ•…æ„ä¼¤å®³ç½ªæ„æˆè¦ä»¶",
        "ç¦»å©šè´¢äº§åˆ†å‰²åŸåˆ™"
    ]
    
    print("="*80)
    print("ä¼˜åŒ–æ•ˆæœå…¨é¢å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    results_summary = []
    
    for query in test_cases:
        print(f"\næµ‹è¯•æŸ¥è¯¢: {query}")
        
        # æµ‹è¯•å„ç§é…ç½®
        configs = [
            {"enable_hybrid": False, "label": "åŸå§‹"},
            {"enable_hybrid": True, "label": "æ··åˆæ’åº"},
        ]
        
        query_results = {"query": query, "configs": {}}
        
        for config in configs:
            start_time = time.time()
            result = await service.search(query, top_k=5, **config)
            end_time = time.time()
            
            # è®¡ç®—å¹³å‡ç›¸å…³åº¦
            scores = [r.get('score', 0) for r in result['results']]
            avg_score = sum(scores) / len(scores) if scores else 0
            response_time = (end_time - start_time) * 1000
            
            query_results["configs"][config["label"]] = {
                "avg_score": avg_score,
                "response_time": response_time,
                "results_count": len(result['results'])
            }
            
            print(f"  {config['label']}: å¹³å‡ç›¸å…³åº¦ {avg_score:.4f}, å“åº”æ—¶é—´ {response_time:.1f}ms")
        
        results_summary.append(query_results)
    
    # æ€»ä½“ç»Ÿè®¡
    print(f"\n" + "="*50)
    print("æ€»ä½“æ•ˆæœå¯¹æ¯”")
    print("="*50)
    
    for config_name in ["åŸå§‹", "æ··åˆæ’åº"]:
        scores = []
        times = []
        
        for result in results_summary:
            if config_name in result["configs"]:
                config_data = result["configs"][config_name]
                scores.append(config_data["avg_score"])
                times.append(config_data["response_time"])
        
        if scores:
            avg_score = sum(scores) / len(scores)
            avg_time = sum(times) / len(times)
            print(f"\n{config_name}é…ç½®:")
            print(f"  å¹³å‡ç›¸å…³åº¦: {avg_score:.4f}")
            print(f"  å¹³å‡å“åº”æ—¶é—´: {avg_time:.1f}ms")
    
    return results_summary

if __name__ == "__main__":
    results = asyncio.run(comprehensive_comparison())
```

---

## ğŸ“‹ ä¼˜åŒ–å®æ–½æ£€æŸ¥æ¸…å•

### é˜¶æ®µä¸€æ£€æŸ¥æ¸…å•
- [ ] åˆ›å»º `src/services/hybrid_ranking_service.py`
- [ ] ä¿®æ”¹ `src/services/retrieval_service.py` é›†æˆæ··åˆæ’åº
- [ ] åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯æ•ˆæœ
- [ ] ç¡®è®¤ç›¸å…³åº¦æå‡ (ç›®æ ‡: 0.68 â†’ 0.75+)

### é˜¶æ®µäºŒæ£€æŸ¥æ¸…å•  
- [ ] å®‰è£… networkx ä¾èµ–
- [ ] åˆ›å»º `src/services/legal_knowledge_graph.py`
- [ ] é›†æˆçŸ¥è¯†å›¾è°±åˆ°æ£€ç´¢æœåŠ¡
- [ ] æµ‹è¯•å…³è”å…³ç³»åŠŸèƒ½
- [ ] éªŒè¯å¯è§£é‡Šæ€§å¢å¼º

### é˜¶æ®µä¸‰æ£€æŸ¥æ¸…å•
- [ ] å®‰è£…è®­ç»ƒç›¸å…³ä¾èµ–
- [ ] åˆ›å»º `src/models/contrastive_training.py`
- [ ] è¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆä¼˜åŒ–æ¨¡å‹
- [ ] ä½¿ç”¨æ–°æ¨¡å‹é‡å»ºå‘é‡ç´¢å¼•
- [ ] å¯¹æ¯”è®­ç»ƒå‰åæ•ˆæœ

### æœ€ç»ˆéªŒæ”¶æ¸…å•
- [ ] è¿è¡Œå®Œæ•´çš„æ•ˆæœå¯¹æ¯”æµ‹è¯•
- [ ] ç¡®è®¤æ‰€æœ‰ä¼˜åŒ–åŠŸèƒ½æ­£å¸¸å·¥ä½œ
- [ ] æ›´æ–°ç³»ç»Ÿæ–‡æ¡£å’ŒAPIè¯´æ˜
- [ ] å‡†å¤‡ä¼˜åŒ–æˆæœæŠ¥å‘Š

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

1. **æ˜ å°„è¡¨åŠ è½½å¤±è´¥**
   - æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤CSVæ–‡ä»¶æ ¼å¼å’Œç¼–ç 
   
2. **è®­ç»ƒå†…å­˜ä¸è¶³**
   - å‡å°batch_sizeå‚æ•°
   - é™åˆ¶è®­ç»ƒæ ·æœ¬æ•°é‡

3. **æ¨¡å‹åŠ è½½é”™è¯¯**
   - ç¡®è®¤æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§
   - æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬å…¼å®¹æ€§

4. **æ€§èƒ½æ²¡æœ‰æå‡**
   - æ£€æŸ¥æ˜ å°„æ•°æ®è´¨é‡
   - è°ƒæ•´æƒé‡å‚æ•°
   - éªŒè¯æµ‹è¯•ç”¨ä¾‹

---

è¿™ä¸ªä¼˜åŒ–æŠ€æœ¯æ–‡æ¡£ä¸ºä½ æä¾›äº†å®Œæ•´çš„å®æ–½è·¯å¾„ã€‚å»ºè®®å…ˆä»é˜¶æ®µä¸€çš„æ··åˆæ’åºå¼€å§‹ï¼Œå› ä¸ºå®ƒèƒ½å¿«é€Ÿçœ‹åˆ°æ•ˆæœã€‚æ¯ä¸ªé˜¶æ®µå®Œæˆåéƒ½å¯ä»¥è¿›è¡Œæµ‹è¯•éªŒè¯ï¼Œç¡®ä¿ä¼˜åŒ–æ•ˆæœç¬¦åˆé¢„æœŸã€‚