# æ³•æ™ºå¯¼èˆªç³»ç»Ÿ - æŠ€æœ¯æ¶æ„æ–‡æ¡£

## ğŸ—ï¸ ç³»ç»Ÿæ¦‚è§ˆ

æ³•æ™ºå¯¼èˆªæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ™ºèƒ½æ³•å¾‹æ£€ç´¢ç³»ç»Ÿï¼Œé‡‡ç”¨ç°ä»£åŒ–çš„å¾®æœåŠ¡æ¶æ„å’ŒAIæŠ€æœ¯æ ˆï¼Œå®ç°è‡ªç„¶è¯­è¨€åˆ°æ³•å¾‹æ¡æ–‡çš„ç²¾å‡†åŒ¹é…ã€‚

### æ ¸å¿ƒæŠ€æœ¯æ ˆ
- **åç«¯æ¡†æ¶**: FastAPI + Python 3.9+
- **AIæ¡†æ¶**: PyTorch + Transformers + Sentence-Transformers
- **å‘é‡æ£€ç´¢**: Faiss
- **æ•°æ®å¤„ç†**: Pandas + NumPy
- **é…ç½®ç®¡ç†**: PyYAML + Pydantic
- **æ—¥å¿—ç³»ç»Ÿ**: Loguru
- **å®¹å™¨åŒ–**: Docker
- **å‰ç«¯**: HTML/CSS/JavaScript (ç®€æ´ç‰ˆ)

---

## ğŸ¯ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å‰ç«¯ç•Œé¢      â”‚â”€â”€â”€â”€â”‚   APIç½‘å…³       â”‚â”€â”€â”€â”€â”‚   ä¸šåŠ¡é€»è¾‘å±‚     â”‚
â”‚  Web UI         â”‚    â”‚  FastAPI        â”‚    â”‚  Service Layer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   AIæ¨¡å‹å±‚      â”‚
                       â”‚  Model Layer    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å‘é‡ç´¢å¼•      â”‚    â”‚   æ•°æ®å­˜å‚¨      â”‚    â”‚   é…ç½®ç®¡ç†      â”‚
â”‚  Faiss Index    â”‚    â”‚  CSV/Pickle     â”‚    â”‚  Config Files   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

#### 1. æ•°æ®å¤„ç†å±‚ (Data Processing Layer)
```python
data/
â”œâ”€â”€ raw/                    # åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ raw_laws(1).csv    # æ³•å¾‹æ¡æ–‡åŸå§‹æ•°æ®
â”‚   â”œâ”€â”€ raw_cases(1).csv   # æ¡ˆä¾‹åŸå§‹æ•°æ®
â”‚   â””â”€â”€ ç²¾ç¡®æ˜ å°„è¡¨.csv      # æ¡æ–‡æ¡ˆä¾‹æ˜ å°„å…³ç³»
â””â”€â”€ processed/             # å¤„ç†åæ•°æ®
    â”œâ”€â”€ unified_kb.csv     # ç»Ÿä¸€çŸ¥è¯†åº“
    â”œâ”€â”€ embeddings.npy     # å‘é‡è¡¨ç¤º
    â””â”€â”€ metadata.json      # å…ƒæ•°æ®ä¿¡æ¯
```

**èŒè´£**:
- æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
- æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯
- ç»Ÿä¸€æ•°æ®æ ¼å¼å’Œç¼–ç 

#### 2. AIæ¨¡å‹å±‚ (AI Model Layer)
```python
src/models/
â”œâ”€â”€ embedder.py           # æ–‡æœ¬å‘é‡åŒ–
â”œâ”€â”€ retriever.py         # æ£€ç´¢å¼•æ“
â”œâ”€â”€ trainer.py           # æ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ evaluator.py         # æ€§èƒ½è¯„ä¼°
â””â”€â”€ knowledge_graph.py   # çŸ¥è¯†å›¾è°±
```

**æ ¸å¿ƒæ¨¡å—**:

**a) æ–‡æœ¬åµŒå…¥æ¨¡å— (Text Embedder)**
```python
class LegalEmbedder:
    """æ³•å¾‹æ–‡æœ¬å‘é‡åŒ–å™¨"""
    
    def __init__(self, model_name: str, device: str = "cpu"):
        self.model = SentenceTransformer(model_name)
        self.device = device
        
    def embed_texts(self, texts: List[str]) -> np.ndarray:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        embeddings = self.model.encode(
            texts, 
            batch_size=32,
            normalize_embeddings=True,
            show_progress_bar=True
        )
        return embeddings
```

**b) æ£€ç´¢å¼•æ“ (Retriever)**
```python
class LegalRetriever:
    """æ³•å¾‹æ–‡æ¡£æ£€ç´¢å™¨"""
    
    def __init__(self, index_path: str, embedder: LegalEmbedder):
        self.index = faiss.read_index(index_path)
        self.embedder = embedder
        self.metadata = self._load_metadata()
    
    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # 1. æŸ¥è¯¢å‘é‡åŒ–
        query_embedding = self.embedder.embed_texts([query])
        
        # 2. å‘é‡æ£€ç´¢
        scores, indices = self.index.search(query_embedding, top_k)
        
        # 3. ç»“æœæ•´ç†
        results = []
        for score, idx in zip(scores[0], indices[0]):
            result = self.metadata[idx].copy()
            result['score'] = float(score)
            results.append(result)
        
        return results
```

#### 3. APIæœåŠ¡å±‚ (API Service Layer)
```python
src/api/
â”œâ”€â”€ main.py              # FastAPIåº”ç”¨å…¥å£
â”œâ”€â”€ endpoints.py         # APIè·¯ç”±å®šä¹‰
â”œâ”€â”€ models.py           # Pydanticæ•°æ®æ¨¡å‹
â”œâ”€â”€ dependencies.py     # ä¾èµ–æ³¨å…¥
â””â”€â”€ middleware.py       # ä¸­é—´ä»¶
```

**APIæ¶æ„**:
```python
from fastapi import FastAPI, Depends
from pydantic import BaseModel

class SearchRequest(BaseModel):
    query: str
    top_k: int = 10
    include_laws: bool = True
    include_cases: bool = True

@app.post("/api/v1/search")
async def search_documents(
    request: SearchRequest,
    retriever: LegalRetriever = Depends(get_retriever)
):
    results = retriever.search(
        query=request.query,
        top_k=request.top_k
    )
    return {"status": "success", "data": {"results": results}}
```

#### 4. é…ç½®ç®¡ç†å±‚ (Configuration Layer)
```yaml
# config/config.yaml
model:
  pretrained_model_name: "shibing624/text2vec-base-chinese"
  embedding_dim: 768
  max_sequence_length: 512

retrieval:
  index_type: "IndexFlatIP"
  top_k: 10
  search_batch_size: 32

api:
  host: "0.0.0.0"
  port: 8000
  timeout_seconds: 30
```

---

## ğŸ§  AIç®—æ³•è®¾è®¡

### 1. è¯­ä¹‰å‘é‡åŒ– (Semantic Embedding)

**ç®—æ³•æµç¨‹**:
```
åŸå§‹æ–‡æœ¬ â†’ æ–‡æœ¬é¢„å¤„ç† â†’ BERTç¼–ç  â†’ å‘é‡è¡¨ç¤º â†’ æ ‡å‡†åŒ–
```

**æŠ€æœ¯ç»†èŠ‚**:
- **é¢„è®­ç»ƒæ¨¡å‹**: shibing624/text2vec-base-chinese
- **å‘é‡ç»´åº¦**: 768ç»´
- **æœ€å¤§åºåˆ—é•¿åº¦**: 512 tokens
- **æ ‡å‡†åŒ–æ–¹æ³•**: L2 normalization

### 2. å‘é‡æ£€ç´¢ (Vector Retrieval)

**ç´¢å¼•ç»“æ„**:
```python
# Faissç´¢å¼•é…ç½®
index = faiss.IndexFlatIP(768)  # å†…ç§¯æœç´¢
index = faiss.IndexIVFFlat(quantizer, 768, nlist)  # å€’æ’ç´¢å¼•ï¼ˆå¤§æ•°æ®é›†ï¼‰
```

**æ£€ç´¢ç­–ç•¥**:
1. **ç²—å¬å›**: Faisså¿«é€Ÿæ£€ç´¢Top-Kå€™é€‰
2. **ç²¾æ’åº**: è¯­ä¹‰ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…
3. **åå¤„ç†**: å»é‡ã€å…³è”åˆ†æã€ç»“æœå¢å¼º

### 3. æ··åˆæ’åºç®—æ³• (Hybrid Ranking)

```python
def hybrid_ranking(query: str, candidates: List[Dict], alpha: float = 0.7):
    """æ··åˆæ’åºï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…"""
    
    # 1. è¯­ä¹‰ç›¸ä¼¼åº¦å¾—åˆ† (70%)
    semantic_scores = [doc['semantic_score'] for doc in candidates]
    
    # 2. å…³é”®è¯åŒ¹é…å¾—åˆ† (30%)  
    keyword_scores = []
    query_keywords = extract_keywords(query)
    
    for doc in candidates:
        doc_keywords = extract_keywords(doc['content'])
        keyword_score = jaccard_similarity(query_keywords, doc_keywords)
        keyword_scores.append(keyword_score)
    
    # 3. åŠ æƒèåˆ
    final_scores = []
    for sem_score, kw_score in zip(semantic_scores, keyword_scores):
        final_score = alpha * sem_score + (1 - alpha) * kw_score
        final_scores.append(final_score)
    
    # 4. é‡æ–°æ’åº
    sorted_indices = np.argsort(final_scores)[::-1]
    return [candidates[i] for i in sorted_indices]
```

---

## ğŸ“Š æ•°æ®æ¶æ„

### 1. æ•°æ®æ¨¡å‹

**ç»Ÿä¸€çŸ¥è¯†åº“ç»“æ„**:
```python
@dataclass
class LegalDocument:
    id: str                    # å”¯ä¸€æ ‡è¯†ç¬¦
    type: str                 # ç±»å‹: "law" | "case"
    title: str                # æ ‡é¢˜
    content: str              # æ­£æ–‡å†…å®¹
    category: str             # åˆ†ç±»
    source: str               # æ¥æº
    keywords: List[str]       # å…³é”®è¯
    created_date: datetime    # åˆ›å»ºæ—¶é—´
    embedding: np.ndarray     # å‘é‡è¡¨ç¤º
    related_ids: List[str]    # å…³è”æ–‡æ¡£ID
```

**æ˜ å°„å…³ç³»æ¨¡å‹**:
```python
@dataclass
class DocumentMapping:
    law_id: str              # æ³•æ¡ID
    case_id: str             # æ¡ˆä¾‹ID
    relation_type: str       # å…³ç³»ç±»å‹
    confidence: float        # ç½®ä¿¡åº¦
    source: str              # æ˜ å°„æ¥æº
```

### 2. æ•°æ®æµè½¬

```
åŸå§‹CSVæ•°æ® â†’ æ•°æ®æ¸…æ´— â†’ ç»Ÿä¸€æ ¼å¼ â†’ å‘é‡åŒ– â†’ ç´¢å¼•æ„å»º â†’ æœåŠ¡éƒ¨ç½²
     â†“            â†“          â†“         â†“         â†“         â†“
   raw_data â†’ clean_data â†’ kb_data â†’ embeddings â†’ index â†’ api
```

**å¤„ç†ç®¡é“**:
```python
class DataPipeline:
    """æ•°æ®å¤„ç†ç®¡é“"""
    
    def __init__(self, config: Config):
        self.config = config
        self.embedder = LegalEmbedder(config.model.name)
    
    def process(self):
        # 1. åŠ è½½åŸå§‹æ•°æ®
        raw_data = self.load_raw_data()
        
        # 2. æ•°æ®æ¸…æ´—
        clean_data = self.clean_data(raw_data)
        
        # 3. æ–‡æœ¬å‘é‡åŒ–
        embeddings = self.embedder.embed_texts(clean_data['content'])
        
        # 4. æ„å»ºç´¢å¼•
        index = self.build_index(embeddings)
        
        # 5. ä¿å­˜ç»“æœ
        self.save_results(clean_data, embeddings, index)
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹ä¼˜åŒ–

**é‡åŒ–åŠ é€Ÿ**:
```python
# æ¨¡å‹é‡åŒ–ï¼ˆæœªæ¥ç‰ˆæœ¬ï¼‰
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)
```

**æ‰¹å¤„ç†ä¼˜åŒ–**:
```python
def batch_embed(texts: List[str], batch_size: int = 32):
    """æ‰¹é‡å‘é‡åŒ–ä¼˜åŒ–"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        batch_embeddings = model.encode(batch_texts)
        embeddings.extend(batch_embeddings)
    return np.array(embeddings)
```

### 2. ç´¢å¼•ä¼˜åŒ–

**åˆ†å±‚ç´¢å¼•**:
```python
# å¤§æ•°æ®é›†ä½¿ç”¨IVFç´¢å¼•
nlist = 100  # èšç±»ä¸­å¿ƒæ•°é‡
quantizer = faiss.IndexFlatL2(768)
index = faiss.IndexIVFFlat(quantizer, 768, nlist)
index.train(embeddings)
```

**GPUåŠ é€Ÿ**:
```python
# GPUç´¢å¼•åŠ é€Ÿ
if torch.cuda.is_available():
    res = faiss.StandardGpuResources()
    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
```

### 3. ç¼“å­˜ç­–ç•¥

**å¤šçº§ç¼“å­˜**:
```python
# 1. å†…å­˜ç¼“å­˜ï¼ˆçƒ­ç‚¹æŸ¥è¯¢ï¼‰
from cachetools import TTLCache
query_cache = TTLCache(maxsize=1000, ttl=3600)

# 2. å‘é‡ç¼“å­˜ï¼ˆè®¡ç®—ç»“æœï¼‰
embedding_cache = {}

# 3. ç»“æœç¼“å­˜ï¼ˆæœ€ç»ˆç»“æœï¼‰
@lru_cache(maxsize=500)
def cached_search(query_hash: str) -> List[Dict]:
    # ç¼“å­˜æœç´¢ç»“æœ
    pass
```

---

## ğŸ” å®‰å…¨æ¶æ„

### 1. æ•°æ®å®‰å…¨

**æ•æ„Ÿä¿¡æ¯ä¿æŠ¤**:
- æ•°æ®è„±æ•å¤„ç†
- è®¿é—®æƒé™æ§åˆ¶
- ä¼ è¾“åŠ å¯†ï¼ˆHTTPSï¼‰
- å­˜å‚¨åŠ å¯†

**é…ç½®å®‰å…¨**:
```python
# ç¯å¢ƒå˜é‡ç®¡ç†
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    API_KEY = os.getenv("API_KEY")
    DATABASE_URL = os.getenv("DATABASE_URL")
    SECRET_KEY = os.getenv("SECRET_KEY")
```

### 2. APIå®‰å…¨

**è¯·æ±‚é™åˆ¶**:
```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/v1/search")
@limiter.limit("100/minute")  # æ¯åˆ†é’Ÿæœ€å¤š100æ¬¡è¯·æ±‚
async def search_endpoint():
    pass
```

---

## ğŸ“ˆ ç›‘æ§å’Œæ—¥å¿—

### 1. ç³»ç»Ÿç›‘æ§

**æ€§èƒ½æŒ‡æ ‡**:
- APIå“åº”æ—¶é—´
- æ¨¡å‹æ¨ç†è€—æ—¶
- ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡
- é”™è¯¯ç‡å’ŒæˆåŠŸç‡

**ç›‘æ§ä»£ç **:
```python
import time
import psutil
from prometheus_client import Counter, Histogram

# æŒ‡æ ‡å®šä¹‰
REQUEST_COUNT = Counter('api_requests_total', 'Total API requests')
REQUEST_LATENCY = Histogram('api_request_duration_seconds', 'Request latency')

@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    REQUEST_COUNT.inc()
    REQUEST_LATENCY.observe(process_time)
    
    return response
```

### 2. æ—¥å¿—ç³»ç»Ÿ

**æ—¥å¿—é…ç½®**:
```python
from loguru import logger

logger.add(
    "logs/api.log",
    rotation="1 day",
    retention="30 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} | {message}",
    compression="zip"
)

# ä½¿ç”¨ç¤ºä¾‹
logger.info("ç”¨æˆ·æŸ¥è¯¢: {}", query)
logger.error("æ¨¡å‹åŠ è½½å¤±è´¥: {}", error)
```

---

## ğŸ”§ éƒ¨ç½²æ¶æ„

### 1. å®¹å™¨åŒ–éƒ¨ç½²

**Dockerfile**:
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY src/ ./src/
COPY config/ ./config/

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨åº”ç”¨
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Docker Compose**:
```yaml
version: '3.8'
services:
  legal-ai:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    environment:
      - LOG_LEVEL=INFO
    restart: unless-stopped
    
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - legal-ai
```

### 2. æ‰©å±•éƒ¨ç½²

**è´Ÿè½½å‡è¡¡**:
- Nginxåå‘ä»£ç†
- å¤šå®ä¾‹éƒ¨ç½²
- å¥åº·æ£€æŸ¥æœºåˆ¶

**è‡ªåŠ¨æ‰©ç¼©å®¹**:
- åŸºäºCPU/å†…å­˜ä½¿ç”¨ç‡
- åŸºäºè¯·æ±‚é˜Ÿåˆ—é•¿åº¦
- Kubernetes HPA

---

## ğŸ“‹ å¼€å‘è·¯çº¿å›¾

### é˜¶æ®µ1: åŸºç¡€æ¶æ„ âœ…
- [x] é¡¹ç›®ç»“æ„æ­å»º
- [x] é…ç½®ç®¡ç†ç³»ç»Ÿ
- [x] åŸºç¡€APIæ¡†æ¶
- [x] æ•°æ®å¤„ç†ç®¡é“

### é˜¶æ®µ2: æ ¸å¿ƒåŠŸèƒ½ ğŸ”„
- [ ] é¢„è®­ç»ƒæ¨¡å‹é›†æˆ
- [ ] å‘é‡æ£€ç´¢å¼•æ“
- [ ] APIæ¥å£å®ç°
- [ ] åŸºç¡€æ€§èƒ½æµ‹è¯•

### é˜¶æ®µ3: æ¨¡å‹ä¼˜åŒ– ğŸ“‹
- [ ] é¢†åŸŸæ•°æ®æ„å»º
- [ ] æ¨¡å‹ç²¾è°ƒè®­ç»ƒ
- [ ] æ€§èƒ½è¯„ä¼°ä½“ç³»
- [ ] æ··åˆæ’åºç®—æ³•

### é˜¶æ®µ4: åŠŸèƒ½å¢å¼º ğŸ“‹
- [ ] çŸ¥è¯†å›¾è°±æ„å»º
- [ ] æ™ºèƒ½è§£é‡ŠåŠŸèƒ½
- [ ] é«˜çº§è¿‡æ»¤å™¨
- [ ] ç”¨æˆ·ç•Œé¢å¼€å‘

### é˜¶æ®µ5: ç”Ÿäº§å°±ç»ª ğŸ“‹
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] å®‰å…¨åŠ å›º
- [ ] ç›‘æ§å‘Šè­¦
- [ ] æ–‡æ¡£å®Œå–„

---

## ğŸ¯ æ¶æ„ä¼˜åŠ¿

1. **å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºåŠŸèƒ½æ‰©å±•
2. **é«˜æ€§èƒ½**: å‘é‡æ£€ç´¢ + ç¼“å­˜ä¼˜åŒ–
3. **å¯ç»´æŠ¤æ€§**: æ¸…æ™°çš„åˆ†å±‚æ¶æ„å’Œä»£ç è§„èŒƒ
4. **çµæ´»æ€§**: é…ç½®é©±åŠ¨ï¼Œæ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼
5. **å¯é æ€§**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œç›‘æ§ä½“ç³»

---

**ğŸ“ è¯´æ˜**: æœ¬æ–‡æ¡£ä¼šéšç€ç³»ç»Ÿå¼€å‘è¿›å±•æŒç»­æ›´æ–°ï¼Œç¡®ä¿æ¶æ„è®¾è®¡ä¸å®é™…å®ç°ä¿æŒä¸€è‡´ã€‚