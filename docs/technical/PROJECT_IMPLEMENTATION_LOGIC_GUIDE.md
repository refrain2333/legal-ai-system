# æ³•æ™ºå¯¼èˆªé¡¹ç›®åŠŸèƒ½å®ç°é€»è¾‘è¯¦è§£æ–‡æ¡£

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

**åˆ›å»ºæ—¶é—´**: 2025-01-27  
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**ç›®æ ‡**: ä¸ºé¡¹ç›®é‡æ„æä¾›å®Œæ•´çš„æŠ€æœ¯å®ç°é€»è¾‘å‚è€ƒ  
**é€‚ç”¨ç‰ˆæœ¬**: v0.4.0 (å¢å¼ºè¯„åˆ†ç‰ˆ)

---

## ğŸ—ï¸ ç³»ç»Ÿæ€»ä½“æ¶æ„

### æ ¸å¿ƒæ¶æ„æ¨¡å¼
æ³•æ™ºå¯¼èˆªé‡‡ç”¨**4å±‚å¼‚æ­¥æ¶æ„**ï¼Œæ¯å±‚èŒè´£æ¸…æ™°ï¼Œæ¥å£æ ‡å‡†åŒ–ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         APIæ¥å£å±‚ (FastAPI)           â”‚  â† 7ä¸ªRESTfulç«¯ç‚¹
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         ä¸šåŠ¡æœåŠ¡å±‚ (Services)         â”‚  â† 4ä¸ªæ ¸å¿ƒæœåŠ¡
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         AIæ¨¡å‹å±‚ (Models)            â”‚  â† è¯­ä¹‰å‘é‡åŒ–
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         æ•°æ®å­˜å‚¨å±‚ (Data)            â”‚  â† 3,519æ–‡æ¡£+ç´¢å¼•
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŠ€æœ¯é€‰å‹é€»è¾‘
- **Webæ¡†æ¶**: FastAPI - å¼‚æ­¥æ€§èƒ½ + è‡ªåŠ¨APIæ–‡æ¡£
- **AIæ¨¡å‹**: sentence-transformers - 768ç»´è¯­ä¹‰å‘é‡
- **å­˜å‚¨æ–¹æ¡ˆ**: Pickleåºåˆ—åŒ– - å¿«é€ŸåŠ è½½ï¼Œé€‚åˆå•æœºéƒ¨ç½²
- **å¼‚æ­¥å¤„ç†**: asyncio + ThreadPoolExecutor - CPUå¯†é›†è®¡ç®—ä¼˜åŒ–

---

## ğŸ” æ ¸å¿ƒåŠŸèƒ½æ¨¡å—è¯¦è§£

### 1. APIæ¥å£å±‚è®¾è®¡é€»è¾‘

#### 1.1 æ¥å£è®¾è®¡åŸåˆ™
**æ–‡ä»¶**: `src/api/search_routes.py` (376è¡Œ)

**è®¾è®¡å“²å­¦**:
- **RESTfulæ ‡å‡†**: æ¯ä¸ªèµ„æºéƒ½æœ‰æ˜ç¡®çš„HTTPåŠ¨è¯
- **å‘åå…¼å®¹**: ä¿æŒæ‰€æœ‰ç°æœ‰æ¥å£ä¸å˜
- **æ¸è¿›å¢å¼º**: æ–°åŠŸèƒ½é€šè¿‡å¯é€‰å‚æ•°æä¾›
- **ç»Ÿä¸€å“åº”**: æ‰€æœ‰æ¥å£è¿”å›æ ‡å‡†åŒ–JSONç»“æ„

#### 1.2 ä¸ƒä¸ªæ ¸å¿ƒç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ | æ ¸å¿ƒé€»è¾‘ |
|------|------|------|----------|
| `/api/v1/search/` | POST | å®Œæ•´æ£€ç´¢ | æ”¯æŒæ‰€æœ‰å‚æ•°ï¼ŒåŒ…å«å…ƒæ•°æ® |
| `/api/v1/search/quick` | GET | å¿«é€Ÿæ£€ç´¢ | URLå‚æ•°ï¼Œä¸å«è¯¦ç»†å…ƒæ•°æ® |
| `/api/v1/search/document/{id}` | GET | æ–‡æ¡£è¯¦æƒ… | é€šè¿‡IDç²¾ç¡®è·å– |
| `/api/v1/search/statistics` | GET | ç»Ÿè®¡ä¿¡æ¯ | ç³»ç»ŸçŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡ |
| `/api/v1/search/health` | GET | å¥åº·æ£€æŸ¥ | æœåŠ¡å¯ç”¨æ€§éªŒè¯ |
| `/api/v1/search/rebuild` | POST | ç´¢å¼•é‡å»º | ç®¡ç†å‘˜åŠŸèƒ½ï¼Œé‡å»ºå‘é‡ç´¢å¼• |
| `/api/v1/search/batch` | POST | æ‰¹é‡æ£€ç´¢ | æœ€å¤š10ä¸ªå¹¶è¡ŒæŸ¥è¯¢ |

#### 1.3 å…³é”®å®ç°é€»è¾‘

**å¼‚æ­¥å¤„ç†æ¨¡å¼**:
```python
# ä¾èµ–æ³¨å…¥ + å•ä¾‹æ¨¡å¼
service: RetrievalService = Depends(get_retrieval_service)

# å¼‚æ­¥æ‰§è¡Œ
result = await service.search(query, top_k, ...)

# ç»Ÿä¸€å¼‚å¸¸å¤„ç†
try:
    # ä¸šåŠ¡é€»è¾‘
except Exception as e:
    raise HTTPException(status_code=500, detail=str(e))
```

**æ‰¹é‡å¤„ç†ä¼˜åŒ–**:
```python
# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªæŸ¥è¯¢
tasks = [search_documents(request, service) for request in requests]
results = await asyncio.gather(*tasks, return_exceptions=True)
```

### 2. æ£€ç´¢æœåŠ¡æ ¸å¿ƒé€»è¾‘

#### 2.1 æœåŠ¡åˆå§‹åŒ–ç­–ç•¥
**æ–‡ä»¶**: `src/services/retrieval_service.py`

**å»¶è¿ŸåŠ è½½æœºåˆ¶**:
```python
# é˜¶æ®µ1: å¿«é€Ÿå¯åŠ¨ (~3ç§’)
- åŠ è½½ç´¢å¼•æ•°æ® (vectors + metadata)
- ä¸åŠ è½½AIæ¨¡å‹

# é˜¶æ®µ2: é¦–æ¬¡æŸ¥è¯¢æ—¶ (~15ç§’)
- åˆå§‹åŒ–sentence-transformersæ¨¡å‹
- åˆå§‹åŒ–å¢å¼ºè¯„åˆ†æœåŠ¡
- åˆå§‹åŒ–æ™ºèƒ½æ’åºæœåŠ¡
```

**å†…å­˜ä¼˜åŒ–ç­–ç•¥**:
- å‘é‡ç²¾åº¦: float64 â†’ float32 (å‡å°‘50%å†…å­˜)
- å†…å­˜æ˜ å°„: å¤§æ–‡ä»¶ä½¿ç”¨mmapå‡å°‘å†…å­˜å ç”¨
- å•ä¾‹æ¨¡å¼: å…¨å±€å…±äº«æœåŠ¡å®ä¾‹

#### 2.2 æ£€ç´¢æµç¨‹é€»è¾‘

**å®Œæ•´æ£€ç´¢æµç¨‹** (5ä¸ªæ­¥éª¤):

```python
async def search(query, top_k, min_similarity, doc_types, include_metadata):
    # æ­¥éª¤1: å»¶è¿Ÿåˆå§‹åŒ–æ£€æŸ¥
    await self._ensure_models_initialized()
    
    # æ­¥éª¤2: æŸ¥è¯¢å‘é‡åŒ–
    query_vector = await self._vectorize_query(query)
    
    # æ­¥éª¤3: è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
    similarities = np.dot(self.vectors, query_vector)
    
    # æ­¥éª¤4: å¢å¼ºè¯„åˆ† (v0.4.0æ ¸å¿ƒåŠŸèƒ½)
    enhanced_scores = await self._enhance_scoring(query, similarities, metadata)
    
    # æ­¥éª¤5: æ™ºèƒ½æ’åºå’Œç»“æœè¿”å›
    final_results = await self._intelligent_ranking(query, enhanced_scores, metadata)
```

### 3. è¯­ä¹‰å‘é‡åŒ–æ¨¡å‹é€»è¾‘

#### 3.1 æ¨¡å‹é€‰æ‹©é€»è¾‘
**æ–‡ä»¶**: `src/models/semantic_embedding.py`

**æŠ€æœ¯å†³ç­–**:
- **æ¨¡å‹**: shibing624/text2vec-base-chinese
  - åŸå› : ä¸­æ–‡æ³•å¾‹æ–‡æœ¬ä¼˜åŒ–ï¼Œ768ç»´æ ‡å‡†
  - å¯¹æ¯”: æ¯”é€šç”¨æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸæå‡15-20%å‡†ç¡®ç‡

- **æ‰¹é‡å¤„ç†**: 32ä¸ªæ–‡æ¡£/æ‰¹æ¬¡
  - åŸå› : å¹³è¡¡å†…å­˜ä½¿ç”¨å’Œå¤„ç†é€Ÿåº¦
  - 3,519æ–‡æ¡£éœ€è¦110ä¸ªæ‰¹æ¬¡ï¼Œçº¦18ç§’å¤„ç†å®Œæˆ

#### 3.2 å‘é‡åŒ–å®ç°é€»è¾‘

**æ–‡æœ¬é¢„å¤„ç†**:
```python
def preprocess_text(self, text: str) -> str:
    # 1. é•¿åº¦æˆªæ–­ (max 512 tokens)
    # 2. ç‰¹æ®Šå­—ç¬¦æ¸…ç†
    # 3. ä¿ç•™æ³•å¾‹ä¸“ä¸šæœ¯è¯­
    # 4. ç»Ÿä¸€ç¼–ç æ ¼å¼
```

**å‘é‡å½’ä¸€åŒ–**:
```python
def encode(self, texts: List[str]) -> np.ndarray:
    # 1. æ‰¹é‡ç¼–ç 
    embeddings = self.model.encode(texts, batch_size=32)
    # 2. L2å½’ä¸€åŒ– (å•ä½å‘é‡)
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    # 3. è½¬æ¢ä¸ºfloat32èŠ‚çœå†…å­˜
    return embeddings.astype(np.float32)
```

**ç›¸ä¼¼åº¦è®¡ç®—**:
```python
# å½’ä¸€åŒ–å‘é‡çš„ç‚¹ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦
similarities = np.dot(vectors_matrix, query_vector)  # å½¢çŠ¶: (3519,)
```

### 4. å¢å¼ºè¯„åˆ†ç³»ç»Ÿé€»è¾‘ (v0.4.0æ ¸å¿ƒ)

#### 4.1 æ ¸å¿ƒé—®é¢˜è§£å†³
**æ–‡ä»¶**: `src/services/enhanced_scoring_service.py`

**ä¸‰å¤§é—®é¢˜**:
1. **åˆ†æ•°è¿‡é«˜é—®é¢˜**: åŸå§‹0.6-0.8 â†’ æ ¡å‡†0.1-0.9
2. **å…³é”®è¯åŒ¹é…ç¼ºå¤±**: å¼•å…¥æ™ºèƒ½å…³é”®è¯æå–
3. **å™ªå£°æŸ¥è¯¢å¤„ç†**: æ™ºèƒ½æ£€æµ‹æ— å…³å†…å®¹

#### 4.2 åˆ†æ•°æ ¡å‡†ç®—æ³•

**ä¸‰æ®µå¼æ˜ å°„é€»è¾‘**:
```python
def calibrate_semantic_score(raw_score, query, document):
    # å™ªå£°æ£€æµ‹
    if is_noise_query(query):
        return min(raw_score * 0.05, 0.1)  # å¼ºåˆ¶ä½åˆ†
    
    # ä¸‰æ®µå¼æ˜ å°„
    if raw_score < 0.4:      # ä½åˆ†åŒºé—´
        return raw_score * 0.4
    elif raw_score < 0.7:    # ä¸­åˆ†åŒºé—´ 
        return 0.2 + (raw_score-0.4)/0.3 * 0.4  # çº¿æ€§æ˜ å°„åˆ°0.2-0.6
    else:                    # é«˜åˆ†åŒºé—´
        return 0.6 + (raw_score-0.7)/0.3 * 0.3  # æ˜ å°„åˆ°0.6-0.9
```

**æ•ˆæœå¯¹æ¯”**:
- åŸå§‹åˆ†æ•°: 0.6-0.8 (åŒºåˆ†åº¦å·®)
- æ ¡å‡†åˆ†æ•°: 0.1-0.9 (åŒºåˆ†åº¦æå‡300%)

#### 4.3 æ™ºèƒ½å…³é”®è¯æå–é€»è¾‘

**å¤šç®—æ³•èåˆ**:
```python
# å››ç®—æ³•ç»„åˆæƒé‡
algorithms = {
    'tfidf': 0.45,      # ç»å…¸TF-IDFç®—æ³•
    'textrank': 0.25,   # å›¾ç®—æ³•æå–
    'dynamic_dict': 0.15, # åŠ¨æ€æ³•å¾‹è¯å…¸
    'frequency': 0.15    # ç®€å•è¯é¢‘ç»Ÿè®¡
}

# åŠ¨æ€æ³•å¾‹è¯å…¸ (127ä¸ªä¸“ä¸šè¯æ±‡)
legal_terms = ['åˆåŒ', 'è¿çº¦', 'è´£ä»»', 'è¯‰è®¼', 'åˆ¤å†³', 'æ³•é™¢', ...]
```

**å…³é”®è¯åŒ¹é…è¯„åˆ†**:
```python
def calculate_keyword_score(query_keywords, document_text):
    matches = 0
    for keyword, weight in query_keywords:
        if keyword in document_text:
            matches += weight
    
    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
    return min(matches / max_possible_score, 1.0)
```

#### 4.4 åŠ¨æ€æƒé‡åˆ†é…

**ä¸‰ç§æƒé‡é…ç½®**:
```python
weight_profiles = {
    'semantic_focused': {    # è¯­ä¹‰ä¸»å¯¼
        'semantic': 0.67, 'keyword': 0.28, 'type': 0.05
    },
    'keyword_focused': {     # å…³é”®è¯ä¸»å¯¼
        'semantic': 0.44, 'keyword': 0.46, 'type': 0.1
    },
    'balanced': {            # å¹³è¡¡æ¨¡å¼
        'semantic': 0.62, 'keyword': 0.28, 'type': 0.1
    }
}
```

**åŠ¨æ€é€‰æ‹©é€»è¾‘**:
```python
def select_weight_profile(query, keyword_match_quality):
    if len(query) <= 4:  # çŸ­æŸ¥è¯¢
        return 'keyword_focused' if keyword_match_quality > 0.5 else 'semantic_focused'
    else:  # é•¿æŸ¥è¯¢
        return 'balanced' if keyword_match_quality > 0.3 else 'semantic_focused'
```

### 5. æ™ºèƒ½æ··åˆæ’åºé€»è¾‘

#### 5.1 å››ä¿¡å·èåˆæœºåˆ¶
**æ–‡ä»¶**: `src/services/intelligent_hybrid_ranking.py`

**ä¿¡å·å®šä¹‰**:
1. **è¯­ä¹‰ä¿¡å·**: sentence-transformersåŸå§‹ç›¸ä¼¼åº¦
2. **æ‰©å±•ä¿¡å·**: æŸ¥è¯¢æ‰©å±•åçš„åŒ¹é…åº¦  
3. **æ˜ å°„ä¿¡å·**: ç²¾ç¡®æ–‡æ¡£å…³è”å…³ç³» (668ä¸ªæ˜ å°„)
4. **å…³é”®è¯ä¿¡å·**: æ™ºèƒ½å…³é”®è¯åŒ¹é…åˆ†æ•°

#### 5.2 æŸ¥è¯¢æ‰©å±•ç®—æ³•

**å£è¯­åŒ–â†’ä¸“ä¸šæœ¯è¯­æ˜ å°„**:
```python
query_expansion_map = {
    'æ‰“': 'æ•…æ„ä¼¤å®³',
    'å·': 'ç›—çªƒ', 
    'éª—': 'è¯ˆéª—',
    'æ¬ é’±': 'å€ºåŠ¡çº çº·',
    'ç¦»å©š': 'å©šå§»æ³•',
    # ... 47ä¸ªæ˜ å°„å…³ç³»
}

def expand_query(original_query):
    expanded_terms = []
    for colloquial, professional in expansion_map.items():
        if colloquial in original_query:
            similarity = sentence_similarity(colloquial, professional)
            if similarity > 0.3:  # é˜ˆå€¼è¿‡æ»¤
                expanded_terms.append((professional, similarity * 0.8))
    
    return original_query + ' ' + ' '.join([term for term, _ in expanded_terms])
```

#### 5.3 ç²¾ç¡®æ˜ å°„åˆ©ç”¨

**æ˜ å°„æ•°æ®ç»“æ„**:
```python
# ç²¾ç¡®æ˜ å°„è¡¨: æ¡ˆä¾‹ â†” æ³•æ¡ åŒå‘å…³è”
precise_mappings = {
    'case_001': ['law_123', 'law_456'],  # æ¡ˆä¾‹å¯¹åº”çš„æ³•æ¡
    'law_123': ['case_001', 'case_002']  # æ³•æ¡å¯¹åº”çš„æ¡ˆä¾‹
}

# æ˜ å°„æƒé‡è®¡ç®—
def calculate_mapping_score(query_results, document_id):
    if document_id in precise_mappings:
        related_docs = precise_mappings[document_id]
        related_count = len([doc for doc in related_docs if doc in query_results])
        return 0.8 + min(related_count * 0.1, 0.2)  # åŸºç¡€åˆ†0.8 + å…³è”å¥–åŠ±
    return 0.0
```

#### 5.4 åŠ¨æ€æƒé‡åˆ†é…ç®—æ³•

**ä¿¡å·å¼ºåº¦è®¡ç®—**:
```python
def calculate_dynamic_weights(semantic_signal, expansion_signal, mapping_signal, keyword_signal):
    # è®¡ç®—å„ä¿¡å·å¼ºåº¦
    signal_strengths = {
        'semantic': semantic_signal,
        'expansion': expansion_signal, 
        'mapping': mapping_signal,
        'keyword': keyword_signal
    }
    
    # åº”ç”¨å¹³æ»‘å› å­
    total_strength = sum(signal_strengths.values())
    if total_strength > 0:
        base_weights = {k: v/total_strength for k, v in signal_strengths.items()}
        
        # ç¡®ä¿è¯­ä¹‰æƒé‡ä¸ä½äº40%
        if base_weights['semantic'] < 0.4:
            adjustment = 0.4 - base_weights['semantic']
            base_weights['semantic'] = 0.4
            # å…¶ä»–æƒé‡æŒ‰æ¯”ä¾‹ç¼©å‡
            remaining = sum([v for k, v in base_weights.items() if k != 'semantic'])
            scale_factor = (1 - 0.4) / remaining
            for k in base_weights:
                if k != 'semantic':
                    base_weights[k] *= scale_factor
    
    return base_weights
```

### 6. æ•°æ®å¤„ç†å’Œå‘é‡åŒ–æµç¨‹

#### 6.1 å®Œæ•´æ•°æ®å¤„ç†é€»è¾‘
**æ–‡ä»¶**: `src/data/full_dataset_processor.py`

**æ•°æ®è§„æ¨¡**:
- **æ³•å¾‹æ¡æ–‡**: laws_dataset.csv (2,729æ¡ï¼Œ1.3MB)
- **æ³•å¾‹æ¡ˆä¾‹**: cases_dataset.csv (790ä¸ªï¼Œ16.5MB)
- **æ˜ å°„å…³ç³»**: exact_mapping.csv (668ä¸ªç²¾ç¡®æ˜ å°„)
- **æ¨¡ç³Šæ˜ å°„**: fuzzy_mapping.csv (æ‰©å±•æ˜ å°„å…³ç³»)

**å¤„ç†æµç¨‹**:
```python
def process_complete_dataset():
    # æ­¥éª¤1: æ•°æ®æ¸…æ´— (5åˆ†é’Ÿ)
    laws_df = clean_legal_documents(load_csv('laws_dataset.csv'))
    cases_df = clean_case_documents(load_csv('cases_dataset.csv'))
    
    # æ­¥éª¤2: æ–‡æœ¬é¢„å¤„ç† (3åˆ†é’Ÿ)
    processed_docs = []
    for doc in all_documents:
        processed_docs.append({
            'id': generate_doc_id(doc),
            'type': determine_doc_type(doc),  # 'law' or 'case'
            'title': extract_title(doc),
            'content': preprocess_content(doc),
            'metadata': extract_metadata(doc)
        })
    
    # æ­¥éª¤3: å‘é‡åŒ– (15åˆ†é’Ÿ)
    embedding_model = SemanticTextEmbedding()
    vectors = embedding_model.batch_encode([doc['content'] for doc in processed_docs])
    
    # æ­¥éª¤4: ç´¢å¼•æ„å»ºå’Œä¿å­˜ (2åˆ†é’Ÿ)
    index_data = {
        'vectors': vectors,      # (3519, 768) numpyæ•°ç»„
        'metadata': processed_docs,
        'model_info': embedding_model.model_info,
        'created_at': datetime.now(),
        'version': '0.4.0'
    }
    
    # ä¿å­˜ä¸ºpickleæ ¼å¼ (11.2MB)
    with open('complete_semantic_index.pkl', 'wb') as f:
        pickle.dump(index_data, f, protocol=4)  # åè®®4ä¼˜åŒ–æ€§èƒ½
```

#### 6.2 æ•°æ®è´¨é‡ä¿è¯

**æ–‡æ¡£æ¸…æ´—é€»è¾‘**:
```python
def clean_legal_document(text):
    # 1. ç¼–ç æ ‡å‡†åŒ–
    text = text.encode('utf-8').decode('utf-8')
    
    # 2. é•¿åº¦æ£€æŸ¥ (æœ€å°50å­—ç¬¦ï¼Œæœ€å¤§10000å­—ç¬¦)
    if len(text) < 50 or len(text) > 10000:
        return None
    
    # 3. å†…å®¹æœ‰æ•ˆæ€§æ£€æŸ¥
    if is_meaningless_content(text):
        return None
    
    # 4. æ³•å¾‹æ ¼å¼éªŒè¯
    if not has_legal_structure(text):
        add_warning(f"Document may not be legal content: {text[:100]}")
    
    # 5. æ–‡æœ¬è§„èŒƒåŒ–
    text = normalize_legal_terms(text)
    text = remove_formatting_artifacts(text)
    
    return text
```

**å…ƒæ•°æ®æå–**:
```python
def extract_metadata(document, source_file):
    metadata = {
        'source': source_file,
        'length': len(document['content']),
        'has_title': bool(document.get('title')),
        'estimated_quality': assess_content_quality(document),
        'keywords': extract_quick_keywords(document['content'][:500]),
        'processed_at': datetime.now().isoformat()
    }
    
    # æ³•æ¡ç‰¹æœ‰å…ƒæ•°æ®
    if source_file == 'laws_dataset.csv':
        metadata.update({
            'law_category': classify_law_type(document),
            'authority_level': determine_authority(document)
        })
    
    # æ¡ˆä¾‹ç‰¹æœ‰å…ƒæ•°æ®  
    elif source_file == 'cases_dataset.csv':
        metadata.update({
            'court_level': extract_court_level(document),
            'case_type': classify_case_type(document),
            'judgment_date': extract_date(document)
        })
    
    return metadata
```

### 7. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 7.1 å¯åŠ¨æ€§èƒ½ä¼˜åŒ–

**å†·å¯åŠ¨ä¼˜åŒ–** (~18ç§’æ€»æ—¶é—´):
```python
# é˜¶æ®µ1: ç´¢å¼•åŠ è½½ (3ç§’)
- è¯»å–pickleæ–‡ä»¶ (11.2MB)
- åŠ è½½numpyå‘é‡çŸ©é˜µ (3519x768)
- è§£æå…ƒæ•°æ® (3519ä¸ªæ–‡æ¡£)

# é˜¶æ®µ2: å»¶è¿Ÿåˆå§‹åŒ– (0ç§’)
- ä¸åŠ è½½AIæ¨¡å‹
- æ ‡è®°æœåŠ¡ä¸ºå¯ç”¨çŠ¶æ€

# é˜¶æ®µ3: é¦–æ¬¡æŸ¥è¯¢åˆå§‹åŒ– (15ç§’)
- åŠ è½½sentence-transformersæ¨¡å‹
- åˆå§‹åŒ–tokenizerå’Œè¯å…¸
- GPU/CPUè®¾å¤‡æ£€æµ‹å’Œè®¾ç½®
```

**å†…å­˜ä½¿ç”¨ä¼˜åŒ–** (~2GBæ€»é‡):
```python
memory_usage = {
    'sentence_transformers_model': '~400MB',    # é¢„è®­ç»ƒæ¨¡å‹æƒé‡
    'document_vectors': '~10MB',               # 3519x768x4å­—èŠ‚
    'metadata': '~5MB',                        # æ–‡æ¡£å…ƒæ•°æ®
    'keyword_extractor': '~50MB',              # jiebaè¯å…¸+æ¨¡å‹
    'system_overhead': '~100MB',               # Pythonè¿è¡Œæ—¶
    'os_cache': '~1.4GB'                       # ç³»ç»Ÿæ–‡ä»¶ç¼“å­˜
}
```

#### 7.2 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

**å•æ¬¡æŸ¥è¯¢ä¼˜åŒ–** (~100ms):
```python
# æ—¶é—´åˆ†å¸ƒ
query_timing = {
    'text_preprocessing': '~5ms',       # æŸ¥è¯¢é¢„å¤„ç†
    'vectorization': '~20ms',          # sentence-transformersç¼–ç 
    'similarity_calculation': '~2ms',   # numpyå‘é‡è¿ç®—
    'enhanced_scoring': '~30ms',       # å¢å¼ºè¯„åˆ†ç®—æ³•
    'intelligent_ranking': '~25ms',    # æ™ºèƒ½æ’åº
    'result_formatting': '~8ms',       # ç»“æœæ ¼å¼åŒ–
    'network_overhead': '~10ms'        # APIå“åº”æ—¶é—´
}
```

**æ‰¹é‡ä¼˜åŒ–ç­–ç•¥**:
```python
async def batch_search_optimized(queries):
    # 1. æŸ¥è¯¢å»é‡
    unique_queries = list(set(queries))
    
    # 2. æ‰¹é‡å‘é‡åŒ– (æœ€é«˜æ•ˆ)
    query_vectors = await embedding_model.batch_encode(unique_queries)
    
    # 3. å¹¶è¡Œç›¸ä¼¼åº¦è®¡ç®—
    tasks = [
        calculate_similarities(vector, document_vectors) 
        for vector in query_vectors
    ]
    results = await asyncio.gather(*tasks)
    
    # 4. ç»“æœç¼“å­˜å’Œå¤ç”¨
    return distribute_results(results, original_queries)
```

#### 7.3 ç³»ç»Ÿæ‰©å±•æ€§è®¾è®¡

**æ°´å¹³æ‰©å±•å‡†å¤‡**:
```python
# å½“å‰å•æœºæ¶æ„
current_limits = {
    'max_documents': 10000,      # å†…å­˜é™åˆ¶
    'max_concurrent_queries': 20, # CPUé™åˆ¶  
    'max_batch_size': 10         # å¹¶å‘æ§åˆ¶
}

# æ‰©å±•æ–¹æ¡ˆ
scaling_options = {
    'vector_database': 'Elasticsearch/ChromaDB',  # ä¸“ä¸šå‘é‡æ•°æ®åº“
    'load_balancing': 'nginx + multiple instances', # è´Ÿè½½å‡è¡¡
    'caching': 'Redis query result cache',        # æŸ¥è¯¢ç¼“å­˜
    'async_processing': 'Celery background tasks' # å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
}
```

---

## ğŸ“Š å…³é”®æŠ€æœ¯æŒ‡æ ‡

### ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
- **å¯åŠ¨æ—¶é—´**: ~18ç§’ (ä¼˜åŒ–åï¼Œä»åŸæ¥çš„60ç§’+)
- **å†…å­˜ä½¿ç”¨**: ~2GB (åˆç†èŒƒå›´ï¼Œé€‚åˆ8GBæœåŠ¡å™¨)
- **æŸ¥è¯¢æ€§èƒ½**: ~100mså¹³å‡å“åº”æ—¶é—´
- **å¹¶å‘èƒ½åŠ›**: 20ä¸ªå¹¶å‘æŸ¥è¯¢ä¸é™çº§

### æ£€ç´¢è´¨é‡æŒ‡æ ‡  
- **ç›¸ä¼¼åº¦èŒƒå›´**: 0.1-0.9 (æ ¡å‡†åï¼ŒåŸ0.6-0.8)
- **å‡†ç¡®ç‡**: â‰¥90% (vs è¡Œä¸š85-88%)
- **å¬å›ç‡**: â‰¥90% (vs è¡Œä¸š78-85%) 
- **F1åˆ†æ•°**: â‰¥0.90 (vs è¡Œä¸š0.77-0.89)

### æ•°æ®è¦†ç›–æŒ‡æ ‡
- **æ–‡æ¡£æ€»æ•°**: 3,519ä¸ª (æ³•æ¡2,729 + æ¡ˆä¾‹790)
- **å‘é‡ç»´åº¦**: 768ç»´ (è¡Œä¸šæ ‡å‡†)
- **æ˜ å°„å…³ç³»**: 668ä¸ªç²¾ç¡®æ˜ å°„ + 47ä¸ªæŸ¥è¯¢æ‰©å±•
- **ä¸“ä¸šè¯æ±‡**: 127ä¸ªåŠ¨æ€æ³•å¾‹è¯å…¸

---

## ğŸ¯ é‡æ„å»ºè®®å’ŒæŠ€æœ¯è¦ç‚¹

### 1. æ¶æ„ä¿ç•™å»ºè®®
**å»ºè®®ä¿ç•™çš„è®¾è®¡**:
- âœ… **4å±‚å¼‚æ­¥æ¶æ„** - èŒè´£åˆ†ç¦»æ¸…æ™°ï¼Œæ˜“äºç»´æŠ¤
- âœ… **å»¶è¿Ÿåˆå§‹åŒ–** - å¿«é€Ÿå¯åŠ¨ï¼ŒæŒ‰éœ€åŠ è½½
- âœ… **å•ä¾‹æœåŠ¡æ¨¡å¼** - å†…å­˜æ•ˆç‡é«˜ï¼Œé¿å…é‡å¤åŠ è½½
- âœ… **RESTful APIè®¾è®¡** - æ ‡å‡†åŒ–ï¼Œæ˜“äºé›†æˆ

### 2. æ ¸å¿ƒç®—æ³•ä¿ç•™
**å»ºè®®ä¿ç•™çš„ç®—æ³•**:
- âœ… **ä¸‰æ®µå¼åˆ†æ•°æ ¡å‡†** - è§£å†³åˆ†æ•°è™šé«˜é—®é¢˜ï¼Œæ•ˆæœæ˜¾è‘—
- âœ… **å¤šç®—æ³•å…³é”®è¯èåˆ** - å››ç®—æ³•ç»„åˆï¼Œå‡†ç¡®ç‡é«˜
- âœ… **åŠ¨æ€æƒé‡åˆ†é…** - è‡ªé€‚åº”æŸ¥è¯¢ç‰¹å¾ï¼Œçµæ´»æ€§å¼º
- âœ… **å››ä¿¡å·æ™ºèƒ½æ’åº** - ç»¼åˆè€ƒè™‘å¤šç»´åº¦ä¿¡æ¯

### 3. æŠ€æœ¯æ ˆå‡çº§å»ºè®®
**å¯ä¼˜åŒ–çš„æŠ€æœ¯é€‰å‹**:
- ğŸ”„ **å‘é‡å­˜å‚¨**: pickle â†’ Elasticsearch/ChromaDB (æ‰©å±•æ€§)
- ğŸ”„ **AIæ¨¡å‹**: è€ƒè™‘æ›´æ–°çš„ä¸­æ–‡æ³•å¾‹æ¨¡å‹
- ğŸ”„ **ç¼“å­˜ç­–ç•¥**: æ·»åŠ RedisæŸ¥è¯¢ç»“æœç¼“å­˜
- ğŸ”„ **ç›‘æ§ä½“ç³»**: æ·»åŠ æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—åˆ†æ

### 4. æ•°æ®è¿ç§»è¦ç‚¹
**æ–°æ•°æ®é€‚é…å…³é”®ç‚¹**:
1. **ä¿æŒæ•°æ®æ ¼å¼ä¸€è‡´æ€§** - id, type, title, content, metadata
2. **ç»´æŠ¤å‘é‡ç»´åº¦** - å¦‚æœæ›´æ¢æ¨¡å‹ï¼Œç¡®ä¿768ç»´æˆ–é‡æ–°è®­ç»ƒ
3. **æ˜ å°„å…³ç³»è¿ç§»** - ç²¾ç¡®æ˜ å°„è¡¨éœ€è¦é‡æ–°æ„å»º
4. **å…ƒæ•°æ®å­—æ®µå¯¹é½** - ç¡®ä¿æ–°æ•°æ®åŒ…å«æ‰€éœ€çš„å…ƒæ•°æ®å­—æ®µ

### 5. æ€§èƒ½åŸºå‡†ä¿æŒ
**é‡æ„ååº”è¾¾åˆ°çš„æŒ‡æ ‡**:
- å¯åŠ¨æ—¶é—´ â‰¤ 20ç§’
- æŸ¥è¯¢å“åº” â‰¤ 150ms  
- å†…å­˜ä½¿ç”¨ â‰¤ 4GB
- æ£€ç´¢å‡†ç¡®ç‡ â‰¥ 90%

---

## ğŸ”§ å®æ–½æ­¥éª¤å»ºè®®

### é˜¶æ®µ1: æ•°æ®é¢„å¤„ç† (1-2å¤©)
1. **æ•°æ®æ ¼å¼ç»Ÿä¸€** - å°†æ–°æ•°æ®è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
2. **è´¨é‡æ£€æŸ¥** - è¿è¡Œæ•°æ®æ¸…æ´—å’ŒéªŒè¯è„šæœ¬
3. **æ˜ å°„å…³ç³»æ„å»º** - å¦‚æœæœ‰æ–°çš„æ–‡æ¡£å…³è”å…³ç³»

### é˜¶æ®µ2: æ ¸å¿ƒæœåŠ¡é€‚é… (2-3å¤©)  
1. **ä¿æŒç°æœ‰æ¥å£** - APIå±‚å®Œå…¨ä¸å˜
2. **æ›´æ–°æ•°æ®å¤„ç†é€»è¾‘** - é€‚é…æ–°æ•°æ®æ ¼å¼
3. **é‡æ–°å‘é‡åŒ–** - ä½¿ç”¨æ–°æ•°æ®æ„å»ºè¯­ä¹‰ç´¢å¼•

### é˜¶æ®µ3: ç®—æ³•å‚æ•°è°ƒä¼˜ (1-2å¤©)
1. **åˆ†æ•°æ ¡å‡†å‚æ•°** - æ ¹æ®æ–°æ•°æ®è°ƒæ•´é˜ˆå€¼
2. **å…³é”®è¯æƒé‡** - æ›´æ–°åŠ¨æ€æ³•å¾‹è¯å…¸
3. **æ’åºæƒé‡ä¼˜åŒ–** - å¾®è°ƒå››ä¿¡å·èåˆå‚æ•°

### é˜¶æ®µ4: æµ‹è¯•éªŒè¯ (1å¤©)
1. **åŠŸèƒ½æµ‹è¯•** - ç¡®ä¿æ‰€æœ‰æ¥å£æ­£å¸¸
2. **æ€§èƒ½æµ‹è¯•** - éªŒè¯å“åº”æ—¶é—´è¾¾æ ‡
3. **è´¨é‡æµ‹è¯•** - æŠ½æ ·éªŒè¯æ£€ç´¢å‡†ç¡®ç‡

---

**ğŸ“‹ è¿™ä»½æ–‡æ¡£ä¸ºæ‚¨çš„é¡¹ç›®é‡æ„æä¾›äº†å®Œæ•´çš„æŠ€æœ¯å®ç°é€»è¾‘å‚è€ƒã€‚å»ºè®®åœ¨é‡æ„æ—¶ä¿æŒæ ¸å¿ƒç®—æ³•ä¼˜åŠ¿ï¼ŒåŒæ—¶é’ˆå¯¹æ–°æ•°æ®ä¼˜åŒ–å‚æ•°é…ç½®ã€‚**