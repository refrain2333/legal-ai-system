#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨
ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
"""

from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import json
import logging

logger = logging.getLogger(__name__)


class EvaluationReporter:
    """è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, results: Dict[str, Any]):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            results: è¯„ä¼°ç»“æœæ•°æ®
        """
        self.results = results
        self.report_lines = []
    
    def generate_text_report(self) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„æŠ¥å‘Š
        
        Returns:
            æŠ¥å‘Šæ–‡æœ¬
        """
        self.report_lines = []
        
        # æ ‡é¢˜
        self._add_header()
        
        # æ¦‚è¦ä¿¡æ¯
        self._add_summary_section()
        
        # è¯¦ç»†æŒ‡æ ‡
        self._add_metrics_section()
        
        # æŒ‰æŸ¥è¯¢ç±»å‹çš„ç»“æœ
        self._add_type_specific_results()
        
        # å»ºè®®å’Œç»“è®º
        self._add_recommendations()
        
        return '\n'.join(self.report_lines)
    
    def _add_header(self):
        """æ·»åŠ æŠ¥å‘Šå¤´éƒ¨"""
        self.report_lines.extend([
            "=" * 80,
            "æ³•å¾‹æœç´¢ç³»ç»Ÿè¯„ä¼°æŠ¥å‘Š",
            "=" * 80,
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"è¯„ä¼°å¼€å§‹: {self.results.get('start_time', 'N/A')}",
            f"è¯„ä¼°ç»“æŸ: {self.results.get('end_time', 'N/A')}",
            f"æ€»è€—æ—¶: {self.results.get('duration_seconds', 0):.2f} ç§’",
            f"æµ‹è¯•æŸ¥è¯¢æ•°: {self.results.get('test_queries_count', 0)}",
            "",
        ])
    
    def _add_summary_section(self):
        """æ·»åŠ æ‘˜è¦éƒ¨åˆ†"""
        summary = self.results.get('summary', {})
        
        self.report_lines.extend([
            "-" * 80,
            "æ‰§è¡Œæ‘˜è¦",
            "-" * 80,
            f"ç»¼åˆå¾—åˆ†: {summary.get('overall_score', 0):.2%}",
            "",
            "å…³é”®æŒ‡æ ‡:",
        ])
        
        for metric, value in summary.get('key_metrics', {}).items():
            self.report_lines.append(f"  - {metric}: {value:.4f}")
        
        if summary.get('strengths'):
            self.report_lines.extend(["", "ä¼˜åŠ¿:"])
            for strength in summary['strengths']:
                self.report_lines.append(f"  + {strength}")
        
        if summary.get('weaknesses'):
            self.report_lines.extend(["", "å¾…æ”¹è¿›:"])
            for weakness in summary['weaknesses']:
                self.report_lines.append(f"  - {weakness}")
        
        self.report_lines.append("")
    
    def _add_metrics_section(self):
        """æ·»åŠ è¯¦ç»†æŒ‡æ ‡éƒ¨åˆ†"""
        metrics = self.results.get('metrics', {})
        overall = metrics.get('overall', {})
        
        self.report_lines.extend([
            "-" * 80,
            "æ€»ä½“æ€§èƒ½æŒ‡æ ‡",
            "-" * 80,
        ])
        
        # æŒ‰Kå€¼ç»„ç»‡æŒ‡æ ‡
        for k in [1, 3, 5, 10]:
            k_metrics = {
                key: value for key, value in overall.items()
                if f'@{k}_' in key
            }
            
            if k_metrics:
                self.report_lines.append(f"\n@{k} æŒ‡æ ‡:")
                for metric_name, value in sorted(k_metrics.items()):
                    clean_name = metric_name.replace(f'@{k}_', ' ').replace('_', ' ').title()
                    self.report_lines.append(f"  {clean_name}: {value:.4f}")
        
        # å…¶ä»–æŒ‡æ ‡
        other_metrics = {
            key: value for key, value in overall.items()
            if '@' not in key and 'semantic' not in key
        }
        
        if other_metrics:
            self.report_lines.append("\nå…¶ä»–æŒ‡æ ‡:")
            for metric_name, value in sorted(other_metrics.items()):
                clean_name = metric_name.replace('_', ' ').title()
                self.report_lines.append(f"  {clean_name}: {value:.4f}")
        
        self.report_lines.append("")
    
    def _add_type_specific_results(self):
        """æ·»åŠ æŒ‰æŸ¥è¯¢ç±»å‹çš„ç»“æœ"""
        metrics_by_type = self.results.get('metrics', {}).get('by_type', {})
        
        self.report_lines.extend([
            "-" * 80,
            "åˆ†ç±»è¯„ä¼°ç»“æœ",
            "-" * 80,
        ])
        
        type_names = {
            'article_to_cases': 'æ³•æ¡â†’æ¡ˆä¾‹æœç´¢',
            'case_to_articles': 'æ¡ˆä¾‹â†’æ³•æ¡æœç´¢',
            'crime_keywords': 'ç½ªåå…³é”®è¯æœç´¢',
            'crime_consistency': 'ç½ªåä¸€è‡´æ€§è¯„ä¼°',
            'mixed': 'æ··åˆæœç´¢'
        }
        
        for query_type, type_metrics in metrics_by_type.items():
            type_name = type_names.get(query_type, query_type)
            self.report_lines.extend([
                "",
                f"ã€{type_name}ã€‘",
                "-" * 40,
            ])
            
            # ç‰¹æ®Šå¤„ç†ç½ªåä¸€è‡´æ€§è¯„ä¼°æ˜¾ç¤º
            if query_type == 'crime_consistency':
                # ä¼˜å…ˆæ˜¾ç¤ºæ¡ˆä¾‹çº§è¦†ç›–ç‡æŒ‡æ ‡
                priority_metrics = ['case_coverage_rate_mean', 'overall_case_coverage_rate', 
                                  'total_covered_cases', 'total_all_cases']
                traditional_metrics = ['consistency_precision_mean', 'consistency_recall_mean', 
                                     'consistency_jaccard_mean']
                
                metric_names = {
                    'case_coverage_rate_mean': 'æ¡ˆä¾‹è¦†ç›–ç‡(å¹³å‡)',
                    'overall_case_coverage_rate': 'æ¡ˆä¾‹è¦†ç›–ç‡(æ•´ä½“)',
                    'total_covered_cases': 'è¦†ç›–æ¡ˆä¾‹æ•°',
                    'total_all_cases': 'æ€»æ¡ˆä¾‹æ•°',
                    'consistency_precision_mean': 'ä¼ ç»Ÿç²¾ç¡®ç‡',
                    'consistency_recall_mean': 'ä¼ ç»Ÿå¬å›ç‡', 
                    'consistency_jaccard_mean': 'Jaccardç³»æ•°'
                }
                
                # æ˜¾ç¤ºæ ¸å¿ƒæŒ‡æ ‡
                self.report_lines.append("  ğŸ¯ æ ¸å¿ƒæŒ‡æ ‡ï¼ˆæ¡ˆä¾‹çº§è¦†ç›–ç‡ï¼‰:")
                for metric in priority_metrics:
                    if metric in type_metrics:
                        clean_name = metric_names.get(metric, metric)
                        value = type_metrics[metric]
                        if 'total_' in metric:
                            self.report_lines.append(f"    {clean_name}: {int(value)}")
                        else:
                            self.report_lines.append(f"    {clean_name}: {value:.4f} ({value*100:.1f}%)")
                
                # æ˜¾ç¤ºä¼ ç»ŸæŒ‡æ ‡ï¼ˆå‚è€ƒï¼‰
                self.report_lines.append("  ğŸ“Š ä¼ ç»ŸæŒ‡æ ‡ï¼ˆå‚è€ƒï¼‰:")
                for metric in traditional_metrics:
                    if metric in type_metrics:
                        clean_name = metric_names.get(metric, metric)
                        value = type_metrics[metric]
                        self.report_lines.append(f"    {clean_name}: {value:.4f} ({value*100:.1f}%)")
                
                # æ˜¾ç¤ºæŸ¥è¯¢æ•°é‡
                if 'total_queries' in type_metrics:
                    queries_count = int(type_metrics['total_queries'])
                    self.report_lines.append(f"  ğŸ“ æµ‹è¯•æŸ¥è¯¢æ•°é‡: {queries_count}")
            else:
                # å…¶ä»–ç±»å‹ä½¿ç”¨é€šç”¨æŒ‡æ ‡å±•ç¤º
                key_metrics = ['precision@5_mean', 'recall@5_mean', 'f1@5_mean',
                              'average_precision_mean', 'semantic_accuracy']
                
                for metric in key_metrics:
                    if metric in type_metrics:
                        clean_name = metric.replace('@5_mean', '').replace('_', ' ').title()
                        value = type_metrics[metric]
                        self.report_lines.append(f"  {clean_name}: {value:.4f}")
        
        self.report_lines.append("")
    
    def _add_recommendations(self):
        """æ·»åŠ å»ºè®®éƒ¨åˆ†"""
        summary = self.results.get('summary', {})
        recommendations = summary.get('recommendations', [])
        
        if recommendations:
            self.report_lines.extend([
                "-" * 80,
                "æ”¹è¿›å»ºè®®",
                "-" * 80,
            ])
            
            for i, rec in enumerate(recommendations, 1):
                self.report_lines.append(f"{i}. {rec}")
        
        self.report_lines.extend([
            "",
            "=" * 80,
            "æŠ¥å‘Šç»“æŸ",
            "=" * 80,
        ])
    
    def save_text_report(self, output_path: Path = None) -> Path:
        """
        ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            import sys
            from pathlib import Path
            # æ·»åŠ evaluationç›®å½•åˆ°è·¯å¾„
            eval_root = Path(__file__).resolve().parent.parent
            if str(eval_root) not in sys.path:
                sys.path.insert(0, str(eval_root))
            from config.eval_settings import RESULTS_DIR
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = RESULTS_DIR / f"evaluation_report_{timestamp}.txt"
        
        report_text = self.generate_text_report()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def generate_json_report(self) -> Dict[str, Any]:
        """
        ç”ŸæˆJSONæ ¼å¼çš„æŠ¥å‘Š
        
        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        return {
            'metadata': {
                'generated_at': datetime.now().isoformat(),
                'evaluation_duration': self.results.get('duration_seconds', 0),
                'total_queries': self.results.get('test_queries_count', 0)
            },
            'summary': self.results.get('summary', {}),
            'metrics': self.results.get('metrics', {}),
            'detailed_results': self._summarize_detailed_results()
        }
    
    def _summarize_detailed_results(self) -> Dict[str, Any]:
        """
        æ±‡æ€»è¯¦ç»†ç»“æœ
        
        Returns:
            æ±‡æ€»çš„è¯¦ç»†ç»“æœ
        """
        detailed = self.results.get('detailed_results', {})
        summary = {}
        
        for query_type, results in detailed.items():
            # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„æŸ¥è¯¢
            success_count = sum(1 for r in results if not r.get('error'))
            error_count = sum(1 for r in results if r.get('error'))
            
            # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
            response_times = [r.get('response_time', 0) for r in results if r.get('response_time')]
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            
            summary[query_type] = {
                'total_queries': len(results),
                'successful': success_count,
                'errors': error_count,
                'avg_response_time': avg_response_time
            }
        
        return summary
    
    def save_json_report(self, output_path: Path = None) -> Path:
        """
        ä¿å­˜JSONæŠ¥å‘Š
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            import sys
            from pathlib import Path
            # æ·»åŠ evaluationç›®å½•åˆ°è·¯å¾„  
            eval_root = Path(__file__).resolve().parent.parent
            if str(eval_root) not in sys.path:
                sys.path.insert(0, str(eval_root))
            from config.eval_settings import RESULTS_DIR
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = RESULTS_DIR / f"evaluation_report_{timestamp}.json"
        
        report_json = self.generate_json_report()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_json, f, ensure_ascii=False, indent=2)
        
        logger.info(f"JSONæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        return output_path