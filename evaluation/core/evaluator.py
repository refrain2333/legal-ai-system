#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»è¯„ä¼°å¼•æ“ - å®Œå–„ç‰ˆ
åè°ƒæ•´ä¸ªè¯„ä¼°æµç¨‹ï¼Œæ”¯æŒç½ªåä¸€è‡´æ€§è¯„ä¼°
"""

import sys
import time
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime
import json

# ===== å…³é”®ä¿®å¤ï¼šåœ¨ä»»ä½•å¯¼å…¥ä¹‹å‰è®¾ç½®è·¯å¾„ =====
def _setup_paths():
    """åœ¨æ¨¡å—çº§åˆ«è®¾ç½®è·¯å¾„"""
    # é¡¹ç›®æ ¹ç›®å½•
    project_root = Path(__file__).resolve().parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    # è¯„ä¼°ç³»ç»Ÿæ ¹ç›®å½•  
    eval_root = Path(__file__).resolve().parent.parent
    if str(eval_root) not in sys.path:
        sys.path.insert(0, str(eval_root))
    
    # éªŒè¯è·¯å¾„
    src_dir = project_root / "src"
    if not src_dir.exists():
        raise ImportError(f"srcç›®å½•ä¸å­˜åœ¨: {src_dir}")

# ç«‹å³è®¾ç½®è·¯å¾„
_setup_paths()

# ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥æ¨¡å—
try:
    from ..core.metrics import MetricsCalculator, SemanticMetrics
    from ..data.test_generator import TestDataGenerator, GroundTruthManager
    from ..data.ground_truth import GroundTruthLoader
    from ..config.eval_settings import EVALUATION_CONFIG, RESULTS_DIR
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    from core.metrics import MetricsCalculator, SemanticMetrics
    from data.test_generator import TestDataGenerator, GroundTruthManager
    from data.ground_truth import GroundTruthLoader
    from config.eval_settings import EVALUATION_CONFIG, RESULTS_DIR

logger = logging.getLogger(__name__)


class LegalSearchEvaluator:
    """æ³•å¾‹æœç´¢ç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self, search_service=None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            search_service: æœç´¢æœåŠ¡å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.search_service = search_service
        self.metrics_calculator = MetricsCalculator()
        self.semantic_metrics = SemanticMetrics()
        self.ground_truth_loader = GroundTruthLoader()
        
        # å¿…é¡»ä½¿ç”¨çœŸå®çš„æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ï¼Œç¦æ­¢å†…ç½®æ¨¡æ‹Ÿç”Ÿæˆå™¨
        try:
            # ä¿®å¤å¯¼å…¥è·¯å¾„
            from evaluation.data.test_generator import TestDataGenerator
            self.test_generator = TestDataGenerator()
            logger.info("æˆåŠŸåŠ è½½çœŸå®TestDataGenerator")
        except Exception as e:
            logger.error(f"TestDataGeneratorå¯¼å…¥å¤±è´¥: {e}")
            # å°è¯•ç›´æ¥å¯¼å…¥
            try:
                sys.path.insert(0, str(Path(__file__).parent.parent))
                from data.test_generator import TestDataGenerator
                self.test_generator = TestDataGenerator()
                logger.info("æˆåŠŸåŠ è½½çœŸå®TestDataGeneratorï¼ˆå¤‡ç”¨è·¯å¾„ï¼‰")
            except Exception as e2:
                logger.error(f"æ‰€æœ‰TestDataGeneratorå¯¼å…¥å°è¯•å‡å¤±è´¥: {e2}")
                raise RuntimeError(f"å¿…é¡»ä½¿ç”¨çœŸå®TestDataGeneratorï¼Œç¦æ­¢æ¨¡æ‹Ÿæ•°æ®: {e2}")
        
        self.evaluation_results = {}
        self.start_time = None
        self.end_time = None
        
        # é…ç½®
        self.config = EVALUATION_CONFIG
        
        # å¦‚æœæ²¡æœ‰æä¾›æœç´¢æœåŠ¡ï¼Œå°è¯•åŠ è½½é»˜è®¤çš„
        if self.search_service is None:
            self._load_search_engine()
    
    def _load_search_engine(self):
        """åŠ è½½æœç´¢å¼•æ“"""
        try:
            # ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
            # evaluation/core/evaluator.py -> evaluation/core -> evaluation -> é¡¹ç›®æ ¹ç›®å½•
            current_file = Path(__file__).resolve()
            evaluation_dir = current_file.parent.parent  # evaluationç›®å½•
            project_root = evaluation_dir.parent         # é¡¹ç›®æ ¹ç›®å½•
            
            logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
            
            if str(project_root) not in sys.path:
                sys.path.insert(0, str(project_root))
                logger.info(f"æ·»åŠ åˆ°sys.path: {project_root}")
            
            # éªŒè¯srcç›®å½•å­˜åœ¨
            src_dir = project_root / "src"
            if not src_dir.exists():
                raise ImportError(f"srcç›®å½•ä¸å­˜åœ¨: {src_dir}")
            
            logger.info(f"srcç›®å½•éªŒè¯é€šè¿‡: {src_dir}")
            
            # æ¸…é™¤å¯èƒ½çš„æ¨¡å—ç¼“å­˜å†²çª
            modules_to_clear = []
            for name in list(sys.modules.keys()):
                if name.startswith('src.') or (name == 'src' and not hasattr(sys.modules[name], '__file__')):
                    modules_to_clear.append(name)
            
            for module_name in modules_to_clear:
                del sys.modules[module_name]
                logger.debug(f"æ¸…é™¤æ¨¡å—ç¼“å­˜: {module_name}")
            
            logger.info(f"æ¸…é™¤äº† {len(modules_to_clear)} ä¸ªæ¨¡å—ç¼“å­˜")
            
            # å¯¼å…¥æœç´¢å¼•æ“
            logger.info("å°è¯•å¯¼å…¥æœç´¢å¼•æ“...")
            logger.info(f"å½“å‰sys.pathå‰3é¡¹: {sys.path[:3]}")
            
            # æ£€æŸ¥srcæ¨¡å—çŠ¶æ€
            if 'src' in sys.modules:
                logger.info(f"srcæ¨¡å—å·²åœ¨ç¼“å­˜ä¸­: {sys.modules['src']}")
            else:
                logger.info("srcæ¨¡å—ä¸åœ¨ç¼“å­˜ä¸­")
            
            # åˆ†æ­¥å¯¼å…¥ï¼Œä¾¿äºè°ƒè¯•
            try:
                import src
                logger.info(f"âœ“ æˆåŠŸå¯¼å…¥src: {src}")
            except Exception as e:
                logger.error(f"âœ— å¯¼å…¥srcå¤±è´¥: {e}")
                raise
            
            try:
                import src.infrastructure
                logger.info(f"âœ“ æˆåŠŸå¯¼å…¥src.infrastructure: {src.infrastructure}")
            except Exception as e:
                logger.error(f"âœ— å¯¼å…¥src.infrastructureå¤±è´¥: {e}")
                raise
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸å®é™…é¡¹ç›®ç›¸åŒçš„æœç´¢æœåŠ¡è·¯å¾„
            from src.services.search_service import SearchService
            from src.infrastructure.repositories import get_legal_document_repository
            
            logger.info("åˆå§‹åŒ–æœç´¢æœåŠ¡ï¼ˆä¸å®é™…é¡¹ç›®ç›¸åŒè·¯å¾„ï¼‰...")
            repository = get_legal_document_repository()
            self.search_service = SearchService(repository)
            logger.info("æœç´¢æœåŠ¡å‡†å¤‡å°±ç»ª")
            
        except Exception as e:
            logger.error(f"åŠ è½½æœç´¢å¼•æ“å¤±è´¥: {e}")
            logger.error("è¯„ä¼°ç³»ç»Ÿå¿…é¡»ä½¿ç”¨çœŸå®æœç´¢å¼•æ“ï¼Œæ— æ³•ç»§ç»­")
            raise RuntimeError(f"æ— æ³•åŠ è½½çœŸå®æœç´¢å¼•æ“: {e}. è¯·æ£€æŸ¥é¡¹ç›®ç¯å¢ƒå’Œä¾èµ–é…ç½®.")
    
    
    def _create_builtin_test_generator(self):
        """åˆ›å»ºå†…ç½®çš„æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""
        class BuiltinTestGenerator:
            def __init__(self):
                self.loaded = False
            
            def load_data(self) -> bool:
                """æ¨¡æ‹Ÿæ•°æ®åŠ è½½"""
                self.loaded = True
                return True
            
            def generate_all_test_queries(self) -> Dict[str, List[Dict]]:
                """ç”Ÿæˆæ‰€æœ‰æµ‹è¯•æŸ¥è¯¢"""
                test_queries = {
                    'article_to_cases': [],
                    'case_to_articles': [],
                    'crime_consistency': []
                }
                
                # ç”Ÿæˆæ³•æ¡åˆ°æ¡ˆä¾‹çš„æµ‹è¯•æŸ¥è¯¢
                for i in range(1, 6):  # 5ä¸ªæµ‹è¯•æŸ¥è¯¢
                    article_num = 100 + i
                    test_queries['article_to_cases'].append({
                        'query_id': f'article_{article_num}',
                        'query_type': 'article_to_cases',
                        'query_text': f'æ¨¡æ‹Ÿæ³•æ¡å†…å®¹ {article_num}',
                        'article_number': article_num
                    })
                
                # ç”Ÿæˆæ¡ˆä¾‹åˆ°æ³•æ¡çš„æµ‹è¯•æŸ¥è¯¢
                for i in range(1, 6):  # 5ä¸ªæµ‹è¯•æŸ¥è¯¢
                    case_id = f'case_{i:03d}'
                    test_queries['case_to_articles'].append({
                        'query_id': f'case_{case_id}',
                        'query_type': 'case_to_articles',
                        'query_text': f'æ¨¡æ‹Ÿæ¡ˆä¾‹äº‹å® {i}',
                        'case_id': case_id
                    })
                
                # ç”Ÿæˆç½ªåä¸€è‡´æ€§æµ‹è¯•æŸ¥è¯¢
                crime_names = ['ç›—çªƒç½ª', 'æ•…æ„ä¼¤å®³ç½ª', 'è¯ˆéª—ç½ª', 'æŠ¢åŠ«ç½ª', 'æ¯’å“çŠ¯ç½ª']
                for i, crime in enumerate(crime_names):
                    test_queries['crime_consistency'].append({
                        'query_id': f'crime_{i+1}',
                        'query_type': 'crime_consistency',
                        'query_text': crime,
                        'crime_name': crime
                    })
                
                return test_queries
        
        return BuiltinTestGenerator()
    
    def _create_builtin_ground_truth_manager(self):
        """åˆ›å»ºå†…ç½®çš„Ground Truthç®¡ç†å™¨"""
        class BuiltinGroundTruthManager:
            def __init__(self, test_queries):
                self.test_queries = test_queries
            
            def get_ground_truth(self, query_id: str) -> Dict[str, Any]:
                """è·å–æŸ¥è¯¢çš„Ground Truth"""
                # åŸºäºæŸ¥è¯¢IDç”Ÿæˆæ¨¡æ‹Ÿçš„Ground Truth
                if query_id.startswith('article_'):
                    article_num = int(query_id.replace('article_', ''))
                    return {
                        'ground_truth_cases': [f'case_{i:03d}' for i in range(1, 4)]  # æ¨¡æ‹Ÿ3ä¸ªç›¸å…³æ¡ˆä¾‹
                    }
                elif query_id.startswith('case_'):
                    return {
                        'ground_truth_articles': [101, 102, 103]  # æ¨¡æ‹Ÿ3ä¸ªç›¸å…³æ³•æ¡
                    }
                else:
                    return {
                        'ground_truth_cases': [f'case_{i:03d}' for i in range(1, 3)],
                        'ground_truth_articles': [101, 102]
                    }
        
        return BuiltinGroundTruthManager
    
    def _generate_correct_test_queries(self) -> Dict[str, List[Dict]]:
        """åŸºäºçœŸå®æ•°æ®å…³è”ç”Ÿæˆæ­£ç¡®çš„æµ‹è¯•æŸ¥è¯¢ - é¿å…å¾ªç¯ä¾èµ–"""
        test_queries = {
            'article_to_cases': [],
            'case_to_articles': [],
            'crime_consistency': []
        }
        
        sample_size = self.config.get('test_sample_size', 5)
        
        # 1. ç”Ÿæˆæ³•æ¡åˆ°æ¡ˆä¾‹çš„æŸ¥è¯¢ - ä½¿ç”¨çœŸå®çš„æ¡ˆä¾‹å…³è”æ•°æ®
        article_case_mapping = self.ground_truth_loader.article_case_mapping
        if article_case_mapping:
            import random
            # é€‰æ‹©æœ‰å…³è”æ¡ˆä¾‹çš„æ³•æ¡
            articles_with_cases = [art for art, cases in article_case_mapping.items() if cases]
            selected_articles = random.sample(articles_with_cases, min(sample_size, len(articles_with_cases)))
            
            for article_num in selected_articles:
                # è·å–æ³•æ¡å†…å®¹
                article_data = self.ground_truth_loader.articles_dict.get(article_num, {})
                query_text = article_data.get('content', f'ç¬¬{article_num}æ¡')[:200]
                
                # ä½¿ç”¨çœŸå®çš„æ¡ˆä¾‹å…³è”ä½œä¸ºGround Truth
                ground_truth_cases = article_case_mapping[article_num]
                
                test_queries['article_to_cases'].append({
                    'query_id': f'article_{article_num}',
                    'query_type': 'article_to_cases',
                    'query_text': query_text,
                    'article_number': article_num,
                    'ground_truth_cases': ground_truth_cases,
                    'metadata': {
                        'title': article_data.get('title', ''),
                        'chapter': article_data.get('chapter', '')
                    }
                })
        
        # 2. ç”Ÿæˆæ¡ˆä¾‹åˆ°æ³•æ¡çš„æŸ¥è¯¢ - ä½¿ç”¨æ¡ˆä¾‹ä¸­çš„relevant_articles
        case_article_mapping = self.ground_truth_loader.case_article_mapping
        if case_article_mapping:
            import random
            # é€‰æ‹©æœ‰å…³è”æ³•æ¡çš„æ¡ˆä¾‹
            cases_with_articles = [case for case, articles in case_article_mapping.items() if articles]
            selected_cases = random.sample(cases_with_articles, min(sample_size, len(cases_with_articles)))
            
            for case_id in selected_cases:
                # è·å–æ¡ˆä¾‹å†…å®¹
                case_data = self.ground_truth_loader.cases_dict.get(case_id, {})
                query_text = case_data.get('fact', f'æ¡ˆä¾‹{case_id}çš„äº‹å®')[:200]
                
                # ä½¿ç”¨æ¡ˆä¾‹ä¸­é¢„å­˜çš„relevant_articlesä½œä¸ºGround Truth
                ground_truth_articles = case_article_mapping[case_id]
                
                test_queries['case_to_articles'].append({
                    'query_id': f'case_{case_id}',
                    'query_type': 'case_to_articles',
                    'query_text': query_text,
                    'case_id': case_id,
                    'ground_truth_articles': ground_truth_articles,
                    'metadata': {
                        'accusations': case_data.get('accusations', []),
                        'criminals': case_data.get('criminals', [])
                    }
                })
        
        # 3. ç”Ÿæˆç½ªåä¸€è‡´æ€§æŸ¥è¯¢ - å¿…é¡»ä»crime.txtæ–‡ä»¶è¯»å–ï¼Œæ— fallbackæœºåˆ¶
        try:
            from ..config.eval_settings import CRIME_TYPES_PATH
        except ImportError:
            # å¤„ç†ç›´æ¥è¿è¡Œæ—¶çš„å¯¼å…¥é—®é¢˜
            from config.eval_settings import CRIME_TYPES_PATH
        
        if not CRIME_TYPES_PATH.exists():
            raise FileNotFoundError(f"ç½ªåæ–‡ä»¶ä¸å­˜åœ¨: {CRIME_TYPES_PATH}ï¼Œç½ªåä¸€è‡´æ€§è¯„ä¼°æ— æ³•è¿›è¡Œ")
        
        with open(CRIME_TYPES_PATH, 'r', encoding='utf-8') as f:
            all_crimes = [line.strip() for line in f if line.strip()]
        
        if not all_crimes:
            raise ValueError(f"ç½ªåæ–‡ä»¶ä¸ºç©º: {CRIME_TYPES_PATH}ï¼Œç½ªåä¸€è‡´æ€§è¯„ä¼°æ— æ³•è¿›è¡Œ")
        
        import random
        consistency_sample_size = self.config.get('crime_consistency_sample_size', 20)
        
        if len(all_crimes) < consistency_sample_size:
            raise ValueError(f"ç½ªåæ–‡ä»¶ä¸­çš„ç½ªåæ•°é‡({len(all_crimes)})å°‘äºæ‰€éœ€æ ·æœ¬æ•°é‡({consistency_sample_size})")
        
        selected_crimes = random.sample(all_crimes, consistency_sample_size)
        
        for i, crime in enumerate(selected_crimes):
            test_queries['crime_consistency'].append({
                'query_id': f'consistency_{i+1:02d}',
                'query_type': 'crime_consistency',
                'query_text': crime,
                'crime_name': crime
            })
            
        logger.info(f"æˆåŠŸä»{CRIME_TYPES_PATH}åŠ è½½{len(selected_crimes)}ä¸ªç½ªåç”¨äºä¸€è‡´æ€§è¯„ä¼°")
        
        logger.info(f"ç”Ÿæˆæ­£ç¡®æµ‹è¯•æŸ¥è¯¢: æ³•æ¡â†’æ¡ˆä¾‹({len(test_queries['article_to_cases'])}ä¸ª), "
                   f"æ¡ˆä¾‹â†’æ³•æ¡({len(test_queries['case_to_articles'])}ä¸ª), "
                   f"ç½ªåä¸€è‡´æ€§({len(test_queries['crime_consistency'])}ä¸ª)")
        
        return test_queries
    
    def _create_simple_ground_truth_manager(self, test_queries):
        """åˆ›å»ºç®€å•çš„Ground Truthç®¡ç†å™¨ï¼Œç›´æ¥ä½¿ç”¨æŸ¥è¯¢ä¸­çš„Ground Truthæ•°æ®"""
        class SimpleGroundTruthManager:
            def __init__(self, test_queries):
                self.ground_truth_index = {}
                self._build_index(test_queries)
            
            def _build_index(self, test_queries):
                for query_type, queries in test_queries.items():
                    for query in queries:
                        query_id = query['query_id']
                        self.ground_truth_index[query_id] = {
                            'type': query_type,
                            'ground_truth_cases': query.get('ground_truth_cases', []),
                            'ground_truth_articles': query.get('ground_truth_articles', []),
                            'metadata': query.get('metadata', {})
                        }
            
            def get_ground_truth(self, query_id: str) -> Dict[str, Any]:
                return self.ground_truth_index.get(query_id, {})
        
        return SimpleGroundTruthManager(test_queries)
    
    async def evaluate(self, test_queries: Optional[Dict[str, List[Dict]]] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹
        
        Args:
            test_queries: æµ‹è¯•æŸ¥è¯¢é›†ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        self.start_time = datetime.now()
        logger.info(f"å¼€å§‹è¯„ä¼°: {self.start_time}")
        
        try:
            # 1. å‡†å¤‡æ•°æ®
            logger.info("=== æ­¥éª¤1: å‡†å¤‡è¯„ä¼°æ•°æ® ===")
            if not await self._prepare_data(test_queries):
                raise Exception("æ•°æ®å‡†å¤‡å¤±è´¥")
            
            # 2. æ‰§è¡Œè¯„ä¼°
            logger.info("=== æ­¥éª¤2: æ‰§è¡Œæœç´¢è¯„ä¼° ===")
            evaluation_results = await self._run_evaluation()
            
            # 3. è®¡ç®—æŒ‡æ ‡
            logger.info("=== æ­¥éª¤3: è®¡ç®—è¯„ä¼°æŒ‡æ ‡ ===")
            metrics_results = self._calculate_metrics(evaluation_results)
            
            # 4. ç”Ÿæˆæ±‡æ€»
            logger.info("=== æ­¥éª¤4: ç”Ÿæˆè¯„ä¼°æ±‡æ€» ===")
            summary = self._generate_summary(metrics_results)
            
            self.end_time = datetime.now()
            duration = (self.end_time - self.start_time).total_seconds()
            
            # ç»„åˆæœ€ç»ˆç»“æœ
            self.evaluation_results = {
                'start_time': self.start_time.isoformat(),
                'end_time': self.end_time.isoformat(),
                'duration_seconds': duration,
                'test_queries_count': sum(len(q) for q in self.test_queries.values()),
                'detailed_results': evaluation_results,
                'metrics': metrics_results,
                'summary': summary
            }
            
            logger.info(f"è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return self.evaluation_results
            
        except Exception as e:
            logger.error(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _prepare_data(self, test_queries: Optional[Dict] = None) -> bool:
        """
        å‡†å¤‡è¯„ä¼°æ•°æ®
        
        Args:
            test_queries: æµ‹è¯•æŸ¥è¯¢é›†
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # åŠ è½½Ground Truthæ•°æ®
            if not self.ground_truth_loader.load():
                logger.error("åŠ è½½Ground Truthæ•°æ®å¤±è´¥")
                return False
            
            # ç”Ÿæˆæˆ–ä½¿ç”¨æä¾›çš„æµ‹è¯•æŸ¥è¯¢
            if test_queries is None:
                logger.info("ç”Ÿæˆæµ‹è¯•æŸ¥è¯¢é›†...")
                # ä½¿ç”¨æ­£ç¡®çš„Ground Truthæ•°æ®ç”Ÿæˆæµ‹è¯•æŸ¥è¯¢ï¼Œé¿å…å¾ªç¯ä¾èµ–
                self.test_queries = self._generate_correct_test_queries()
            else:
                self.test_queries = test_queries
            
            # åˆ›å»ºGround Truthç®¡ç†å™¨
            self.ground_truth_manager = self._create_simple_ground_truth_manager(self.test_queries)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = self.ground_truth_loader.get_statistics()
            logger.info(f"æ•°æ®ç»Ÿè®¡: {stats}")
            
            return True
            
        except Exception as e:
            logger.error(f"å‡†å¤‡æ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    async def _run_evaluation(self) -> Dict[str, List[Dict]]:
        """
        æ‰§è¡Œè¯„ä¼°æµ‹è¯•
        
        Returns:
            è¯„ä¼°ç»“æœ
        """
        all_results = {}
        
        for query_type, queries in self.test_queries.items():
            logger.info(f"è¯„ä¼° {query_type} ç±»å‹æŸ¥è¯¢ ({len(queries)} ä¸ª)...")
            
            results = []
            for i, query in enumerate(queries):
                if i % 10 == 0:
                    logger.info(f"  è¿›åº¦: {i}/{len(queries)}")
                
                # æ‰§è¡Œæœç´¢
                search_result = await self._execute_single_search(query)
                
                # è¯„ä¼°ç»“æœ
                evaluated_result = self._evaluate_single_result(query, search_result)
                results.append(evaluated_result)
            
            all_results[query_type] = results
            logger.info(f"å®Œæˆ {query_type} è¯„ä¼°")
        
        return all_results
    
    async def _execute_single_search(self, query: Dict) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªæœç´¢æŸ¥è¯¢ - ä½¿ç”¨ä¸å®é™…é¡¹ç›®ç›¸åŒçš„æœç´¢æœåŠ¡
        
        Args:
            query: æŸ¥è¯¢æ•°æ®
            
        Returns:
            æœç´¢ç»“æœ
        """
        try:
            query_text = query.get('query_text', '')
            query_type = query.get('query_type', '')
            
            # ğŸ” æ˜¾ç¤ºæ¯æ¬¡æœç´¢çš„å…³é”®è¯ï¼ˆç”¨æˆ·è¦æ±‚åœ¨æ§åˆ¶å°æŸ¥çœ‹æ‰€æœ‰æœç´¢æ“ä½œï¼‰
            print("=" * 80)
            print(f"ğŸ” æœç´¢æ“ä½œè¯¦æƒ…:")
            print(f"æŸ¥è¯¢ID: {query.get('query_id', 'N/A')}")
            print(f"æŸ¥è¯¢ç±»å‹: {query_type}")
            if query.get('article_number'):
                print(f"æ³•æ¡ç¼–å·: {query.get('article_number')}")
            print(f"æœç´¢å…³é”®è¯: '{query_text}'")
            print(f"å…³é”®è¯é•¿åº¦: {len(query_text)} å­—ç¬¦")
            print("=" * 80)
            
            # åŒæ—¶è®°å½•åˆ°æ—¥å¿—
            logger.info("=" * 80)
            logger.info(f"ğŸ” æœç´¢æ“ä½œè¯¦æƒ…:")
            logger.info(f"æŸ¥è¯¢ID: {query.get('query_id', 'N/A')}")
            logger.info(f"æŸ¥è¯¢ç±»å‹: {query_type}")
            if query.get('article_number'):
                logger.info(f"æ³•æ¡ç¼–å·: {query.get('article_number')}")
            logger.info(f"æœç´¢å…³é”®è¯: '{query_text}'")
            logger.info(f"å…³é”®è¯é•¿åº¦: {len(query_text)} å­—ç¬¦")
            logger.info("=" * 80)
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸å®é™…é¡¹ç›®ç›¸åŒçš„æœç´¢æœåŠ¡è°ƒç”¨æ–¹å¼
            if query_type == 'article_to_cases':
                # æ³•æ¡åˆ°æ¡ˆä¾‹ï¼šä½¿ç”¨æ··åˆæœç´¢ï¼Œåªå–æ¡ˆä¾‹éƒ¨åˆ†
                service_result = await self.search_service.search_documents_mixed(
                    query_text=query_text,
                    articles_count=0,  # ä¸è¦æ³•æ¡
                    cases_count=20     # åªè¦æ¡ˆä¾‹
                )
                if service_result.get('success'):
                    search_results = service_result.get('cases', [])
                else:
                    search_results = []
                    
            elif query_type == 'case_to_articles':
                # æ¡ˆä¾‹åˆ°æ³•æ¡ï¼šä½¿ç”¨æ··åˆæœç´¢ï¼Œåªå–æ³•æ¡éƒ¨åˆ†
                service_result = await self.search_service.search_documents_mixed(
                    query_text=query_text,
                    articles_count=5,   # åªè¦å‰5ä¸ªæ³•æ¡
                    cases_count=0       # ä¸è¦æ¡ˆä¾‹
                )
                if service_result.get('success'):
                    search_results = service_result.get('articles', [])
                else:
                    search_results = []
                    
            elif query_type == 'crime_consistency':
                # ç½ªåä¸€è‡´æ€§è¯„ä¼° - ä½¿ç”¨ä¸“ç”¨æœç´¢ï¼ˆ3æ³•æ¡+5-10æ¡ˆä¾‹ï¼‰
                service_result = await self.search_service.search_documents_for_crime_consistency(
                    query_text=query_text
                )
                if service_result.get('success'):
                    # ä¿æŒåŸå§‹ç»“æ„ï¼Œç”¨äºä¸€è‡´æ€§åˆ†æ
                    search_results = {
                        'articles': service_result.get('articles', []),
                        'cases': service_result.get('cases', [])
                    }
                else:
                    search_results = {'articles': [], 'cases': []}
            else:
                # æ··åˆæœç´¢
                service_result = await self.search_service.search_documents_mixed(
                    query_text=query_text,
                    articles_count=10,
                    cases_count=10
                )
                if service_result.get('success'):
                    search_results = (service_result.get('articles', []) + 
                                    service_result.get('cases', []))
                else:
                    search_results = []
            
            return {
                'query_id': query.get('query_id'),
                'search_results': search_results,
                'response_time': time.time()  # ç®€åŒ–çš„å“åº”æ—¶é—´
            }
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return {
                'query_id': query.get('query_id'),
                'search_results': [],
                'error': str(e)
            }
    
    def _evaluate_single_result(self, query: Dict, search_result: Dict) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæœç´¢ç»“æœ - ä½¿ç”¨æ­£ç¡®çš„Ground TruthéªŒè¯é€»è¾‘
        
        Args:
            query: æŸ¥è¯¢æ•°æ®
            search_result: æœç´¢ç»“æœ
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        query_type = query.get('query_type')
        search_results = search_result.get('search_results', [])
        
        # ç‰¹æ®Šå¤„ç†ç½ªåä¸€è‡´æ€§è¯„ä¼°
        if query_type == 'crime_consistency':
            consistency_metrics = self.semantic_metrics.crime_consistency_metrics(search_results)
            return {
                'query_id': query.get('query_id'),
                'query_type': query_type,
                'query_text': query.get('query_text'),
                'consistency_metrics': consistency_metrics,
                'search_result_summary': {
                    'articles_count': len(search_results.get('articles', [])) if isinstance(search_results, dict) else 0,
                    'cases_count': len(search_results.get('cases', [])) if isinstance(search_results, dict) else 0
                }
            }
        
        # æå–æ£€ç´¢åˆ°çš„æ–‡æ¡£ID
        retrieved_ids = []
        relevant_ids = []  # ğŸ”§ ä¿®å¤ï¼šæå‰åˆå§‹åŒ–relevant_ids
        relevance_scores = {}
        
        # è°ƒè¯•ä¿¡æ¯
        logger.info(f"å¤„ç†æŸ¥è¯¢ç±»å‹: {query_type}")
        logger.info(f"æœç´¢ç»“æœæ•°é‡: {len(search_results) if isinstance(search_results, list) else 'Not a list'}")
        logger.info(f"æœç´¢ç»“æœç±»å‹: {type(search_results)}")
        
        # ğŸ”„ ä¼˜åŒ–è¯„ä¼°é€»è¾‘ï¼šæ³•æ¡åˆ°æ¡ˆä¾‹æ£€ç´¢è¯„ä¼°
        if query_type == 'article_to_cases':
            query_article_num = query.get('article_number')
            if query_article_num is None:
                logger.warning("æŸ¥è¯¢ä¸­ç¼ºå°‘article_numberå­—æ®µ")
                retrieved_ids = []
                relevant_ids = []
            else:
                print(f"\nğŸ“Š æ³•æ¡åˆ°æ¡ˆä¾‹æ£€ç´¢è¯„ä¼° - ç¬¬{query_article_num}æ¡")
                print(f"æœç´¢ç›®æ ‡ï¼šåœ¨20ä¸ªæœç´¢ç»“æœä¸­æ‰¾åˆ°relevant_articlesåŒ…å«{query_article_num}çš„æ¡ˆä¾‹")
                print("-" * 60)
                
                relevant_count = 0
                for i, result in enumerate(search_results[:20]):  # åªè¯„ä¼°å‰20ä¸ª
                    case_relevant_articles = result.get('relevant_articles', [])
                    case_id = result.get('case_id') or result.get('id')
                    similarity = result.get('similarity', result.get('score', 0.0))
                    accusations = result.get('accusations', [])

                    # ç¡®ä¿case_idæ ¼å¼ç»Ÿä¸€
                    if case_id and not str(case_id).startswith('case_'):
                        case_id = f"case_{case_id}"

                    if case_id:
                        retrieved_ids.append(case_id)
                        relevance_scores[case_id] = similarity
                        
                        # æ£€æŸ¥æ˜¯å¦ç›¸å…³
                        is_relevant = isinstance(case_relevant_articles, list) and query_article_num in case_relevant_articles
                        if is_relevant:
                            relevant_ids.append(case_id)
                            relevant_count += 1
                            status = "âœ… ç›¸å…³"
                        else:
                            status = "âŒ ä¸ç›¸å…³"
                        
                        print(f"æ¡ˆä¾‹{i+1:2d}: {case_id} | ç›¸ä¼¼åº¦:{similarity:.4f} | {status}")
                        print(f"        ç½ªå: {accusations}")
                        print(f"        å¼•ç”¨æ³•æ¡: {case_relevant_articles}")

                print("-" * 60)
                print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ:")
                print(f"  - æœç´¢åˆ°æ¡ˆä¾‹æ•°: {len(retrieved_ids)}")
                print(f"  - ç›¸å…³æ¡ˆä¾‹æ•°: {relevant_count}")
                print(f"  - åŒ¹é…ç‡: {relevant_count/len(retrieved_ids)*100:.1f}%" if retrieved_ids else "0%")
                logger.info(f"æ³•æ¡{query_article_num}è¯„ä¼°ï¼šæœç´¢{len(retrieved_ids)}ä¸ªæ¡ˆä¾‹ï¼Œæ‰¾åˆ°{relevant_count}ä¸ªç›¸å…³æ¡ˆä¾‹")
                
        elif query_type == 'case_to_articles':
            # ä¼˜åŒ–æ¡ˆä¾‹åˆ°æ³•æ¡è¯„ä¼°ï¼šåªè¦åŒ…å«çœŸå®ç­”æ¡ˆå°±æ»¡åˆ†ï¼Œå¯ä»¥å¤šä¸å¯ä»¥å°‘
            case_relevant_articles = query.get('ground_truth_articles', [])
            case_id = query.get('case_id', query.get('query_id', 'æœªçŸ¥æ¡ˆä¾‹'))
            
            print(f"\nğŸ“Š æ¡ˆä¾‹åˆ°æ³•æ¡æ£€ç´¢è¯„ä¼° - {case_id}")
            print(f"çœŸå®ç­”æ¡ˆï¼š{case_relevant_articles}")
            print(f"è¯„ä¼°æ ‡å‡†ï¼šåœ¨å‰5ä¸ªæ³•æ¡ä¸­åŒ…å«æ ‡å‡†ç­”æ¡ˆï¼Œæ’åè¶Šé å‰å¾—åˆ†è¶Šé«˜")
            print("-" * 60)
            
            found_articles = []  # æ‰¾åˆ°çš„çœŸå®ç­”æ¡ˆæ³•æ¡
            first_found_position = None  # ç¬¬ä¸€ä¸ªçœŸå®ç­”æ¡ˆçš„ä½ç½®
            
            for i, result in enumerate(search_results[:5]):  # åªè¯„ä¼°å‰5ä¸ªç»“æœ
                article_num = result.get('article_number')
                if article_num is None:
                    # å°è¯•ä»idå­—æ®µæå–
                    result_id = result.get('id', '')
                    if isinstance(result_id, str) and result_id.startswith('article_'):
                        try:
                            article_num = int(result_id.replace('article_', ''))
                        except ValueError:
                            pass
                    elif isinstance(result_id, int):
                        article_num = result_id

                if article_num is not None:
                    try:
                        article_num = int(article_num)
                        retrieved_ids.append(article_num)
                        similarity = result.get('similarity', result.get('score', 0.0))
                        relevance_scores[article_num] = similarity
                        
                        # æ£€æŸ¥æ˜¯å¦åœ¨çœŸå®ç­”æ¡ˆä¸­
                        is_relevant = article_num in case_relevant_articles
                        if is_relevant:
                            found_articles.append(article_num)
                            if first_found_position is None:
                                first_found_position = i + 1
                            status = "âœ… æ ‡å‡†ç­”æ¡ˆ"
                        else:
                            status = "âŒ æ— å…³æ³•æ¡"
                        
                        title = result.get('title', f'ç¬¬{article_num}æ¡')
                        print(f"æ³•æ¡{i+1:2d}: ç¬¬{article_num}æ¡ | ç›¸ä¼¼åº¦:{similarity:.4f} | {status}")
                        print(f"        æ ‡é¢˜: {title[:50]}...")
                        
                    except (ValueError, TypeError):
                        logger.warning(f"æ— æ³•è½¬æ¢æ³•æ¡IDä¸ºæ•´æ•°: {article_num}")

            # è®¡ç®—è¯„ä¼°ç»“æœ - ä¸“æ³¨å®Œæ•´æ€§å’Œæ’åºè´¨é‡
            found_all = set(found_articles) >= set(case_relevant_articles)  # æ˜¯å¦åŒ…å«æ‰€æœ‰çœŸå®ç­”æ¡ˆ
            coverage = len(found_articles) / len(case_relevant_articles) if case_relevant_articles else 0
            
            # è®¡ç®—æ’åºè´¨é‡å¾—åˆ†
            ranking_score = 0.0
            if first_found_position:
                if first_found_position <= 2:
                    ranking_score = 1.0  # æ»¡åˆ†ï¼šç¬¬1-2ä½
                elif first_found_position <= 4:
                    ranking_score = 0.8  # é«˜åˆ†ï¼šç¬¬3-4ä½
                elif first_found_position == 5:
                    ranking_score = 0.6  # ä¸­ç­‰ï¼šç¬¬5ä½
            
            # ç»¼åˆè´¨é‡å¾—åˆ† = å®Œæ•´æ€§ Ã— æ’åºè´¨é‡
            quality_score = coverage * ranking_score if ranking_score > 0 else 0
            
            print("-" * 60)
            print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ:")
            print(f"  - çœŸå®ç­”æ¡ˆ: {case_relevant_articles}")
            print(f"  - æ‰¾åˆ°ç­”æ¡ˆ: {found_articles}")
            print(f"  - è¦†ç›–ç‡: {coverage*100:.1f}% ({len(found_articles)}/{len(case_relevant_articles)})")
            print(f"  - å®Œæ•´æ€§: {'âœ… æ»¡åˆ†' if found_all else 'âŒ ç¼ºå¤±'}")
            print(f"  - æœ€ä½³æ’å: ç¬¬{first_found_position}ä½" if first_found_position else "âŒ æœªæ‰¾åˆ°")
            print(f"  - æ’åºè´¨é‡: {ranking_score*100:.1f}%")
            print(f"  - ç»¼åˆè´¨é‡: {quality_score*100:.1f}% (å®Œæ•´æ€§Ã—æ’åºè´¨é‡)")
            
            # è¯„åˆ†é€»è¾‘ï¼šå®Œå…¨å†—ä½™å®½å®¹ - åªçœ‹å®Œæ•´æ€§ï¼Œä¸æƒ©ç½šå†—ä½™
            # ä¼ ç»Ÿæ–¹å¼ï¼šprecision = æ‰¾åˆ°çš„æ ‡å‡†ç­”æ¡ˆ / 5 (ä¼šæƒ©ç½šå†—ä½™)
            # å†—ä½™å®½å®¹ï¼šåªè¦æ‰¾å…¨æ ‡å‡†ç­”æ¡ˆå°±æ»¡åˆ†ï¼Œå†—ä½™æ³•æ¡å®Œå…¨ä¸å½±å“è¯„åˆ†
            
            # å¯¹äºmetricsè®¡ç®—ï¼Œæˆ‘ä»¬éœ€è¦"æ¬ºéª—"ç³»ç»Ÿï¼š
            # è®©retrieved_idsåªåŒ…å«æ‰¾åˆ°çš„æ ‡å‡†ç­”æ¡ˆï¼Œè¿™æ ·precisionå°±æ˜¯100%
            relevant_ids = case_relevant_articles  # æ ‡å‡†ç­”æ¡ˆ
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            # found_articlesæ˜¯æ•´æ•°åˆ—è¡¨ï¼š[232, 234]
            # case_relevant_articleså¯èƒ½æ˜¯æ•´æ•°æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œéœ€è¦ç»Ÿä¸€
            
            # ç»Ÿä¸€è½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
            retrieved_ids = [str(art) for art in found_articles] if found_articles else []
            relevant_ids = [str(art) for art in case_relevant_articles] if case_relevant_articles else []
            
            # å¦‚æœæ²¡æ‰¾åˆ°ä»»ä½•æ ‡å‡†ç­”æ¡ˆï¼Œä¸ºäº†é¿å…é™¤é›¶é”™è¯¯ï¼Œè‡³å°‘è¿”å›ä¸€ä¸ªå ä½ç¬¦
            if not retrieved_ids:
                retrieved_ids = ["placeholder"]  # è¿™æ ·precisionå°±æ˜¯0ï¼Œç¬¦åˆé¢„æœŸ  
            logger.info(f"æ¡ˆä¾‹{case_id}è¯„ä¼°ï¼šè¦†ç›–ç‡{coverage*100:.1f}%ï¼Œæœ€ä½³æ’åç¬¬{first_found_position or 'âˆ'}ä½")
            
        else:
            # å…¶ä»–æŸ¥è¯¢ç±»å‹ä¿æŒåŸé€»è¾‘
            print(f"\nğŸ“Š æœªçŸ¥æŸ¥è¯¢ç±»å‹è¯„ä¼° - {query_type}")
            for i, result in enumerate(search_results):
                doc_id = result.get('id')
                if doc_id is not None:
                    retrieved_ids.append(doc_id)
                    relevance_scores[doc_id] = result.get('similarity', result.get('score', 0.0))

            relevant_ids = (query.get('ground_truth_cases', []) +
                          query.get('ground_truth_articles', []))
        
        # è®¡ç®—äº¤é›†ç”¨äºè°ƒè¯•
        intersection = set(retrieved_ids) & set(relevant_ids)
        logger.info(f"äº¤é›†æ•°é‡: {len(intersection)}, äº¤é›†å†…å®¹: {list(intersection)[:5]}")

        # è®¡ç®—æŒ‡æ ‡
        metrics = self.metrics_calculator.calculate_all_metrics(
            retrieved=retrieved_ids,
            relevant=relevant_ids,
            k_values=self.config['top_k_values'],
            relevance_scores=relevance_scores
        )
        
        # æ˜¾ç¤ºä¸­æ–‡æŒ‡æ ‡ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†è¯„ä¼°æŒ‡æ ‡:")
        print(f"  ğŸ¯ ç²¾ç¡®åº¦@5:  {metrics.get('precision@5', 0):.4f} ({metrics.get('precision@5', 0)*100:.1f}%)")
        print(f"  ğŸ¯ ç²¾ç¡®åº¦@10: {metrics.get('precision@10', 0):.4f} ({metrics.get('precision@10', 0)*100:.1f}%)")
        print(f"  ğŸ¯ ç²¾ç¡®åº¦@20: {metrics.get('precision@20', 0):.4f} ({metrics.get('precision@20', 0)*100:.1f}%)")
        print(f"  ğŸ“ˆ å¬å›ç‡@5:  {metrics.get('recall@5', 0):.4f} ({metrics.get('recall@5', 0)*100:.1f}%)")
        print(f"  ğŸ“ˆ å¬å›ç‡@10: {metrics.get('recall@10', 0):.4f} ({metrics.get('recall@10', 0)*100:.1f}%)")
        print(f"  ğŸ”— F1åˆ†æ•°@10: {metrics.get('f1@10', 0):.4f}")
        print(f"  ğŸ† MAPå¾—åˆ†:    {metrics.get('map', 0):.4f}")
        print("=" * 80)
        
        # æ·»åŠ å‘½ä¸­å¼æŒ‡æ ‡ï¼ˆç‰¹åˆ«é€‚åˆæ¡ˆä¾‹åˆ°æ³•æ¡æ£€ç´¢ï¼‰
        if query_type == 'case_to_articles':
            # å¯¹äºæ¡ˆä¾‹åˆ°æ³•æ¡ï¼Œæ·»åŠ Hit@KæŒ‡æ ‡ï¼ˆåªè¦æ‰¾åˆ°ä»»æ„ä¸€ä¸ªæ­£ç¡®æ³•æ¡å³å¯ï¼‰
            hit_metrics = {}
            for k in self.config['top_k_values']:
                retrieved_k = retrieved_ids[:k]
                hit_metrics[f'hit@{k}'] = 1.0 if any(doc_id in relevant_ids for doc_id in retrieved_k) else 0.0
            
            # è®¡ç®—MRRï¼ˆå¹³å‡å€’æ’ä½ç½®ï¼‰
            first_relevant_rank = None
            for i, doc_id in enumerate(retrieved_ids):
                if doc_id in relevant_ids:
                    first_relevant_rank = i + 1
                    break
            
            mrr = 1.0 / first_relevant_rank if first_relevant_rank else 0.0
            hit_metrics['mrr'] = mrr
            
            # åˆå¹¶æŒ‡æ ‡
            metrics.update(hit_metrics)
        
        return {
            'query_id': query['query_id'],
            'query_type': query_type,
            'retrieved': retrieved_ids,
            'relevant': relevant_ids,
            'metrics': metrics,
            'response_time': search_result.get('response_time'),
            'error': search_result.get('error')
        }
    
    def _calculate_metrics(self, evaluation_results: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """
        è®¡ç®—æ±‡æ€»æŒ‡æ ‡
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœ
            
        Returns:
            æŒ‡æ ‡æ±‡æ€»
        """
        metrics_by_type = {}
        
        for query_type, results in evaluation_results.items():
            # ç‰¹æ®Šå¤„ç†ç½ªåä¸€è‡´æ€§è¯„ä¼°
            if query_type == 'crime_consistency':
                consistency_results = []
                for r in results:
                    if 'consistency_metrics' in r:
                        consistency_results.append(r['consistency_metrics'])
                
                if consistency_results:
                    # è®¡ç®—å¹³å‡ä¸€è‡´æ€§æŒ‡æ ‡
                    avg_precision = sum(r['precision'] for r in consistency_results) / len(consistency_results)
                    avg_recall = sum(r['recall'] for r in consistency_results) / len(consistency_results)
                    avg_jaccard = sum(r['jaccard'] for r in consistency_results) / len(consistency_results)
                    
                    # æ¡ˆä¾‹çº§è¦†ç›–ç‡ - æ–°çš„æ ¸å¿ƒæŒ‡æ ‡
                    avg_case_coverage = sum(r.get('case_coverage_rate', 0) for r in consistency_results) / len(consistency_results)
                    total_covered_cases = sum(r.get('covered_cases_count', 0) for r in consistency_results)
                    total_all_cases = sum(r.get('total_cases_count', 0) for r in consistency_results)
                    
                    metrics_by_type[query_type] = {
                        # ä¼ ç»ŸæŒ‡æ ‡
                        'consistency_precision_mean': avg_precision,
                        'consistency_recall_mean': avg_recall,
                        'consistency_jaccard_mean': avg_jaccard,
                        
                        # æ–°çš„æ ¸å¿ƒæŒ‡æ ‡
                        'case_coverage_rate_mean': avg_case_coverage,
                        'total_covered_cases': total_covered_cases,
                        'total_all_cases': total_all_cases,
                        'overall_case_coverage_rate': total_covered_cases / total_all_cases if total_all_cases > 0 else 0.0,
                        
                        'total_queries': len(consistency_results)
                    }
                continue
            
            # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
            all_metrics = [r['metrics'] for r in results if 'metrics' in r]
            
            # èšåˆæŒ‡æ ‡
            if all_metrics:
                aggregated = self.metrics_calculator.aggregate_metrics(all_metrics)
                metrics_by_type[query_type] = aggregated
            
            # è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§æŒ‡æ ‡
            if query_type == 'article_to_cases':
                # å‡†å¤‡æ•°æ®æ ¼å¼
                search_results = []
                for r in results:
                    article_num = int(r['query_id'].replace('article_', ''))
                    search_results.append({
                        'article_number': article_num,
                        'retrieved_cases': r['retrieved']
                    })
                
                accuracy = self.semantic_metrics.article_case_accuracy(
                    search_results,
                    self.ground_truth_loader.article_case_mapping
                )
                metrics_by_type[query_type]['semantic_accuracy'] = accuracy
            
            elif query_type == 'case_to_articles':
                # å‡†å¤‡æ•°æ®æ ¼å¼
                search_results = []
                for r in results:
                    case_id = r['query_id'].replace('case_', '')
                    search_results.append({
                        'case_id': case_id,
                        'retrieved_articles': r['retrieved']  # ä¿æŒå­—æ®µåä¸€è‡´
                    })
                
                accuracy = self.semantic_metrics.case_article_accuracy(
                    search_results,
                    self.ground_truth_loader.case_article_mapping
                )
                metrics_by_type[query_type]['semantic_accuracy'] = accuracy
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        overall_metrics = self._calculate_overall_metrics(metrics_by_type)
        
        return {
            'by_type': metrics_by_type,
            'overall': overall_metrics
        }
    
    def _calculate_overall_metrics(self, metrics_by_type: Dict) -> Dict[str, float]:
        """
        è®¡ç®—æ€»ä½“æŒ‡æ ‡
        
        Args:
            metrics_by_type: æŒ‰ç±»å‹åˆ†ç»„çš„æŒ‡æ ‡
            
        Returns:
            æ€»ä½“æŒ‡æ ‡
        """
        overall = {}
        
        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡åç§°
        all_metric_names = set()
        for type_metrics in metrics_by_type.values():
            all_metric_names.update(type_metrics.keys())
        
        # è®¡ç®—åŠ æƒå¹³å‡
        for metric_name in all_metric_names:
            values = []
            weights = []
            
            for query_type, type_metrics in metrics_by_type.items():
                if metric_name in type_metrics:
                    values.append(type_metrics[metric_name])
                    # æƒé‡åˆ†é… - è°ƒæ•´è®©ç½ªåä¸€è‡´æ€§å 40%
                    if query_type == 'crime_consistency':
                        weights.append(2.0)  # æé«˜ç½ªåä¸€è‡´æ€§æƒé‡åˆ°40%
                    elif query_type in ['article_to_cases', 'case_to_articles']:  
                        weights.append(1.5)  # é™ä½æ ¸å¿ƒåŠŸèƒ½æƒé‡ï¼Œå¹³è¡¡å æ¯”
                    else:
                        weights.append(1.0)
            
            if values:
                # è®¡ç®—åŠ æƒå¹³å‡
                weighted_sum = sum(v * w for v, w in zip(values, weights))
                total_weight = sum(weights)
                overall[metric_name] = weighted_sum / total_weight if total_weight > 0 else 0
        
        return overall
    
    def _generate_summary(self, metrics_results: Dict) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¯„ä¼°æ‘˜è¦
        
        Args:
            metrics_results: æŒ‡æ ‡ç»“æœ
            
        Returns:
            æ‘˜è¦ä¿¡æ¯
        """
        summary = {
            'overall_score': 0.0,
            'key_metrics': {},
            'strengths': [],
            'weaknesses': [],
            'recommendations': []
        }
        
        overall = metrics_results.get('overall', {})
        
        # æå–å…³é”®æŒ‡æ ‡ - ä¼˜åŒ–ç»„åˆï¼Œå‡å°‘å†—ä½™ï¼Œçªå‡ºæ ¸å¿ƒ
        key_metrics = [
            'semantic_accuracy',              # è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼ˆé€šç”¨ï¼‰
            'case_coverage_rate_mean',        # æ¡ˆä¾‹çº§è¦†ç›–ç‡ï¼ˆç½ªåä¸€è‡´æ€§æ ¸å¿ƒï¼‰
            'consistency_recall_mean',        # ä¸€è‡´æ€§å¬å›ç‡ï¼ˆç½ªåä¸€è‡´æ€§ï¼‰
            'precision@5_mean',               # å®Œæ•´æ€§å¾—åˆ†ï¼ˆæ¡ˆä¾‹â†’æ³•æ¡ï¼Œå®Œå…¨å†—ä½™å®½å®¹åï¼‰
            'average_precision_mean'          # æ’åºè´¨é‡ï¼ˆæ³•æ¡â†’æ¡ˆä¾‹ï¼‰
        ]
        
        # è¯´æ˜ï¼šåˆ é™¤å†—ä½™çš„consistency_precision_meanå’Œconsistency_jaccard_mean
        # å› ä¸ºå®ƒä»¬ä¸case_coverage_rate_meanè¡¡é‡çš„æ˜¯åŒç±»é—®é¢˜
        
        for metric in key_metrics:
            if metric in overall:
                value = overall[metric]
                summary['key_metrics'][metric] = round(value, 4)
                
                # è¯„ä¼°å¼ºå¼±é¡¹
                if value >= 0.7:
                    summary['strengths'].append(f"{metric}: {value:.2%}")
                elif value < 0.5:
                    summary['weaknesses'].append(f"{metric}: {value:.2%}")
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        if summary['key_metrics']:
            summary['overall_score'] = sum(summary['key_metrics'].values()) / len(summary['key_metrics'])
        
        # ç”Ÿæˆå»ºè®®
        if summary['overall_score'] < 0.6:
            summary['recommendations'].append("æ•´ä½“æ€§èƒ½éœ€è¦æ˜¾è‘—æ”¹è¿›")
        
        
        if overall.get('precision@5_mean', 0) < 0.5:
            summary['recommendations'].append("éœ€è¦æé«˜ç²¾ç¡®ç‡ï¼Œè€ƒè™‘ä¼˜åŒ–æ’åºç®—æ³•æˆ–åŠ å¼ºç›¸å…³æ€§åˆ¤æ–­")
        
        if overall.get('consistency_jaccard_mean', 0) < 0.3:
            summary['recommendations'].append("ç½ªåæœç´¢çš„æ³•æ¡-æ¡ˆä¾‹ä¸€è‡´æ€§è¾ƒä½ï¼Œéœ€è¦ä¼˜åŒ–è¯­ä¹‰ç†è§£æ¨¡å‹")
        
        return summary
    
    def save_results(self, output_dir: Path = None):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        if output_dir is None:
            output_dir = RESULTS_DIR
        
        # åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæ ¼å¼
        json_path = output_dir / f"evaluation_results_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.evaluation_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {json_path}")
        
        return json_path
