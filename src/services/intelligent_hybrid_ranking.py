#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ··åˆæ’åºå¼•æ“ v2.0
ç»“åˆOPTIMIZATION_GUIDEå’ŒSMART_OPTIMIZATION_PLANçš„æ ¸å¿ƒæ€è·¯
å®ç°ï¼šæ™ºèƒ½æŸ¥è¯¢æ‰©å±• + å¤šä¿¡å·èåˆæ’åº
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
from pathlib import Path
import pickle
import asyncio
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re

class IntelligentHybridRankingService:
    """æ™ºèƒ½æ··åˆæ’åºå¼•æ“ - å®Œæ•´ç‰ˆ"""
    
    def __init__(self):
        print("Initializing Intelligent Hybrid Ranking Service v2.0...")
        
        # 1. åŸºç¡€ç»„ä»¶åˆå§‹åŒ–
        self.embedding_model = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
        # 2. åŠ è½½æ˜ å°„æ•°æ®
        self.mapping_data = self._load_mapping_data()
        self.precise_mappings = self._build_precise_mapping_index()
        self.fuzzy_mappings = self._build_fuzzy_mapping_index()
        
        # 3. æ„å»ºæ™ºèƒ½æŸ¥è¯¢æ‰©å±•ç´¢å¼•
        self.query_expansion_index = self._build_query_expansion_index()
        
        # 4. åˆå§‹åŒ–å¤šä¿¡å·æƒé‡
        self.signal_weights = {
            'semantic': 0.5,        # åŸå§‹è¯­ä¹‰ç›¸ä¼¼åº¦ 50%
            'expansion': 0.25,      # æŸ¥è¯¢æ‰©å±•åŒ¹é… 25%
            'mapping': 0.15,        # æ˜ å°„å…³ç³»åŠ æƒ 15%
            'keyword': 0.10         # å…³é”®è¯åŒ¹é… 10%
        }
        
        print("Smart Hybrid Ranking Service initialized successfully!")
    
    def _load_mapping_data(self) -> Dict:
        """åŠ è½½æ˜ å°„è¡¨æ•°æ®"""
        try:
            precise_df = pd.read_csv('data/raw/ç²¾ç¡®æ˜ å°„è¡¨.csv')
            fuzzy_df = pd.read_csv('data/raw/ç²¾ç¡®+æ¨¡ç³ŠåŒ¹é…æ˜ å°„è¡¨.csv')
            
            print(f"Loaded mapping data: {len(precise_df)} precise, {len(fuzzy_df)} total mappings")
            
            return {
                'precise': precise_df,
                'fuzzy': fuzzy_df,
                'precise_count': len(precise_df),
                'fuzzy_count': len(fuzzy_df)
            }
        except Exception as e:
            print(f"Warning: æ˜ å°„è¡¨åŠ è½½å¤±è´¥ - {e}")
            return {'precise': pd.DataFrame(), 'fuzzy': pd.DataFrame()}
    
    def _build_precise_mapping_index(self) -> Dict:
        """æ„å»ºç²¾ç¡®æ˜ å°„ç´¢å¼• - ä½¿ç”¨IDæ˜ å°„ä¿®å¤"""
        print("Building precise mapping index with ID mapping...")
        
        try:
            # åŠ è½½IDæ˜ å°„å…³ç³»
            import pickle
            with open('data/processed/id_mapping.pkl', 'rb') as f:
                id_mapping = pickle.load(f)
            
            print(f"Loaded ID mapping: {len(id_mapping)} entries")
            
            index = {}
            mapped_count = 0
            
            if not self.mapping_data['precise'].empty:
                for _, row in self.mapping_data['precise'].iterrows():
                    case_id_original = row['æ¡ˆä¾‹ID']
                    law_id_original = row['å¾‹æ³•ID']
                    
                    # ä½¿ç”¨IDæ˜ å°„è½¬æ¢ä¸ºå®é™…æ–‡æ¡£ID
                    case_id = id_mapping.get(case_id_original)
                    law_id = id_mapping.get(law_id_original)
                    
                    if case_id and law_id:  # åªæœ‰å½“ä¸¤ä¸ªIDéƒ½èƒ½æ‰¾åˆ°æ˜ å°„æ—¶æ‰å»ºç«‹å…³ç³»
                        description = row.get('é€‚ç”¨è¯´æ˜', '')
                        main_usage = row.get('ä¸»è¦é€‚ç”¨', 'å¦') == 'æ˜¯'
                        weight = 0.9 if main_usage else 0.7
                        
                        # å»ºç«‹åŒå‘æ˜ å°„å…³ç³»
                        if case_id not in index:
                            index[case_id] = []
                        if law_id not in index:
                            index[law_id] = []
                        
                        index[case_id].append({
                            'related_id': law_id,
                            'type': 'law',
                            'relation': 'precise',
                            'description': description,
                            'weight': weight
                        })
                        
                        index[law_id].append({
                            'related_id': case_id,
                            'type': 'case', 
                            'relation': 'precise',
                            'description': description,
                            'weight': weight
                        })
                        
                        mapped_count += 1
            
            print(f"Built precise mapping index: {len(index)} documents with mappings ({mapped_count} relationships)")
            return index
            
        except Exception as e:
            print(f"Error building precise mapping index: {e}")
            return {}
    
    def _build_fuzzy_mapping_index(self) -> Dict:
        """æ„å»ºæ¨¡ç³Šæ˜ å°„ç´¢å¼• - ä½¿ç”¨IDæ˜ å°„ä¿®å¤"""
        print("Building fuzzy mapping index with ID mapping...")
        
        try:
            # åŠ è½½IDæ˜ å°„å…³ç³»
            import pickle
            with open('data/processed/id_mapping.pkl', 'rb') as f:
                id_mapping = pickle.load(f)
            
            index = {}
            mapped_count = 0
            
            if not self.mapping_data['fuzzy'].empty:
                for _, row in self.mapping_data['fuzzy'].iterrows():
                    case_id_original = row['æ¡ˆä¾‹ID']
                    law_id_original = row['å¾‹æ³•ID']
                    is_precise = row.get('ä¸»è¦é€‚ç”¨', 'å¦') == 'æ˜¯'
                    
                    # è·³è¿‡å·²åœ¨ç²¾ç¡®æ˜ å°„ä¸­çš„å…³ç³»
                    if not is_precise:
                        # ä½¿ç”¨IDæ˜ å°„è½¬æ¢ä¸ºå®é™…æ–‡æ¡£ID
                        case_id = id_mapping.get(case_id_original)
                        law_id = id_mapping.get(law_id_original)
                        
                        if case_id and law_id:
                            if law_id not in index:
                                index[law_id] = []
                            if case_id not in index:
                                index[case_id] = []
                                
                            weight = 0.6  # æ¨¡ç³Šæ˜ å°„æƒé‡è¾ƒä½
                            description = row.get('é€‚ç”¨è¯´æ˜', '')
                            
                            index[law_id].append({
                                'related_id': case_id,
                                'type': 'case',
                                'relation': 'fuzzy',
                                'description': description,
                                'weight': weight
                            })
                            
                            index[case_id].append({
                                'related_id': law_id,
                                'type': 'law',
                                'relation': 'fuzzy',
                                'description': description,
                                'weight': weight
                            })
                            
                            mapped_count += 1
        
            print(f"Built fuzzy mapping index: {len(index)} documents with mappings ({mapped_count} relationships)")
            return index
            
        except Exception as e:
            print(f"Error building fuzzy mapping index: {e}")
            return {}
    
    def _build_query_expansion_index(self) -> Dict:
        """æ„å»ºæ™ºèƒ½æŸ¥è¯¢æ‰©å±•ç´¢å¼• - åŸºäºç°æœ‰æ–‡æ¡£è€Œéæ˜ å°„è¡¨"""
        print("Building intelligent query expansion index...")
        
        try:
            # åŠ è½½å®Œæ•´æ•°æ®é›†
            with open('data/processed/full_dataset.pkl', 'rb') as f:
                full_data = pickle.load(f)
            
            if 'documents' not in full_data:
                print("Warning: No 'documents' key found in dataset")
                return {'pairs': [], 'embeddings': None, 'professional_terms': []}
                
            documents = full_data['documents']
            
            # ç­–ç•¥æ”¹å˜ï¼šç›´æ¥ä»æ–‡æ¡£ä¸­åˆ›å»ºå£è¯­åŒ–-ä¸“ä¸šæœ¯è¯­å¯¹åº”å…³ç³»
            # æ€è·¯ï¼šæ¡ˆä¾‹æ ‡é¢˜é€šå¸¸æ›´å£è¯­åŒ–ï¼Œæ³•æ¡æ ‡é¢˜æ›´ä¸“ä¸šåŒ–
            expansion_pairs = []
            
            # è·å–æ¡ˆä¾‹å’Œæ³•æ¡
            case_docs = [doc for doc in documents if doc['type'] == 'case']
            law_docs = [doc for doc in documents if doc['type'] == 'law']
            
            print(f"Found {len(case_docs)} cases and {len(law_docs)} laws")
            
            # åˆ›å»ºæ‰©å±•å¯¹ï¼šåŸºäºå…³é”®è¯åŒ¹é…
            # è¿™é‡Œç”¨ç®€åŒ–æ–¹æ³•ï¼šæå–æ¡ˆä¾‹ä¸­çš„å…³é”®æ¦‚å¿µï¼ŒåŒ¹é…å¯¹åº”æ³•æ¡
            keywords_mapping = {
                # åˆ‘æ³•ç›¸å…³
                'æ‰“': ['æ•…æ„ä¼¤å®³', 'æš´åŠ›', 'äººèº«ä¼¤å®³'],
                'å·': ['ç›—çªƒ', 'ä¾µå ', 'è´¢äº§çŠ¯ç½ª'],
                'æŠ¢': ['æŠ¢åŠ«', 'æŠ¢å¤º', 'æš´åŠ›çŠ¯ç½ª'], 
                'æ€': ['æ•…æ„æ€äºº', 'æ•…æ„ä¼¤å®³è‡´æ­»'],
                
                # æ°‘æ³•ç›¸å…³
                'ç¦»å©š': ['å©šå§»', 'å®¶åº­', 'è´¢äº§åˆ†å‰²'],
                'è´¢äº§': ['ç»§æ‰¿', 'åˆ†å‰²', 'å…±åŒè´¢äº§'],
                'åˆåŒ': ['è¿çº¦', 'è´£ä»»', 'åˆåŒæ³•'],
                'å·¥èµ„': ['åŠ³åŠ¨', 'åŠ³åŠ¡', 'è–ªé…¬'],
                
                # è¡Œæ”¿æ³•ç›¸å…³
                'äº¤é€š': ['é“è·¯', 'è½¦è¾†', 'äº¤é€šæ³•'],
                'ç¨': ['ç¨æ”¶', 'ç¨æ³•', 'å¾æ”¶'],
            }
            
            # åŸºäºå…³é”®è¯åˆ›å»ºæ˜ å°„
            for colloquial, professional_terms in keywords_mapping.items():
                for term in professional_terms:
                    expansion_pairs.append({
                        'colloquial': colloquial,
                        'professional': term,
                        'confidence': 0.8,
                        'source': 'keyword_mapping',
                        'description': f'{colloquial} -> {term}'
                    })
            
            # é¢å¤–ï¼šä»å®é™…æ–‡æ¡£æ ‡é¢˜ä¸­æå–æ›´å¤šå¯¹åº”å…³ç³»
            for case_doc in case_docs[:100]:  # é™åˆ¶æ•°é‡ï¼Œé¿å…è¿‡å¤š
                case_title = case_doc.get('title', '')
                if case_title and len(case_title) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ ‡é¢˜
                    # ä»æ¡ˆä¾‹æ ‡é¢˜ä¸­æå–å…³é”®è¯
                    for keyword, professional_terms in keywords_mapping.items():
                        if keyword in case_title:
                            for term in professional_terms[:2]:  # æ¯ä¸ªå…³é”®è¯æœ€å¤š2ä¸ªæ‰©å±•
                                expansion_pairs.append({
                                    'colloquial': case_title[:50],  # æˆªå–æ¡ˆä¾‹æ ‡é¢˜
                                    'professional': term,
                                    'confidence': 0.7,
                                    'source': 'case_title_mapping',
                                    'description': f'Case: {case_title[:30]}... -> {term}'
                                })
                            break  # æ¯ä¸ªæ¡ˆä¾‹åªåŒ¹é…ä¸€æ¬¡
            
            print(f"Generated {len(expansion_pairs)} query expansion pairs")
            
            # æ„å»ºè¯­ä¹‰ç´¢å¼•ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–embeddingæ¨¡å‹ï¼‰
            expansion_index = {
                'pairs': expansion_pairs,
                'embeddings': None,  # å»¶è¿Ÿè®¡ç®—
                'professional_terms': [pair['professional'] for pair in expansion_pairs]
            }
            
            return expansion_index
            
        except Exception as e:
            print(f"Error building query expansion index: {e}")
            return {'pairs': [], 'embeddings': None, 'professional_terms': []}
    
    def _ensure_embedding_model(self):
        """ç¡®ä¿embeddingæ¨¡å‹å·²åˆå§‹åŒ–"""
        if self.embedding_model is None:
            print("Initializing embedding model for query expansion...")
            self.embedding_model = SentenceTransformer('shibing624/text2vec-base-chinese')
            
            # è®¡ç®—æ‰©å±•ç´¢å¼•çš„embeddings
            if self.query_expansion_index['embeddings'] is None and self.query_expansion_index['pairs']:
                colloquial_texts = [pair['colloquial'] for pair in self.query_expansion_index['pairs']]
                if colloquial_texts:
                    self.query_expansion_index['embeddings'] = self.embedding_model.encode(colloquial_texts)
                    print(f"Computed embeddings for {len(colloquial_texts)} expansion pairs")
    
    async def expand_user_query(self, user_query: str, max_expansions: int = 3) -> Dict:
        """æ™ºèƒ½æŸ¥è¯¢æ‰©å±• - æ ¸å¿ƒåŠŸèƒ½"""
        self._ensure_embedding_model()
        
        if not self.query_expansion_index['pairs'] or self.query_expansion_index['embeddings'] is None:
            return {
                'original_query': user_query,
                'expanded_query': user_query,
                'expansions': [],
                'expansion_applied': False
            }
        
        try:
            # 1. å¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œè¯­ä¹‰ç¼–ç 
            query_embedding = self.embedding_model.encode([user_query])
            
            # 2. è®¡ç®—ä¸æ‰€æœ‰å£è¯­è¡¨è¾¾çš„ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_embedding, self.query_expansion_index['embeddings'])[0]
            
            # 3. è·å–top-kæœ€ç›¸ä¼¼çš„ä¸“ä¸šæœ¯è¯­
            top_indices = similarities.argsort()[-max_expansions:]
            expansions = []
            
            for idx in reversed(top_indices):  # æŒ‰ç›¸ä¼¼åº¦é™åº
                if similarities[idx] > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    pair = self.query_expansion_index['pairs'][idx]
                    confidence = similarities[idx] * pair['confidence']  # ç»“åˆåŸå§‹ç½®ä¿¡åº¦
                    
                    expansions.append({
                        'term': pair['professional'],
                        'confidence': confidence,
                        'source_colloquial': pair['colloquial'],
                        'similarity': similarities[idx],
                        'description': pair.get('description', '')
                    })
            
            # 4. æ„å»ºæ‰©å±•æŸ¥è¯¢
            expansion_terms = [exp['term'] for exp in expansions]
            expanded_query = user_query
            if expansion_terms:
                expanded_query += " " + " ".join(expansion_terms)
            
            return {
                'original_query': user_query,
                'expanded_query': expanded_query,
                'expansions': expansions,
                'expansion_applied': len(expansions) > 0
            }
            
        except Exception as e:
            print(f"Error in query expansion: {e}")
            return {
                'original_query': user_query,
                'expanded_query': user_query,
                'expansions': [],
                'expansion_applied': False
            }
    
    def _extract_legal_keywords(self, text: str) -> List[str]:
        """æ™ºèƒ½æå–æ³•å¾‹å…³é”®è¯ - æ··åˆæ–¹æ¡ˆ (TextRank + ä¸“ä¸šæœ¯è¯­)"""
        try:
            import jieba
            import jieba.analyse
            
            # æ–¹æ¡ˆ1: ä½¿ç”¨TextRankç®—æ³•æå–è¯­ä¹‰å…³é”®è¯
            # allowPOSé™åˆ¶ä¸ºåè¯å’ŒåŠ¨è¯ï¼Œæ›´é€‚åˆæ³•å¾‹æ–‡æœ¬
            textrank_keywords = jieba.analyse.textrank(
                text, 
                topK=3,  # åªå–å‰3ä¸ªæœ€é‡è¦çš„
                withWeight=False,
                allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn')  # åè¯ã€åŠ¨è¯ç±»
            )
            
            # æ–¹æ¡ˆ2: æ³•å¾‹ä¸“ä¸šæœ¯è¯­åŒ¹é…ï¼ˆæ›´ç²¾å‡†çš„æœ¯è¯­ï¼‰
            legal_terms = [
                'æ•…æ„ä¼¤å®³', 'æ•…æ„æ€äºº', 'ç›—çªƒç½ª', 'æŠ¢åŠ«ç½ª', 'è¯ˆéª—ç½ª', 'åˆåŒè¿çº¦',
                'äº¤é€šäº‹æ•…', 'æœºåŠ¨è½¦', 'äººèº«æŸå®³', 'è´¢äº§æŸå¤±', 'åŠ³åŠ¨äº‰è®®', 'å·¥èµ„',
                'å©šå§»æ³•', 'ç¦»å©š', 'è´¢äº§åˆ†å‰²', 'æŠšå…»è´¹', 'ç»§æ‰¿æƒ', 'æˆ¿äº§çº çº·',
                'è¡Œæ”¿å¤„ç½š', 'åˆ‘äº‹è´£ä»»', 'æ°‘äº‹è´£ä»»', 'èµ”å¿è´£ä»»'
            ]
            
            # æå–æ–‡æœ¬ä¸­å‡ºç°çš„ä¸“ä¸šæœ¯è¯­
            matched_terms = []
            text_lower = text.lower()
            for term in legal_terms:
                if term in text_lower:
                    matched_terms.append(term)
            
            # æ–¹æ¡ˆ3: ç»„åˆå»é‡
            all_keywords = list(set(textrank_keywords + matched_terms))
            
            # é™åˆ¶æœ€å¤š5ä¸ªå…³é”®è¯ï¼Œä¼˜å…ˆä¿ç•™TextRankç»“æœ
            result = textrank_keywords[:3]  # TextRankå‰3ä¸ª
            for term in matched_terms:
                if term not in result and len(result) < 5:
                    result.append(term)
            
            return result
            
        except ImportError:
            print("jiebaæœªå®‰è£…ï¼Œä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–æ–¹æ¡ˆ")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºç¡€æ³•å¾‹æœ¯è¯­åŒ¹é…
            basic_terms = [
                'åˆåŒ', 'è¿çº¦', 'è´£ä»»', 'èµ”å¿', 'ä¼¤å®³', 'äº‹æ•…', 'ç›—çªƒ', 
                'è¯ˆéª—', 'äº¤é€š', 'åŠ³åŠ¨', 'å·¥èµ„', 'å©šå§»', 'ç¦»å©š', 'è´¢äº§'
            ]
            return [term for term in basic_terms if term in text.lower()][:3]
        except Exception as e:
            print(f"å…³é”®è¯æå–å‡ºé”™ï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ¡ˆ: {e}")
            return []
    
    def _calculate_keyword_boost(self, query: str, result: Dict) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åŠ æƒåˆ†æ•° - æ™ºèƒ½åŒ¹é…ç‰ˆ"""
        title = result.get('title', '')
        content_preview = result.get('content_preview', '')
        
        # æå–æŸ¥è¯¢å…³é”®è¯
        query_keywords = self._extract_legal_keywords(query)
        
        if not query_keywords:
            return 0.0  # ğŸ”§ ä¿®å¤ï¼šæ— å…³é”®è¯æ—¶è¿”å›0åˆ†ï¼Œä¸æ˜¯0.5åˆ†
        
        # æå–æ–‡æ¡£å…³é”®è¯
        doc_text = title + ' ' + content_preview
        doc_keywords = self._extract_legal_keywords(doc_text)
        
        if not doc_keywords:
            return 0.0
        
        # æ™ºèƒ½åŒ¹é…ç®—æ³•
        total_matches = 0
        max_possible_matches = len(query_keywords) * len(doc_keywords)
        
        for q_kw in query_keywords:
            for d_kw in doc_keywords:
                # 1. å®Œå…¨åŒ¹é… (æƒé‡1.0)
                if q_kw == d_kw:
                    total_matches += 1.0
                # 2. åŒ…å«åŒ¹é… (æƒé‡0.8)
                elif q_kw in d_kw or d_kw in q_kw:
                    total_matches += 0.8
                # 3. è¯­ä¹‰ç›¸å…³åŒ¹é… (æƒé‡0.6)
                elif self._are_keywords_related(q_kw, d_kw):
                    total_matches += 0.6
        
        if max_possible_matches == 0:
            return 0.0
            
        # å½’ä¸€åŒ–åŒ¹é…åˆ†æ•°
        match_score = total_matches / max_possible_matches
        return min(match_score, 1.0)
    
    def _are_keywords_related(self, word1: str, word2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªå…³é”®è¯æ˜¯å¦è¯­ä¹‰ç›¸å…³"""
        # å®šä¹‰è¯­ä¹‰ç›¸å…³è¯ç»„
        semantic_groups = [
            # åˆ‘æ³•ç›¸å…³
            {'æ•…æ„ä¼¤å®³', 'ä¼¤å®³', 'æš´åŠ›', 'æ®´æ‰“', 'æ‰“å‡»', 'äººèº«ä¼¤å®³'},
            {'ç›—çªƒ', 'å·', 'å·çªƒ', 'ç›—çªƒç½ª', 'è´¢äº§çŠ¯ç½ª'},
            {'è¯ˆéª—', 'æ¬ºè¯ˆ', 'è¯ˆéª—ç½ª', 'è™šå‡'},
            {'æŠ¢åŠ«', 'æŠ¢å¤º', 'æŠ¢åŠ«ç½ª', 'æš´åŠ›çŠ¯ç½ª'},
            
            # æ°‘æ³•ç›¸å…³
            {'åˆåŒ', 'è¿çº¦', 'åˆåŒè¿çº¦', 'è´£ä»»', 'èµ”å¿'},
            {'äº¤é€š', 'äº‹æ•…', 'è½¦ç¥¸', 'æ’è½¦', 'äº¤é€šäº‹æ•…', 'æœºåŠ¨è½¦'},
            {'å©šå§»', 'ç¦»å©š', 'è´¢äº§åˆ†å‰²', 'æŠšå…»', 'å©šå§»æ³•'},
            {'åŠ³åŠ¨', 'å·¥èµ„', 'è–ªé…¬', 'åŠ³åŠ¡', 'åŠ³åŠ¨äº‰è®®'},
            
            # è´¢äº§ç›¸å…³
            {'è´¢äº§', 'æˆ¿äº§', 'ç»§æ‰¿', 'é—äº§', 'æˆ¿å±‹'},
            {'æŸå¤±', 'æŸå®³', 'èµ”å¿', 'è¡¥å¿'}
        ]
        
        # æ£€æŸ¥æ˜¯å¦åœ¨åŒä¸€è¯­ä¹‰ç»„
        for group in semantic_groups:
            if word1 in group and word2 in group:
                return True
        return False
    
    def _calculate_mapping_boost(self, doc_id: str) -> float:
        """è®¡ç®—æ˜ å°„å…³ç³»åŠ æƒåˆ†æ•°"""
        boost_score = 0.0
        
        # ç²¾ç¡®æ˜ å°„åŠ æƒæ›´é«˜
        if doc_id in self.precise_mappings:
            boost_score += 0.8  # ç²¾ç¡®æ˜ å°„åŸºç¡€åˆ†
            boost_score += min(len(self.precise_mappings[doc_id]) * 0.1, 0.2)  # å…³è”æ•°é‡åŠ æƒ
        
        # æ¨¡ç³Šæ˜ å°„åŠ æƒè¾ƒä½
        elif doc_id in self.fuzzy_mappings:
            boost_score += 0.5  # æ¨¡ç³Šæ˜ å°„åŸºç¡€åˆ†
            boost_score += min(len(self.fuzzy_mappings[doc_id]) * 0.05, 0.1)  # å…³è”æ•°é‡åŠ æƒ
        
        return min(boost_score, 1.0)
    
    def _calculate_expansion_boost(self, expanded_query: str, result: Dict, expansion_info: Dict) -> float:
        """è®¡ç®—æŸ¥è¯¢æ‰©å±•åŒ¹é…åˆ†æ•°"""
        if not expansion_info['expansion_applied']:
            return 0.5  # é»˜è®¤åˆ†æ•°
        
        doc_text = result.get('title', '') + ' ' + result.get('content_preview', '')
        expansion_score = 0.0
        
        for expansion in expansion_info['expansions']:
            term = expansion['term']
            confidence = expansion['confidence']
            
            if term in doc_text:
                expansion_score += confidence
        
        # å½’ä¸€åŒ–åˆ°0-1ä¹‹é—´
        max_possible_score = sum(exp['confidence'] for exp in expansion_info['expansions'])
        if max_possible_score > 0:
            expansion_score = expansion_score / max_possible_score
        
        return min(expansion_score, 1.0)
    
    async def enhance_search_results(self, query: str, original_results: List[Dict], 
                                   expansion_info: Dict = None) -> List[Dict]:
        """å¢å¼ºæ£€ç´¢ç»“æœ - å¤šä¿¡å·èåˆ"""
        if not original_results:
            return original_results
        
        # å¦‚æœæ²¡æœ‰æä¾›æ‰©å±•ä¿¡æ¯ï¼Œå…ˆè¿›è¡ŒæŸ¥è¯¢æ‰©å±•
        if expansion_info is None:
            expansion_info = await self.expand_user_query(query)
        
        enhanced_results = []
        
        for result in original_results:
            enhanced_result = result.copy()
            doc_id = result.get('id', '')
            original_score = result.get('score', 0)
            
            # 1. è®¡ç®—å„ä¸ªä¿¡å·çš„åˆ†æ•°
            mapping_score = self._calculate_mapping_boost(doc_id)
            keyword_score = self._calculate_keyword_boost(query, result)
            expansion_score = self._calculate_expansion_boost(
                expansion_info['expanded_query'], result, expansion_info
            )
            
            # 2. çœŸæ­£åŠ¨æ€æƒé‡åˆ†é…ç®—æ³• - åŸºäºåˆ†æ•°æ¯”ä¾‹å…¬å¹³åˆ†é…
            # æ ¸å¿ƒæ€è·¯ï¼šæ¯ä¸ªä¿¡å·çš„æƒé‡ = è¯¥ä¿¡å·åˆ†æ•° / æ‰€æœ‰ä¿¡å·æ€»åˆ†æ•°
            
            # å°†æ‰€æœ‰ä¿¡å·åˆ†æ•°å½’ä¸€åŒ–åˆ°ç›¸åŒé‡çº§ (0-1ä¹‹é—´)
            normalized_scores = {
                'semantic': original_score,  # è¯­ä¹‰åˆ†æ•°å·²ç»æ˜¯0-1ä¹‹é—´
                'expansion': min(expansion_score, 1.0),  # é™åˆ¶åœ¨0-1ä¹‹é—´
                'mapping': min(mapping_score, 1.0),     # é™åˆ¶åœ¨0-1ä¹‹é—´
                'keyword': min(keyword_score, 1.0)      # é™åˆ¶åœ¨0-1ä¹‹é—´
            }
            
            # è®¡ç®—æ€»åˆ†æ•°
            total_signal_score = sum(normalized_scores.values())
            
            # åŠ¨æ€è®¡ç®—æƒé‡ï¼šæ¯ä¸ªä¿¡å·æƒé‡ = è¯¥ä¿¡å·åˆ†æ•° / æ€»åˆ†æ•°
            if total_signal_score > 0:
                semantic_weight = normalized_scores['semantic'] / total_signal_score
                expansion_weight = normalized_scores['expansion'] / total_signal_score
                mapping_weight = normalized_scores['mapping'] / total_signal_score
                keyword_weight = normalized_scores['keyword'] / total_signal_score
            else:
                # å…œåº•æƒ…å†µï¼šå¦‚æœæ‰€æœ‰åˆ†æ•°éƒ½æ˜¯0
                semantic_weight = 1.0
                expansion_weight = mapping_weight = keyword_weight = 0.0
            
            # æ‰“å°æƒé‡åˆ†é…ç”¨äºè°ƒè¯•
            print(f"åŠ¨æ€æƒé‡åˆ†é…: è¯­ä¹‰={semantic_weight:.3f}, æ‰©å±•={expansion_weight:.3f}, "
                  f"æ˜ å°„={mapping_weight:.3f}, å…³é”®è¯={keyword_weight:.3f}")
            
            # 3. è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = (
                original_score * semantic_weight +
                expansion_score * expansion_weight +
                mapping_score * mapping_weight +
                keyword_score * keyword_weight
            )
            
            # å¢å¼ºä¿éšœï¼šå¦‚æœå…¶ä»–ä¿¡å·å¤ªå¼±ï¼Œç¡®ä¿ä¸ä¼šæ‹–ç´¯è¯­ä¹‰åˆ†æ•°
            other_signals_contribution = expansion_score * expansion_weight + \
                                       mapping_score * mapping_weight + \
                                       keyword_score * keyword_weight
            
            # å¦‚æœå…¶ä»–ä¿¡å·çš„è´¡çŒ®å¾ˆå°ï¼ˆ<0.05ï¼‰ï¼Œç›´æ¥ä½¿ç”¨è¯­ä¹‰åˆ†æ•°
            if other_signals_contribution < 0.05:
                final_score = original_score
                semantic_weight = 1.0
                expansion_weight = mapping_weight = keyword_weight = 0.0
            
            # 3. ä¿å­˜è¯¦ç»†ä¿¡æ¯
            enhanced_result.update({
                'original_score': original_score,
                'final_score': final_score,
                'score': final_score,  # æ›´æ–°ä¸»åˆ†æ•°
                'score_breakdown': {
                    'semantic': original_score * semantic_weight,
                    'expansion': expansion_score * expansion_weight,
                    'mapping': mapping_score * mapping_weight,
                    'keyword': keyword_score * keyword_weight
                },
                'boost_details': {
                    'mapping_boost': mapping_score,
                    'keyword_boost': keyword_score,
                    'expansion_boost': expansion_score,
                    'has_precise_mapping': doc_id in self.precise_mappings,
                    'has_fuzzy_mapping': doc_id in self.fuzzy_mappings
                },
                'related_docs': self._get_related_documents(doc_id),
                'hybrid_enhanced': True
            })
            
            enhanced_results.append(enhanced_result)
        
        # é‡æ–°æ’åº
        enhanced_results.sort(key=lambda x: x['final_score'], reverse=True)
        
        return enhanced_results
    
    def _get_related_documents(self, doc_id: str) -> List[Dict]:
        """è·å–ç›¸å…³æ–‡æ¡£ä¿¡æ¯"""
        related = []
        
        # ç²¾ç¡®å…³è”
        if doc_id in self.precise_mappings:
            for rel in self.precise_mappings[doc_id][:3]:  # æœ€å¤š3ä¸ª
                related.append({
                    'id': rel['related_id'],
                    'type': rel['type'],
                    'relation': 'precise',
                    'description': rel['description'],
                    'weight': rel['weight']
                })
        
        # æ¨¡ç³Šå…³è” (å¦‚æœç²¾ç¡®å…³è”ä¸è¶³)
        if len(related) < 3 and doc_id in self.fuzzy_mappings:
            remaining = 3 - len(related)
            for rel in self.fuzzy_mappings[doc_id][:remaining]:
                related.append({
                    'id': rel['related_id'],
                    'type': rel['type'],
                    'relation': 'fuzzy',
                    'description': rel['description'],
                    'weight': rel['weight']
                })
        
        return related
    
    def get_service_stats(self) -> Dict:
        """è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'version': '2.0',
            'precise_mappings_count': len(self.precise_mappings),
            'fuzzy_mappings_count': len(self.fuzzy_mappings),
            'expansion_pairs_count': len(self.query_expansion_index['pairs']),
            'signal_weights': self.signal_weights,
            'features': [
                'intelligent_query_expansion',
                'multi_signal_fusion',
                'mapping_based_ranking',
                'legal_keyword_matching'
            ]
        }
    
    def update_signal_weights(self, new_weights: Dict[str, float]):
        """æ›´æ–°ä¿¡å·æƒé‡ï¼ˆç”¨äºè°ƒä¼˜ï¼‰"""
        total_weight = sum(new_weights.values())
        if abs(total_weight - 1.0) > 0.01:
            raise ValueError(f"æƒé‡æ€»å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º{total_weight}")
        
        self.signal_weights.update(new_weights)
        print(f"Updated signal weights: {self.signal_weights}")


# å…¨å±€æœåŠ¡å®ä¾‹
_intelligent_hybrid_service = None

async def get_intelligent_hybrid_service() -> IntelligentHybridRankingService:
    """è·å–æ™ºèƒ½æ··åˆæ’åºæœåŠ¡å®ä¾‹"""
    global _intelligent_hybrid_service
    if _intelligent_hybrid_service is None:
        _intelligent_hybrid_service = IntelligentHybridRankingService()
    return _intelligent_hybrid_service