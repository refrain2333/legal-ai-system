#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒGeminiå¤šæ¨¡å‹å¤‡é€‰æœºåˆ¶ + GLMå¤‡ç”¨
å½“ä¸»è¦æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨å°è¯•å…¶ä»–Geminiæ¨¡å‹ï¼Œæœ€åæ‰åˆ‡æ¢åˆ°GLM
"""

import os
import asyncio
import logging
from typing import Optional, Dict, Any, List
from openai import AsyncOpenAI, APIError

logger = logging.getLogger(__name__)


import os
import asyncio
import logging
import httpx
import json
import random
from typing import Optional, Dict, Any, List
from openai import AsyncOpenAI, APIError

logger = logging.getLogger(__name__)

class LLMClient:
    """æ”¯æŒCloudflare, GLM, Gemini, SiliconFlowå››æœåŠ¡,å¹¶æŒ‰é¡ºåºå¤‡é€‰çš„LLMå®¢æˆ·ç«¯"""

    def __init__(self, config):
        self.config = config

        # æ£€æŸ¥LLMæ€»å¼€å…³
        self.llm_enabled = getattr(config, 'LLM_ENABLED', True)
        if not self.llm_enabled:
            logger.info("LLMåŠŸèƒ½å·²ç¦ç”¨ï¼Œè·³è¿‡æ‰€æœ‰LLMæœåŠ¡åˆå§‹åŒ–")
            self.cf_client = None
            self.glm_client = None
            self.gemini_client = None
            self.siliconflow_client = None
            return

        # è¯»å–å„æœåŠ¡å•†å¼€å…³
        self.cf_enabled = getattr(config, 'LLM_PROVIDER_CLOUDFLARE', True)
        self.glm_enabled = getattr(config, 'LLM_PROVIDER_GLM', False)
        self.gemini_enabled = getattr(config, 'LLM_PROVIDER_GEMINI', False)
        self.siliconflow_enabled = getattr(config, 'LLM_PROVIDER_SILICONFLOW', False)

        # Cloudflare é…ç½® (ä¸»æœåŠ¡)
        self.cf_config = {
            'account_id': getattr(config, 'CF_ACCOUNT_ID', None),
            'api_token': getattr(config, 'CF_API_TOKEN', None),
            'model': getattr(config, 'CF_MODEL', '@cf/openai/gpt-oss-20b')
        }

        # GLMé…ç½® (å¤‡é€‰1)
        self.glm_config = {
            'api_key': getattr(config, 'GLM_API_KEY', None),
            'base_url': getattr(config, 'GLM_API_BASE', None),
            'model': getattr(config, 'GLM_MODEL', 'glm-4.5-flash')
        }

        # Geminié…ç½® (å¤‡é€‰2)
        self.gemini_config = {
            'api_key': getattr(config, 'GEMINI_API_KEY', None),
            'base_url': getattr(config, 'GEMINI_API_BASE', None),
            'models': getattr(config, 'GEMINI_MODELS', ["gemini-2.5-flash-lite"])
        }

        # ç¡…åŸºæµåŠ¨é…ç½® (å¤‡é€‰3)
        self.siliconflow_config = {
            'api_key': getattr(config, 'SILICONFLOW_API_KEY', None),
            'base_url': getattr(config, 'SILICONFLOW_API_BASE', None),
            'models': getattr(config, 'SILICONFLOW_MODELS', [
                "Qwen/Qwen2.5-7B-Instruct",
                "deepseek-ai/DeepSeek-V2.5",
                "meta-llama/Meta-Llama-3.1-8B-Instruct"
            ])
        }

        # é€šç”¨é…ç½®
        self.timeout = getattr(config, 'LLM_REQUEST_TIMEOUT', 120) # å¢åŠ è¶…æ—¶åˆ°120ç§’
        self.max_retries = getattr(config, 'LLM_MAX_RETRIES', 2) # å¢åŠ é‡è¯•æ¬¡æ•°
        self.enable_fallback = getattr(config, 'LLM_ENABLE_FALLBACK', True)

        # åªåˆå§‹åŒ–å¯ç”¨çš„å®¢æˆ·ç«¯
        self.cf_client = None
        self.glm_client = None
        self.gemini_client = None
        self.siliconflow_client = None

        if self.cf_enabled and self.cf_config['account_id'] and self.cf_config['api_token']:
            # ä¸åœ¨è¿™é‡Œåˆ›å»ºhttpxå®¢æˆ·ç«¯ï¼Œæ”¹ä¸ºæ¯æ¬¡è¯·æ±‚æ—¶åˆ›å»ºæ–°çš„
            self.cf_client = True  # æ ‡è®°ä¸ºå·²å¯ç”¨
            logger.info(f"CloudflareæœåŠ¡å·²å¯ç”¨ï¼Œå°†æŒ‰éœ€åˆ›å»ºhttpxå®¢æˆ·ç«¯")
        else:
            self.cf_client = None
            logger.warning(f"Cloudflareé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡å®¢æˆ·ç«¯åˆ›å»º")

        if self.glm_enabled:
            self.glm_client = self._create_openai_client(self.glm_config, "GLM")

        if self.gemini_enabled:
            self.gemini_client = self._create_openai_client(self.gemini_config, "Gemini")

        if self.siliconflow_enabled:
            self.siliconflow_client = self._create_openai_client(self.siliconflow_config, "SiliconFlow")

        # å½“å‰çŠ¶æ€
        self.current_service = "siliconflow"  # é»˜è®¤ä½¿ç”¨ç¡…åŸºæµåŠ¨ä½œä¸ºä¸»æœåŠ¡
        self.failed_models = set()

        logger.info(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - LLMæ€»å¼€å…³: {self.llm_enabled}")
        logger.info(f"  - Cloudflareå¼€å…³: {self.cf_enabled}, å¯ç”¨: {self.cf_client is not None}")
        logger.info(f"  - GLMå¼€å…³: {self.glm_enabled}, å¯ç”¨: {self.glm_client is not None}")
        logger.info(f"  - Geminiå¼€å…³: {self.gemini_enabled}, å¯ç”¨: {self.gemini_client is not None}")
        logger.info(f"  - SiliconFlowå¼€å…³: {self.siliconflow_enabled}, å¯ç”¨: {self.siliconflow_client is not None}")
        logger.info(f"  - å¯ç”¨å¤‡é€‰: {self.enable_fallback}")

    def _create_openai_client(self, config: Dict[str, Any], service_name: str) -> Optional[AsyncOpenAI]:
        api_key = config.get('api_key')
        base_url = config.get('base_url')
        if not api_key or not base_url:
            logger.warning(f"{service_name} é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return None
        try:
            return AsyncOpenAI(api_key=api_key, base_url=base_url, timeout=float(self.timeout), max_retries=self.max_retries)
        except Exception as e:
            logger.error(f"{service_name} å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return None

    async def generate_text(self, prompt: str, max_tokens: int = 1500, temperature: float = 0.2) -> str:
        if not self.llm_enabled:
            logger.warning("LLMåŠŸèƒ½å·²ç¦ç”¨ï¼Œè¿”å›ç©ºç»“æœ")
            return ""
        return await self._try_with_fallback(prompt, max_tokens, temperature)

    async def _try_with_fallback(self, prompt: str, max_tokens: int, temperature: float) -> str:
        # 1. ä¼˜å…ˆå°è¯• SiliconFlow (å¦‚æœå¯ç”¨)
        if self.siliconflow_enabled and self.siliconflow_client and self.siliconflow_config['models']:
            logger.info("ğŸš€ ä½¿ç”¨SiliconFlowä¸»æœåŠ¡")

            # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œå¤šæ¬¡é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
            for model in self.siliconflow_config['models']:
                if model in self.failed_models:
                    continue

                # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œæœ€å¤š2æ¬¡é‡è¯•
                for attempt in range(self.max_retries + 1):
                    try:
                        result = await self._generate_with_openai_client(self.siliconflow_client, prompt, model, max_tokens, temperature, f"SiliconFlow({model})")
                        if result:
                            self.current_service = "siliconflow"
                            logger.info(f"âœ… ä½¿ç”¨SiliconFlowä¸»æœåŠ¡æˆåŠŸ: {model}")
                            return result
                    except Exception as e:
                        error_type = type(e).__name__
                        if "APITimeoutError" in error_type or "Timeout" in error_type:
                            if attempt < self.max_retries:
                                # æŒ‡æ•°é€€é¿ï¼š1ç§’ï¼Œ2ç§’
                                wait_time = (2 ** attempt)
                                logger.warning(f"SiliconFlowä¸»æœåŠ¡æ¨¡å‹ {model} è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{self.max_retries + 1}): {e}")
                                await asyncio.sleep(wait_time)
                                continue
                            else:
                                logger.warning(f"SiliconFlowä¸»æœåŠ¡æ¨¡å‹ {model} è¶…æ—¶é‡è¯•è€—å°½: {e}")
                                self.failed_models.add(model)
                        else:
                            logger.warning(f"SiliconFlowä¸»æœåŠ¡æ¨¡å‹ {model} å¤±è´¥: {e}")
                            self.failed_models.add(model)
                        break

        # 2. å¦‚æœSiliconFlowå¤±è´¥ï¼Œå°è¯• Cloudflare (å¦‚æœå¯ç”¨)
        if self.enable_fallback and self.cf_enabled and self.cf_client:
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°Cloudflareå¤‡é€‰æœåŠ¡")
            try:
                result = await self._generate_with_cloudflare(prompt, max_tokens, temperature)
                if result:
                    self.current_service = "cloudflare"
                    logger.info(f"âœ… ä½¿ç”¨Cloudflareå¤‡é€‰æœåŠ¡æˆåŠŸ")
                    return result
            except Exception as e:
                logger.error(f"Cloudflareå¤‡é€‰æœåŠ¡å¤±è´¥: {e}")

        # 3. å¦‚æœCloudflareä¹Ÿå¤±è´¥ï¼Œå°è¯• GLM (å¦‚æœå¯ç”¨)
        if self.enable_fallback and self.glm_enabled and self.glm_client:
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°GLMå¤‡é€‰æœåŠ¡")
            try:
                result = await self._generate_with_openai_client(self.glm_client, prompt, self.glm_config['model'], max_tokens, temperature, "GLM")
                if result:
                    self.current_service = "glm"
                    return result
            except Exception as e:
                logger.error(f"GLMå¤‡é€‰æœåŠ¡å¤±è´¥: {e}")

        # 4. å¦‚æœGLMä¹Ÿå¤±è´¥ï¼Œå°è¯• Gemini (å¦‚æœå¯ç”¨)
        if self.enable_fallback and self.gemini_enabled and self.gemini_client and self.gemini_config['models']:
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°Geminiå¤‡é€‰æœåŠ¡")
            for model in self.gemini_config['models']:
                if model in self.failed_models:
                    continue
                try:
                    result = await self._generate_with_openai_client(self.gemini_client, prompt, model, max_tokens, temperature, f"Gemini({model})")
                    if result:
                        self.current_service = "gemini"
                        return result
                except Exception as e:
                    logger.warning(f"Geminiå¤‡é€‰æ¨¡å‹ {model} å¤±è´¥: {e}")
                    self.failed_models.add(model)
                    continue

        logger.error("ğŸš« æ‰€æœ‰å¯ç”¨çš„LLMæœåŠ¡å’Œæ¨¡å‹éƒ½ä¸å¯ç”¨")
        return ""

    async def _generate_with_cloudflare(self, prompt: str, max_tokens: int, temperature: float) -> Optional[str]:
        url = f"https://api.cloudflare.com/client/v4/accounts/{self.cf_config['account_id']}/ai/run/{self.cf_config['model']}"
        headers = {
            "Authorization": f"Bearer {self.cf_config['api_token']}",
            "Content-Type": "application/json"
        }
        data = {"input": prompt}

        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¤§å¹…å‡å°‘é‡è¯•æ¬¡æ•°å’Œè¶…æ—¶æ—¶é—´
        max_retries = 0  # ä¸é‡è¯•ï¼Œå¿«é€Ÿå¤±è´¥
        
        for attempt in range(max_retries + 1):  # æœ€å¤šæ‰§è¡Œ1æ¬¡
            try:
                # ğŸš€ æé™æ€§èƒ½ä¼˜åŒ–ï¼šè¶…çŸ­è¶…æ—¶ï¼Œå¿«é€Ÿå¤±è´¥
                async with httpx.AsyncClient(
                    timeout=httpx.Timeout(65.0, connect=5.0, read=60.0),  # å»¶é•¿è¶…æ—¶ä»¥å¤„ç†æ…¢å“åº”
                    limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),  # å¯ç”¨è¿æ¥é‡ç”¨
                    follow_redirects=True
                ) as client:
                    logger.info(f"ğŸ¤– å‘ Cloudflare AI å‘é€è¯·æ±‚ (å°è¯• {attempt + 1}/{max_retries + 1})")
                    
                    response = await client.post(url, headers=headers, json=data)
                    response.raise_for_status()

                    result_json = response.json()

                    # å¿«é€Ÿè§£æå“åº”
                    if result_json.get('success') and result_json.get('result'):
                        output = result_json['result'].get('output', [])
                        if isinstance(output, list) and len(output) > 0:
                            # é€šå¸¸æœ€åä¸€ä¸ªmessageæ˜¯åŠ©æ‰‹çš„å›ç­”
                            assistant_message = output[-1]
                            if assistant_message.get('role') == 'assistant' and assistant_message.get('content'):
                                content_list = assistant_message['content']
                                if isinstance(content_list, list) and len(content_list) > 0 and content_list[0].get('type') == 'output_text':
                                    generated_text = content_list[0].get('text', '').strip()
                                    if generated_text:
                                        # ç®€åŒ–Unicodeæ¸…ç†
                                        cleaned_text = generated_text.replace('\u202f', ' ').replace('\u2009', ' ').replace('\u200b', '')
                                        logger.debug(f"âœ… Cloudflare AIæˆåŠŸè¿”å›å†…å®¹ï¼Œé•¿åº¦: {len(cleaned_text)}")
                                        return cleaned_text

                    logger.warning(f"âš ï¸ Cloudflare AIè¿”å›äº†éé¢„æœŸçš„æ ¼å¼æˆ–ç©ºå†…å®¹")
                    return None
                    
            except httpx.RemoteProtocolError as e:
                if attempt == max_retries:
                    logger.error(f"âŒ Cloudflareè¿æ¥å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {e}")
                    raise e
                logger.warning(f"ğŸ”„ Cloudflareè¿æ¥åè®®é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                await asyncio.sleep(0.5)  # å¿«é€Ÿé‡è¯•
                continue
                
            except (httpx.ConnectError, httpx.ReadTimeout, httpx.TimeoutException) as e:
                # å¿«é€Ÿå¤±è´¥ï¼Œä¸é‡è¯•
                logger.warning(f"ğŸ”„ Cloudflareç½‘ç»œé”™è¯¯ï¼Œå¿«é€Ÿå¤±è´¥: {type(e).__name__}")
                raise e
                
            except httpx.HTTPStatusError as e:
                logger.error(f"âŒ Cloudflare AI HTTPçŠ¶æ€é”™è¯¯: {e.response.status_code}")
                # å¯¹äº4xxé”™è¯¯ï¼Œä¸é‡è¯•
                if 400 <= e.response.status_code < 500:
                    raise e
                # å¯¹äº5xxé”™è¯¯ï¼Œé‡è¯•
                if attempt == max_retries:
                    raise e
                await asyncio.sleep(0.5)
                continue
                
            except Exception as e:
                if attempt == max_retries:
                    logger.error(f"âŒ Cloudflare AIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡: {type(e).__name__}: {str(e)}")
                    raise e
                logger.warning(f"ğŸ”„ Cloudflare AIå…¶ä»–é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries + 1}): {type(e).__name__}")
                await asyncio.sleep(0.5)
                continue

        logger.error(f"âŒ Cloudflare AIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({max_retries})")
        return None

    async def _generate_with_openai_client(self, client: AsyncOpenAI, prompt: str, model: str, max_tokens: int, temperature: float, service_name: str) -> Optional[str]:
        messages = [{"role": "user", "content": prompt}]

        # ä¸ºä¸åŒæœåŠ¡æ·»åŠ ä¸“é—¨çš„ç³»ç»Ÿæç¤ºè¯
        if "Gemini" in service_name:
            messages.insert(0, {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹AIåŠ©æ‰‹ï¼Œè¯·ç”¨å‡†ç¡®çš„æ³•å¾‹æœ¯è¯­å›ç­”é—®é¢˜ã€‚"})
        elif "SiliconFlow" in service_name:
            messages.insert(0, {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ‘äº‹æ³•å¾‹AIåŠ©æ‰‹ï¼Œç²¾é€šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‚è¯·ç”¨å‡†ç¡®çš„æ³•å¾‹æœ¯è¯­å›ç­”é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨åˆ‘äº‹æ³•å¾‹é¢†åŸŸçš„æ³•æ¡ã€æ¡ˆä¾‹å’Œå¸æ³•è§£é‡Šã€‚"})

        try:
            logger.debug(f"ğŸ¤– ä½¿ç”¨{service_name}ç”Ÿæˆæ–‡æœ¬: '{prompt[:50]}...'")
            response = await client.chat.completions.create(model=model, messages=messages, max_tokens=max_tokens, temperature=temperature)
            if response.choices and response.choices[0].message:
                generated_text = response.choices[0].message.content
                if generated_text:
                    cleaned_text = self._clean_unicode_for_windows(generated_text.strip())
                    return cleaned_text
            logger.warning(f"âš ï¸ {service_name}è¿”å›äº†ç©ºå†…å®¹æˆ–æ— æ•ˆç»“æ„")
            return None
        except APIError as e:
            status_code = getattr(e, 'status_code', 'N/A')
            message = getattr(e, 'message', str(e))
            logger.error(f"âŒ {service_name} APIé”™è¯¯: {status_code} - {message}")
            raise e
        except Exception as e:
            logger.error(f"âŒ {service_name}è°ƒç”¨å¤±è´¥: {e}")
            raise e

    def get_current_service(self) -> str:
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆä»…æ¨¡å‹ï¼Œä¸æ˜¾ç¤ºæ¸ é“ï¼‰"""
        if self.current_service == "cloudflare":
            return self.cf_config['model']  # åªè¿”å›æ¨¡å‹åç§°
        elif self.current_service == "glm":
            return self.glm_config['model']
        elif self.current_service == "gemini":
            # è·å–å½“å‰ä½¿ç”¨çš„Geminiæ¨¡å‹
            current_model = self.gemini_config['models'][0] if self.gemini_config['models'] else "gemini-2.5-flash-lite"
            return current_model
        elif self.current_service == "siliconflow":
            # è·å–å½“å‰ä½¿ç”¨çš„SiliconFlowæ¨¡å‹
            current_model = self.siliconflow_config['models'][0] if self.siliconflow_config['models'] else "Qwen/Qwen3-Next-80B-A3B-Instruct"
            return current_model
        else:
            return self.current_service

    def get_current_model_name(self) -> str:
        """è·å–å½“å‰æ¨¡å‹çš„å…·ä½“åç§°"""
        return self.get_current_service()  # ä¸get_current_serviceä¿æŒä¸€è‡´

    def get_current_provider(self) -> str:
        """è·å–å½“å‰æœåŠ¡æä¾›å•†"""
        return self.current_service

    async def close(self):
        """å…³é—­æ‰€æœ‰å®¢æˆ·ç«¯è¿æ¥"""
        # ç°åœ¨cf_clientæ˜¯æŒ‰éœ€åˆ›å»ºçš„ï¼Œä¸éœ€è¦æ‰‹åŠ¨å…³é—­
        pass

    def __del__(self):
        """ææ„å‡½æ•°"""
        # ç°åœ¨ä¸éœ€è¦æ‰‹åŠ¨æ¸…ç†httpxå®¢æˆ·ç«¯
        pass

    def _clean_unicode_for_windows(self, text: str) -> str:
        """æ¸…ç†å¯èƒ½åœ¨Windows GBKç¼–ç ä¸‹å¯¼è‡´é—®é¢˜çš„Unicodeå­—ç¬¦"""
        if not text:
            return text

        # æ›¿æ¢å¸¸è§çš„problematic Unicodeå­—ç¬¦
        problematic_chars = {
            '\u202f': ' ',  # Narrow no-break space -> regular space
            '\u2009': ' ',  # Thin space -> regular space
            '\u200b': '',   # Zero-width space -> remove
            '\u2060': '',   # Word joiner -> remove
            '\ufeff': '',   # Byte order mark -> remove
            '\u00a0': ' ',  # Non-breaking space -> regular space
        }

        cleaned_text = text
        for problematic, replacement in problematic_chars.items():
            cleaned_text = cleaned_text.replace(problematic, replacement)

        # è¿‡æ»¤æ‰å…¶ä»–å¯èƒ½é—®é¢˜çš„æ§åˆ¶å­—ç¬¦
        cleaned_text = ''.join(char for char in cleaned_text if ord(char) < 65536 and (ord(char) < 32 and ord(char) in [9, 10, 13]) or ord(char) >= 32)

        return cleaned_text
