#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæ–‡æœ¬å‘é‡åŒ–æ¨¡å‹
ä½¿ç”¨TF-IDFå’Œç®€å•çš„æ–‡æœ¬å¤„ç†å®ç°åŸºæœ¬çš„å‘é‡åŒ–åŠŸèƒ½
é€‚ç”¨äºä½é…ç½®ç¯å¢ƒå’Œå¿«é€ŸåŸå‹å¼€å‘
"""

import os
import sys
import numpy as np
import pickle
from typing import List, Union, Optional
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.config.settings import settings
from src.utils.logger import setup_logger

logger = setup_logger(__name__)

class SimpleTextEmbedding:
    """ç®€åŒ–ç‰ˆæ–‡æœ¬å‘é‡åŒ–æ¨¡å‹ç±»"""
    
    def __init__(self, 
                 cache_dir: str = None,
                 max_features: int = None):
        """
        åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹
        
        Args:
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            max_features: TF-IDFæœ€å¤§ç‰¹å¾æ•°
        """
        self.cache_dir = cache_dir or settings.MODEL_CACHE_DIR
        self.max_features = max_features or settings.EMBEDDING_DIM
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=self.max_features,
            stop_words=None,  # ä¸­æ–‡æ²¡æœ‰é¢„å®šä¹‰åœç”¨è¯
            ngram_range=(1, 2),  # ä½¿ç”¨1-gramå’Œ2-gram
            min_df=1,  # æœ€å°æ–‡æ¡£é¢‘ç‡
            max_df=0.95,  # æœ€å¤§æ–‡æ¡£é¢‘ç‡
            sublinear_tf=True,  # ä½¿ç”¨sublinear TF scaling
            norm='l2'  # L2å½’ä¸€åŒ–
        )
        
        # æ˜¯å¦å·²è®­ç»ƒæ ‡å¿—
        self._is_fitted = False
        self._document_vectors = None
        
        logger.info(f"ç®€åŒ–å‘é‡åŒ–æ¨¡å‹åˆå§‹åŒ–é…ç½®:")
        logger.info(f"  æœ€å¤§ç‰¹å¾æ•°: {self.max_features}")
        logger.info(f"  ç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def _preprocess_text(self, text: str) -> str:
        """
        ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬
        """
        if not isinstance(text, str):
            text = str(text)
        
        # åŸºç¡€æ¸…ç†
        text = text.strip()
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = ' '.join(text.split())
        
        # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        try:
            # ä½¿ç”¨ç²¾ç¡®æ¨¡å¼åˆ†è¯
            words = jieba.cut(text, cut_all=False)
            # è¿‡æ»¤å•å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
            words = [word for word in words if len(word.strip()) > 1 and word.strip().isalnum()]
            text = ' '.join(words)
        except Exception as e:
            logger.warning(f"åˆ†è¯å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡: {e}")
        
        # æˆªæ–­è¿‡é•¿æ–‡æœ¬
        max_chars = self.max_features * 2
        if len(text) > max_chars:
            text = text[:max_chars]
            logger.debug("æ–‡æœ¬è¿‡é•¿ï¼Œå·²æˆªæ–­")
        
        return text

    def fit(self, texts: List[str]):
        """
        åœ¨æ–‡æ¡£é›†åˆä¸Šè®­ç»ƒTF-IDFå‘é‡åŒ–å™¨
        
        Args:
            texts: æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹åœ¨ {len(texts)} ä¸ªæ–‡æ¡£ä¸Šè®­ç»ƒTF-IDFå‘é‡åŒ–å™¨...")
        
        # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [self._preprocess_text(text) for text in texts]
        
        # è®­ç»ƒTF-IDFå‘é‡åŒ–å™¨
        self.vectorizer.fit(processed_texts)
        self._is_fitted = True
        
        # è®¡ç®—æ‰€æœ‰æ–‡æ¡£çš„å‘é‡è¡¨ç¤º
        self._document_vectors = self.vectorizer.transform(processed_texts)
        
        logger.info(f"TF-IDFè®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(self.vectorizer.vocabulary_)}")

    def encode_query(self, query: str) -> np.ndarray:
        """
        ç¼–ç å•ä¸ªæŸ¥è¯¢æ–‡æœ¬
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            æŸ¥è¯¢å‘é‡ (1, max_features)
        """
        if not query or not query.strip():
            raise ValueError("æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        if not self._is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")
        
        try:
            # é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬
            processed_query = self._preprocess_text(query)
            
            # å‘é‡åŒ–
            query_vector = self.vectorizer.transform([processed_query])
            
            # è½¬æ¢ä¸ºdenseæ•°ç»„
            query_dense = query_vector.toarray().astype(np.float32)
            
            logger.debug(f"æŸ¥è¯¢å‘é‡åŒ–å®Œæˆï¼Œå½¢çŠ¶: {query_dense.shape}")
            return query_dense
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢ç¼–ç å¤±è´¥: {e}")
            raise

    def encode_documents(self, texts: List[str]) -> np.ndarray:
        """
        æ‰¹é‡ç¼–ç æ–‡æ¡£æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æ–‡æ¡£å‘é‡çŸ©é˜µ (num_docs, max_features)
        """
        if not texts:
            raise ValueError("æ–‡æ¡£åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if not self._is_fitted:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")
        
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡ç¼–ç  {len(texts)} ä¸ªæ–‡æ¡£")
            
            # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
            processed_texts = [self._preprocess_text(text) for text in texts]
            
            # å‘é‡åŒ–
            doc_vectors = self.vectorizer.transform(processed_texts)
            
            # è½¬æ¢ä¸ºdenseæ•°ç»„
            doc_dense = doc_vectors.toarray().astype(np.float32)
            
            logger.info(f"æ–‡æ¡£ç¼–ç å®Œæˆï¼Œå‘é‡çŸ©é˜µå½¢çŠ¶: {doc_dense.shape}")
            return doc_dense
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ‰¹é‡ç¼–ç å¤±è´¥: {e}")
            raise

    def get_embedding_dim(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        if self._is_fitted:
            return len(self.vectorizer.vocabulary_)
        else:
            return self.max_features

    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„"""
        try:
            model_data = {
                'vectorizer': self.vectorizer,
                'is_fitted': self._is_fitted,
                'max_features': self.max_features
            }
            
            with open(save_path, 'wb') as f:
                pickle.dump(model_data, f)
                
            logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        except Exception as e:
            logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise

    def load_model(self, load_path: str):
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½æ¨¡å‹"""
        try:
            with open(load_path, 'rb') as f:
                model_data = pickle.load(f)
            
            self.vectorizer = model_data['vectorizer']
            self._is_fitted = model_data['is_fitted']
            self.max_features = model_data.get('max_features', self.max_features)
            
            logger.info(f"æ¨¡å‹å·²ä» {load_path} åŠ è½½")
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def compute_similarity(self, query_vector: np.ndarray, doc_vectors: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡ (1, dim)
            doc_vectors: æ–‡æ¡£å‘é‡çŸ©é˜µ (num_docs, dim)
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°æ•°ç»„ (num_docs,)
        """
        try:
            # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_vector, doc_vectors)[0]
            return similarities
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            raise

# æµ‹è¯•å‡½æ•°
def test_simple_embedding_model():
    """æµ‹è¯•ç®€åŒ–ç‰ˆå‘é‡åŒ–æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½"""
    try:
        logger.info("å¼€å§‹æµ‹è¯•ç®€åŒ–å‘é‡åŒ–æ¨¡å‹...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_docs = [
            "å½“äº‹äººä¸€æ–¹ä¸å±¥è¡ŒåˆåŒä¹‰åŠ¡æˆ–è€…å±¥è¡ŒåˆåŒä¹‰åŠ¡ä¸ç¬¦åˆçº¦å®šçš„ï¼Œåº”å½“æ‰¿æ‹…ç»§ç»­å±¥è¡Œã€é‡‡å–è¡¥æ•‘æªæ–½æˆ–è€…èµ”å¿æŸå¤±ç­‰è¿çº¦è´£ä»»ã€‚",
            "å…¬å¸æ˜¯ä¼ä¸šæ³•äººï¼Œæœ‰ç‹¬ç«‹çš„æ³•äººè´¢äº§ï¼Œäº«æœ‰æ³•äººè´¢äº§æƒã€‚å…¬å¸ä»¥å…¶å…¨éƒ¨è´¢äº§å¯¹å…¬å¸çš„å€ºåŠ¡æ‰¿æ‹…è´£ä»»ã€‚",
            "æ•…æ„ä¼¤å®³ä»–äººèº«ä½“çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€æ‹˜å½¹æˆ–è€…ç®¡åˆ¶ã€‚è‡´äººé‡ä¼¤çš„ï¼Œå¤„ä¸‰å¹´ä»¥ä¸Šåå¹´ä»¥ä¸‹æœ‰æœŸå¾’åˆ‘ã€‚",
            "åˆåŒå½“äº‹äººåº”å½“æŒ‰ç…§çº¦å®šå…¨é¢å±¥è¡Œè‡ªå·±çš„ä¹‰åŠ¡ã€‚å½“äº‹äººåº”å½“éµå¾ªè¯šå®ä¿¡ç”¨åŸåˆ™ï¼Œæ ¹æ®åˆåŒçš„æ€§è´¨ã€ç›®çš„å’Œäº¤æ˜“ä¹ æƒ¯å±¥è¡Œé€šçŸ¥ã€ååŠ©ã€ä¿å¯†ç­‰ä¹‰åŠ¡ã€‚"
        ]
        
        # åˆå§‹åŒ–æ¨¡å‹
        embedding_model = SimpleTextEmbedding()
        
        # è®­ç»ƒæ¨¡å‹
        embedding_model.fit(test_docs)
        
        # æµ‹è¯•æŸ¥è¯¢ç¼–ç 
        test_query = "åˆåŒè¿çº¦çš„æ³•å¾‹è´£ä»»"
        query_embedding = embedding_model.encode_query(test_query)
        logger.info(f"æŸ¥è¯¢ç¼–ç æµ‹è¯•æˆåŠŸ: {query_embedding.shape}")
        
        # æµ‹è¯•æ–‡æ¡£ç¼–ç 
        doc_embeddings = embedding_model.encode_documents(test_docs)
        logger.info(f"æ–‡æ¡£ç¼–ç æµ‹è¯•æˆåŠŸ: {doc_embeddings.shape}")
        
        # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
        similarities = embedding_model.compute_similarity(query_embedding, doc_embeddings)
        logger.info(f"ç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•æˆåŠŸ: {similarities.shape}, æœ€é«˜ç›¸ä¼¼åº¦: {similarities.max():.3f}")
        
        # éªŒè¯å‘é‡ç»´åº¦
        expected_max_features = embedding_model.get_embedding_dim()
        logger.info(f"å‘é‡ç»´åº¦: {expected_max_features}")
        
        # æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
        test_model_path = os.path.join(embedding_model.cache_dir, "test_model.pkl")
        embedding_model.save_model(test_model_path)
        
        # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½
        new_model = SimpleTextEmbedding()
        new_model.load_model(test_model_path)
        
        # éªŒè¯åŠ è½½çš„æ¨¡å‹
        query_embedding2 = new_model.encode_query(test_query)
        assert np.allclose(query_embedding, query_embedding2), "æ¨¡å‹ä¿å­˜/åŠ è½½åç»“æœä¸ä¸€è‡´"
        
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if os.path.exists(test_model_path):
            os.remove(test_model_path)
        
        logger.info("âœ… ç®€åŒ–å‘é‡åŒ–æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç®€åŒ–å‘é‡åŒ–æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = test_simple_embedding_model()
    if success:
        print("\nğŸ‰ ç®€åŒ–ç‰ˆæ–‡æœ¬å‘é‡åŒ–æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
        print("ğŸ“Œ ä¸‹ä¸€æ­¥: å¯ä»¥å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•ç³»ç»Ÿ")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")