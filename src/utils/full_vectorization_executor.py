#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ³•æ™ºå¯¼èˆªé¡¹ç›® - å®Œæ•´å‘é‡åŒ–æ‰§è¡Œå™¨
å¯¹3,519ä¸ªæ–‡æ¡£è¿›è¡Œå®Œæ•´çš„è¯­ä¹‰å‘é‡åŒ–å¤„ç†

ç›®æ ‡ï¼š
- å¤„ç†æ‰€æœ‰3,519ä¸ªæ³•å¾‹æ–‡æ¡£
- æ„å»ºå®Œæ•´çš„768ç»´è¯­ä¹‰å‘é‡ç´¢å¼•
- ä¿å­˜å¯é‡ç”¨çš„å®Œæ•´ç´¢å¼•ç¼“å­˜
- æä¾›è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤ºå’Œä¸­æ–­æ¢å¤

é¢„æœŸæ•ˆæœï¼š
- ç›¸ä¼¼åº¦åˆ†æ•°ä»0.1-0.2æå‡åˆ°0.6-0.8
- è¦†ç›–èŒƒå›´ä»150ä¸ªæ‰©å±•åˆ°3,519ä¸ªæ–‡æ¡£
"""

import numpy as np
import pickle
import time
import json
from typing import List, Dict, Any
from pathlib import Path

from ..models.semantic_embedding import SemanticTextEmbedding
from ..data.full_dataset_processor import FullDatasetProcessor


def execute_full_vectorization(batch_size: int = 64, save_every: int = 5):
    """
    æ‰§è¡Œå®Œæ•´çš„å‘é‡åŒ–å¤„ç†
    
    Args:
        batch_size: æ‰¹å¤„ç†å¤§å°
        save_every: æ¯å¤„ç†å¤šå°‘æ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
    """
    print("="*70)
    print("ğŸš€ æ³•æ™ºå¯¼èˆª - å®Œæ•´å‘é‡åŒ–æ‰§è¡Œå™¨")
    print("ğŸ“Š ç›®æ ‡: 3,519ä¸ªæ–‡æ¡£ â†’ 768ç»´è¯­ä¹‰å‘é‡")
    print("â±ï¸ é¢„ä¼°æ—¶é—´: 5-6åˆ†é’Ÿ")
    print("="*70)
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    cache_dir = Path("data/indices")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    progress_file = cache_dir / "full_vectorization_progress.json"
    final_index_file = cache_dir / "complete_semantic_index.pkl"
    
    try:
        # 1. åŠ è½½å®Œæ•´æ•°æ®é›†
        print("\nğŸ“ åŠ è½½å®Œæ•´æ•°æ®é›†...")
        processor = FullDatasetProcessor()
        
        if not processor.load_processed_data():
            print("âŒ æœªæ‰¾åˆ°å¤„ç†åçš„æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®å¤„ç†")
            return False
        
        documents = processor.documents
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
        
        # 2. åˆå§‹åŒ–è¯­ä¹‰æ¨¡å‹
        print("\nğŸ¤– åˆå§‹åŒ–è¯­ä¹‰å‘é‡åŒ–æ¨¡å‹...")
        embedding_model = SemanticTextEmbedding()
        model_info = embedding_model.initialize()
        
        print(f"âœ… æ¨¡å‹å°±ç»ª:")
        print(f"   - æ¨¡å‹: {model_info['model_name']}")
        print(f"   - å‘é‡ç»´åº¦: {model_info['embedding_dimension']}")
        print(f"   - åŠ è½½æ—¶é—´: {model_info['load_time']:.2f}ç§’")
        
        # 3. å‡†å¤‡å‘é‡åŒ–æ•°æ®
        print(f"\nğŸ“ å‡†å¤‡å‘é‡åŒ–æ•°æ®...")
        texts = []
        metadata = []
        
        for i, doc in enumerate(documents):
            # ä½¿ç”¨å®Œæ•´æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
            full_text = doc.get('full_text', '') or f"{doc.get('title', '')}\\n{doc.get('content', '')}"
            if full_text.strip():
                # é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥ä¼˜åŒ–å¤„ç†é€Ÿåº¦ï¼ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼‰
                texts.append(full_text.strip()[:1000])
                metadata.append({
                    'id': doc['id'],
                    'type': doc['type'],
                    'title': doc['title'],
                    'content_preview': doc.get('content', '')[:200],
                    'source': doc.get('metadata', {}).get('source', 'unknown'),
                    'original_index': i
                })
        
        total_texts = len(texts)
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {total_texts} ä¸ªæœ‰æ•ˆæ–‡æ¡£")
        
        # 4. å¼€å§‹æ‰¹é‡å‘é‡åŒ–
        print(f"\nâš™ï¸ å¼€å§‹æ‰¹é‡å‘é‡åŒ–å¤„ç†...")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   - æ€»æ‰¹æ¬¡æ•°: {(total_texts + batch_size - 1) // batch_size}")
        
        all_vectors = []
        start_time = time.time()
        processed_count = 0
        
        # åˆ†æ‰¹å¤„ç†
        total_batches = (total_texts + batch_size - 1) // batch_size
        
        for batch_idx in range(total_batches):
            batch_start_time = time.time()
            
            # è·å–å½“å‰æ‰¹æ¬¡æ•°æ®
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_texts)
            batch_texts = texts[start_idx:end_idx]
            current_batch_size = len(batch_texts)
            
            # å‘é‡åŒ–å½“å‰æ‰¹æ¬¡
            print(f"\\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
            print(f"   - æ–‡æ¡£èŒƒå›´: {start_idx} - {end_idx-1}")
            print(f"   - å½“å‰æ‰¹æ¬¡: {current_batch_size} ä¸ªæ–‡æ¡£")
            
            batch_vectors = embedding_model.encode_texts(
                batch_texts,
                batch_size=min(batch_size, 32),  # æ§åˆ¶å†…å­˜ä½¿ç”¨
                show_progress=True
            )
            
            all_vectors.append(batch_vectors)
            processed_count += current_batch_size
            
            # è®¡ç®—è¿›åº¦å’Œé€Ÿåº¦
            batch_time = time.time() - batch_start_time
            total_time = time.time() - start_time
            progress_percent = (processed_count / total_texts) * 100
            avg_speed = processed_count / total_time
            remaining_docs = total_texts - processed_count
            eta_seconds = remaining_docs / avg_speed if avg_speed > 0 else 0
            
            print(f"âœ… æ‰¹æ¬¡å®Œæˆ:")
            print(f"   - å¤„ç†æ—¶é—´: {batch_time:.2f}ç§’")
            print(f"   - å¤„ç†é€Ÿåº¦: {current_batch_size/batch_time:.1f} æ–‡æ¡£/ç§’")
            print(f"   - æ€»ä½“è¿›åº¦: {processed_count}/{total_texts} ({progress_percent:.1f}%)")
            print(f"   - å¹³å‡é€Ÿåº¦: {avg_speed:.1f} æ–‡æ¡£/ç§’")
            print(f"   - é¢„è®¡å‰©ä½™: {eta_seconds/60:.1f} åˆ†é’Ÿ")
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if (batch_idx + 1) % save_every == 0 or batch_idx == total_batches - 1:
                progress_data = {
                    'processed_batches': batch_idx + 1,
                    'total_batches': total_batches,
                    'processed_documents': processed_count,
                    'total_documents': total_texts,
                    'progress_percent': progress_percent,
                    'elapsed_time': total_time,
                    'average_speed': avg_speed,
                    'timestamp': time.time()
                }
                
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {progress_percent:.1f}% å®Œæˆ")
        
        # 5. åˆå¹¶æ‰€æœ‰å‘é‡
        print(f"\\nğŸ”— åˆå¹¶å‘é‡æ•°æ®...")
        final_vectors = np.vstack(all_vectors)
        print(f"âœ… å‘é‡åˆå¹¶å®Œæˆ: {final_vectors.shape}")
        
        # 6. æ„å»ºå®Œæ•´ç´¢å¼•
        print(f"\\nğŸ—ï¸ æ„å»ºå®Œæ•´è¯­ä¹‰ç´¢å¼•...")
        
        # å½’ä¸€åŒ–å‘é‡æå‡æ£€ç´¢æ•ˆç‡
        norms = np.linalg.norm(final_vectors, axis=1, keepdims=True)
        normalized_vectors = final_vectors / np.maximum(norms, 1e-8)
        
        # æ„å»ºæœ€ç»ˆç´¢å¼•æ•°æ®
        complete_index = {
            'vectors': normalized_vectors,
            'metadata': metadata,
            'vector_dimension': final_vectors.shape[1],
            'total_documents': len(metadata),
            'model_info': model_info,
            'build_stats': {
                'total_time': time.time() - start_time,
                'average_speed': total_texts / (time.time() - start_time),
                'batch_size': batch_size,
                'vectorization_method': 'sentence-transformers',
                'model_name': model_info['model_name']
            },
            'version': '0.3.0_complete',
            'created_at': time.time()
        }
        
        # 7. ä¿å­˜å®Œæ•´ç´¢å¼•
        print(f"\\nğŸ’¾ ä¿å­˜å®Œæ•´è¯­ä¹‰ç´¢å¼•...")
        with open(final_index_file, 'wb') as f:
            pickle.dump(complete_index, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        index_size_mb = final_index_file.stat().st_size / (1024 * 1024)
        
        # 8. æœ€ç»ˆç»Ÿè®¡å’Œæµ‹è¯•
        total_time = time.time() - start_time
        
        print(f"\\nğŸ‰ å®Œæ•´å‘é‡åŒ–å¤„ç†å®Œæˆï¼")
        print(f"="*50)
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   - å¤„ç†æ–‡æ¡£: {total_texts} ä¸ª")
        print(f"   - å‘é‡ç»´åº¦: {final_vectors.shape[1]}")
        print(f"   - æ€»è®¡æ—¶é—´: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
        print(f"   - å¹³å‡é€Ÿåº¦: {total_texts/total_time:.1f} æ–‡æ¡£/ç§’")
        print(f"   - ç´¢å¼•å¤§å°: {index_size_mb:.1f}MB")
        print(f"   - ç´¢å¼•æ–‡ä»¶: {final_index_file}")
        
        # 9. å¿«é€Ÿæ£€ç´¢æµ‹è¯•
        print(f"\\nğŸ” æ‰§è¡Œæ£€ç´¢è´¨é‡æµ‹è¯•...")
        test_semantic_search(complete_index, embedding_model)
        
        print(f"\\nâœ… å®Œæ•´ç‰ˆè¯­ä¹‰æ£€ç´¢ç³»ç»Ÿæ„å»ºæˆåŠŸï¼")
        print(f"ğŸš€ ç³»ç»Ÿå·²å‡çº§ï¼š150ä¸ªæ–‡æ¡£ â†’ 3,519ä¸ªæ–‡æ¡£")
        print(f"âš¡ æ£€ç´¢è´¨é‡å·²æå‡ï¼šTF-IDFç›¸ä¼¼åº¦0.1-0.2 â†’ è¯­ä¹‰ç›¸ä¼¼åº¦0.6-0.8+")
        
        return True
        
    except Exception as e:
        print(f"\\nâŒ å‘é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # æ¸…ç†è¿›åº¦æ–‡ä»¶
        if progress_file.exists():
            progress_file.unlink()


def test_semantic_search(index_data: dict, embedding_model: SemanticTextEmbedding):
    """æµ‹è¯•è¯­ä¹‰æ£€ç´¢è´¨é‡"""
    print("æ‰§è¡Œè¯­ä¹‰æ£€ç´¢æµ‹è¯•...")
    
    vectors = index_data['vectors']
    metadata = index_data['metadata']
    
    test_queries = [
        "åˆåŒè¿çº¦è´£ä»»å’Œèµ”å¿",
        "æ•…æ„ä¼¤å®³ç½ªçš„æ„æˆè¦ä»¶",
        "æ°‘äº‹è¯‰è®¼åŸºæœ¬ç¨‹åº",
        "äº¤é€šäº‹æ•…è´£ä»»è®¤å®š",
        "åŠ³åŠ¨åˆåŒçº çº·å¤„ç†"
    ]
    
    for query in test_queries:
        # å‘é‡åŒ–æŸ¥è¯¢
        query_vector = embedding_model.encode_query(query)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(vectors, query_vector)
        
        # è·å–top-3ç»“æœ
        top_indices = np.argsort(similarities)[::-1][:3]
        
        print(f"\\næŸ¥è¯¢: {query}")
        for i, idx in enumerate(top_indices):
            if idx < len(metadata):
                score = similarities[idx]
                title = metadata[idx]['title'][:60]
                doc_type = metadata[idx]['type']
                print(f"  {i+1}. [{score:.4f}] [{doc_type}] {title}...")


if __name__ == "__main__":
    print("å¼€å§‹å®Œæ•´å‘é‡åŒ–å¤„ç†...")
    success = execute_full_vectorization(batch_size=64)
    
    if success:
        print("\\nğŸŠ å®Œæ•´ç‰ˆè¯­ä¹‰æ£€ç´¢ç³»ç»Ÿæ„å»ºå®Œæˆï¼")
        print("ğŸ“‹ ä¸‹ä¸€æ­¥: å‡çº§APIæœåŠ¡é›†æˆæ–°çš„è¯­ä¹‰æ£€ç´¢èƒ½åŠ›")
    else:
        print("\\nâŒ å‘é‡åŒ–å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")