#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å·¥å…·
éªŒè¯criminalå’Œvectorsç›®å½•æ•°æ®çš„ä¸€è‡´æ€§

ä½¿ç”¨æ–¹æ³•:
python tools/data_validation/data_integrity_checker.py
"""

import sys
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Tuple
from datetime import datetime
import logging
from sklearn.metrics.pairwise import cosine_similarity

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataIntegrityChecker:
    """æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, data_root: Optional[Path] = None):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.data_root = data_root or project_root / "data" / "processed"
        self.vectors_dir = self.data_root / "vectors"
        self.criminal_dir = self.data_root / "criminal"
        
        # æ£€æŸ¥ç»“æœ
        self.results = {}
        
        # æ•°æ®å­˜å‚¨
        self.vector_data = {}
        self.criminal_data = {}
        
    def check_all_integrity(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ‰€æœ‰æ•°æ®å®Œæ•´æ€§"""
        print("=" * 60)
        print("ğŸ” æ•°æ®å®Œæ•´æ€§éªŒè¯å·¥å…· - å¼€å§‹æ£€æŸ¥")
        print("=" * 60)
        
        # æ£€æŸ¥ç›®å½•å­˜åœ¨
        if not self._check_directories():
            return {'error': 'æ•°æ®ç›®å½•ä¸å­˜åœ¨'}
        
        # åŠ è½½æ•°æ®
        if not self._load_all_data():
            return {'error': 'æ•°æ®åŠ è½½å¤±è´¥'}
        
        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        self.results['file_consistency'] = self._check_file_consistency()
        self.results['id_mapping'] = self._check_id_mapping()
        self.results['content_integrity'] = self._check_content_integrity()
        self.results['vector_quality'] = self._check_vector_quality()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.results['summary'] = self._generate_summary()
        
        return self.results
    
    def _check_directories(self) -> bool:
        """æ£€æŸ¥ç›®å½•å­˜åœ¨æ€§"""
        print("ğŸ“ æ£€æŸ¥æ•°æ®ç›®å½•...")
        
        if not self.vectors_dir.exists():
            print(f"   âŒ å‘é‡ç›®å½•ä¸å­˜åœ¨: {self.vectors_dir}")
            return False
        print(f"   âœ… å‘é‡ç›®å½•å­˜åœ¨: {self.vectors_dir}")
        
        if not self.criminal_dir.exists():
            print(f"   âŒ åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.criminal_dir}")
            return False
        print(f"   âœ… åŸå§‹æ•°æ®ç›®å½•å­˜åœ¨: {self.criminal_dir}")
        
        return True
    
    def _load_all_data(self) -> bool:
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("\nğŸ“¥ åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        try:
            # åŠ è½½å‘é‡æ•°æ®
            articles_vectors_file = self.vectors_dir / "criminal_articles_vectors.pkl"
            cases_vectors_file = self.vectors_dir / "criminal_cases_vectors.pkl"
            
            if articles_vectors_file.exists():
                with open(articles_vectors_file, 'rb') as f:
                    self.vector_data['articles'] = pickle.load(f)
                print(f"   âœ… åŠ è½½æ³•æ¡å‘é‡æ•°æ®: {self.vector_data['articles']['total_count']}æ¡")
            else:
                print(f"   âŒ æ³•æ¡å‘é‡æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            if cases_vectors_file.exists():
                with open(cases_vectors_file, 'rb') as f:
                    self.vector_data['cases'] = pickle.load(f)
                print(f"   âœ… åŠ è½½æ¡ˆä¾‹å‘é‡æ•°æ®: {self.vector_data['cases']['total_count']}æ¡")
            else:
                print(f"   âŒ æ¡ˆä¾‹å‘é‡æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # åŠ è½½åŸå§‹æ•°æ®
            articles_criminal_file = self.criminal_dir / "criminal_articles.pkl"
            cases_criminal_file = self.criminal_dir / "criminal_cases.pkl"
            
            if articles_criminal_file.exists():
                with open(articles_criminal_file, 'rb') as f:
                    self.criminal_data['articles'] = pickle.load(f)
                print(f"   âœ… åŠ è½½æ³•æ¡åŸå§‹æ•°æ®: {len(self.criminal_data['articles'])}æ¡")
            else:
                print(f"   âŒ æ³•æ¡åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            if cases_criminal_file.exists():
                with open(cases_criminal_file, 'rb') as f:
                    self.criminal_data['cases'] = pickle.load(f)
                print(f"   âœ… åŠ è½½æ¡ˆä¾‹åŸå§‹æ•°æ®: {len(self.criminal_data['cases'])}æ¡")
            else:
                print(f"   âŒ æ¡ˆä¾‹åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            return True
            
        except Exception as e:
            print(f"   âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            logger.exception("Failed to load data")
            return False
    
    def _check_file_consistency(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡ä»¶ä¸€è‡´æ€§"""
        print(f"\nğŸ”— æ£€æŸ¥æ–‡ä»¶ä¸€è‡´æ€§...")
        
        consistency = {'articles': {}, 'cases': {}}
        
        # æ£€æŸ¥æ³•æ¡æ•°æ®ä¸€è‡´æ€§
        articles_vector_count = self.vector_data['articles']['total_count']
        articles_criminal_count = len(self.criminal_data['articles'])
        
        print(f"   ğŸ“Š æ³•æ¡æ•°æ®:")
        print(f"      å‘é‡æ–‡ä»¶: {articles_vector_count}æ¡")
        print(f"      åŸå§‹æ–‡ä»¶: {articles_criminal_count}æ¡")
        
        if articles_vector_count == articles_criminal_count:
            print(f"      âœ… æ³•æ¡æ•°é‡ä¸€è‡´")
            consistency['articles']['count_match'] = True
        else:
            print(f"      âŒ æ³•æ¡æ•°é‡ä¸ä¸€è‡´")
            consistency['articles']['count_match'] = False
        
        consistency['articles']['vector_count'] = articles_vector_count
        consistency['articles']['criminal_count'] = articles_criminal_count
        
        # æ£€æŸ¥æ¡ˆä¾‹æ•°æ®ä¸€è‡´æ€§
        cases_vector_count = self.vector_data['cases']['total_count']
        cases_criminal_count = len(self.criminal_data['cases'])
        
        print(f"   ğŸ“Š æ¡ˆä¾‹æ•°æ®:")
        print(f"      å‘é‡æ–‡ä»¶: {cases_vector_count}æ¡")
        print(f"      åŸå§‹æ–‡ä»¶: {cases_criminal_count}æ¡")
        
        if cases_vector_count == cases_criminal_count:
            print(f"      âœ… æ¡ˆä¾‹æ•°é‡ä¸€è‡´")
            consistency['cases']['count_match'] = True
        else:
            print(f"      âŒ æ¡ˆä¾‹æ•°é‡ä¸ä¸€è‡´")
            consistency['cases']['count_match'] = False
        
        consistency['cases']['vector_count'] = cases_vector_count
        consistency['cases']['criminal_count'] = cases_criminal_count
        
        return consistency
    
    def _check_id_mapping(self) -> Dict[str, Any]:
        """æ£€æŸ¥IDæ˜ å°„å…³ç³»"""
        print(f"\nğŸ”‘ æ£€æŸ¥IDæ˜ å°„å…³ç³»...")
        
        mapping = {'articles': {}, 'cases': {}}
        
        # æ£€æŸ¥æ³•æ¡IDæ˜ å°„
        vector_article_ids = set()
        criminal_article_ids = set()
        
        # ä»å‘é‡å…ƒæ•°æ®æå–ID
        for meta in self.vector_data['articles']['metadata']:
            if 'id' in meta:
                vector_article_ids.add(meta['id'])
            elif 'article_number' in meta:
                vector_article_ids.add(f"article_{meta['article_number']}")
        
        # ä»åŸå§‹æ•°æ®æå–ID
        for article in self.criminal_data['articles']:
            if hasattr(article, 'article_number'):
                criminal_article_ids.add(f"article_{article.article_number}")
            elif hasattr(article, 'id'):
                criminal_article_ids.add(article.id)
            elif isinstance(article, dict):
                if 'article_number' in article:
                    criminal_article_ids.add(f"article_{article['article_number']}")
                elif 'id' in article:
                    criminal_article_ids.add(article['id'])
        
        print(f"   ğŸ“Š æ³•æ¡IDå¯¹æ¯”:")
        print(f"      å‘é‡æ•°æ®IDæ•°é‡: {len(vector_article_ids)}")
        print(f"      åŸå§‹æ•°æ®IDæ•°é‡: {len(criminal_article_ids)}")
        
        # è®¡ç®—äº¤é›†å’Œå·®é›†
        common_article_ids = vector_article_ids & criminal_article_ids
        missing_in_vector = criminal_article_ids - vector_article_ids
        missing_in_criminal = vector_article_ids - criminal_article_ids
        
        print(f"      å…±åŒID: {len(common_article_ids)}")
        if missing_in_vector:
            print(f"      âŒ å‘é‡ä¸­ç¼ºå¤±: {len(missing_in_vector)}ä¸ª")
        if missing_in_criminal:
            print(f"      âŒ åŸå§‹æ•°æ®ä¸­ç¼ºå¤±: {len(missing_in_criminal)}ä¸ª")
        
        if len(common_article_ids) == len(vector_article_ids) == len(criminal_article_ids):
            print(f"      âœ… æ³•æ¡IDå®Œå…¨åŒ¹é…")
            mapping['articles']['perfect_match'] = True
        else:
            print(f"      âŒ æ³•æ¡IDä¸å®Œå…¨åŒ¹é…")
            mapping['articles']['perfect_match'] = False
        
        mapping['articles']['common_ids'] = len(common_article_ids)
        mapping['articles']['missing_in_vector'] = len(missing_in_vector)
        mapping['articles']['missing_in_criminal'] = len(missing_in_criminal)
        mapping['articles']['missing_in_vector_list'] = list(missing_in_vector)[:10]  # åªä¿å­˜å‰10ä¸ªç”¨äºè°ƒè¯•
        mapping['articles']['missing_in_criminal_list'] = list(missing_in_criminal)[:10]
        
        # æ£€æŸ¥æ¡ˆä¾‹IDæ˜ å°„
        vector_case_ids = set()
        criminal_case_ids = set()
        
        # ä»å‘é‡å…ƒæ•°æ®æå–ID
        for meta in self.vector_data['cases']['metadata']:
            if 'case_id' in meta:
                vector_case_ids.add(meta['case_id'])
            elif 'id' in meta:
                vector_case_ids.add(meta['id'])
        
        # ä»åŸå§‹æ•°æ®æå–ID
        for case in self.criminal_data['cases']:
            if hasattr(case, 'case_id'):
                criminal_case_ids.add(case.case_id)
            elif hasattr(case, 'id'):
                criminal_case_ids.add(case.id)
            elif isinstance(case, dict):
                if 'case_id' in case:
                    criminal_case_ids.add(case['case_id'])
                elif 'id' in case:
                    criminal_case_ids.add(case['id'])
        
        print(f"   ğŸ“Š æ¡ˆä¾‹IDå¯¹æ¯”:")
        print(f"      å‘é‡æ•°æ®IDæ•°é‡: {len(vector_case_ids)}")
        print(f"      åŸå§‹æ•°æ®IDæ•°é‡: {len(criminal_case_ids)}")
        
        # è®¡ç®—äº¤é›†å’Œå·®é›†
        common_case_ids = vector_case_ids & criminal_case_ids
        missing_in_vector_cases = criminal_case_ids - vector_case_ids
        missing_in_criminal_cases = vector_case_ids - criminal_case_ids
        
        print(f"      å…±åŒID: {len(common_case_ids)}")
        if missing_in_vector_cases:
            print(f"      âŒ å‘é‡ä¸­ç¼ºå¤±: {len(missing_in_vector_cases)}ä¸ª")
        if missing_in_criminal_cases:
            print(f"      âŒ åŸå§‹æ•°æ®ä¸­ç¼ºå¤±: {len(missing_in_criminal_cases)}ä¸ª")
        
        if len(common_case_ids) == len(vector_case_ids) == len(criminal_case_ids):
            print(f"      âœ… æ¡ˆä¾‹IDå®Œå…¨åŒ¹é…")
            mapping['cases']['perfect_match'] = True
        else:
            print(f"      âŒ æ¡ˆä¾‹IDä¸å®Œå…¨åŒ¹é…")
            mapping['cases']['perfect_match'] = False
        
        mapping['cases']['common_ids'] = len(common_case_ids)
        mapping['cases']['missing_in_vector'] = len(missing_in_vector_cases)
        mapping['cases']['missing_in_criminal'] = len(missing_in_criminal_cases)
        mapping['cases']['missing_in_vector_list'] = list(missing_in_vector_cases)[:10]
        mapping['cases']['missing_in_criminal_list'] = list(missing_in_criminal_cases)[:10]
        
        return mapping
    
    def _check_content_integrity(self) -> Dict[str, Any]:
        """æ£€æŸ¥å†…å®¹å®Œæ•´æ€§"""
        print(f"\nğŸ“ æ£€æŸ¥å†…å®¹å®Œæ•´æ€§...")
        
        integrity = {'articles': {}, 'cases': {}}
        
        # æ£€æŸ¥æ³•æ¡å†…å®¹å®Œæ•´æ€§
        print(f"   ğŸ“Š æ³•æ¡å†…å®¹æ£€æŸ¥:")
        
        empty_content_articles = 0
        total_articles = len(self.criminal_data['articles'])
        
        for article in self.criminal_data['articles']:
            content = None
            if hasattr(article, 'content'):
                content = article.content
            elif hasattr(article, 'full_text'):
                content = article.full_text
            elif isinstance(article, dict):
                content = article.get('content') or article.get('full_text')
            
            if not content or content.strip() == "":
                empty_content_articles += 1
        
        if empty_content_articles == 0:
            print(f"      âœ… æ‰€æœ‰æ³•æ¡éƒ½æœ‰å†…å®¹")
            integrity['articles']['all_have_content'] = True
        else:
            print(f"      âŒ {empty_content_articles}/{total_articles}æ³•æ¡å†…å®¹ä¸ºç©º")
            integrity['articles']['all_have_content'] = False
        
        integrity['articles']['empty_content_count'] = empty_content_articles
        integrity['articles']['total_count'] = total_articles
        
        # æ£€æŸ¥æ¡ˆä¾‹å†…å®¹å®Œæ•´æ€§
        print(f"   ğŸ“Š æ¡ˆä¾‹å†…å®¹æ£€æŸ¥:")
        
        empty_content_cases = 0
        total_cases = len(self.criminal_data['cases'])
        
        for case in self.criminal_data['cases']:
            content = None
            if hasattr(case, 'fact'):
                content = case.fact
            elif hasattr(case, 'content'):
                content = case.content
            elif isinstance(case, dict):
                content = case.get('fact') or case.get('content')
            
            if not content or content.strip() == "":
                empty_content_cases += 1
        
        if empty_content_cases == 0:
            print(f"      âœ… æ‰€æœ‰æ¡ˆä¾‹éƒ½æœ‰å†…å®¹")
            integrity['cases']['all_have_content'] = True
        else:
            print(f"      âŒ {empty_content_cases}/{total_cases}æ¡ˆä¾‹å†…å®¹ä¸ºç©º")
            integrity['cases']['all_have_content'] = False
        
        integrity['cases']['empty_content_count'] = empty_content_cases
        integrity['cases']['total_count'] = total_cases
        
        return integrity
    
    def _check_vector_quality(self) -> Dict[str, Any]:
        """æ£€æŸ¥å‘é‡è´¨é‡"""
        print(f"\nğŸ§® æ£€æŸ¥å‘é‡è´¨é‡...")
        
        quality = {'articles': {}, 'cases': {}}
        
        # æ£€æŸ¥æ³•æ¡å‘é‡è´¨é‡
        print(f"   ğŸ“Š æ³•æ¡å‘é‡è´¨é‡:")
        articles_vectors = self.vector_data['articles']['vectors']
        
        quality['articles'] = self._analyze_vector_quality(articles_vectors, "æ³•æ¡")
        
        # æ£€æŸ¥æ¡ˆä¾‹å‘é‡è´¨é‡
        print(f"   ğŸ“Š æ¡ˆä¾‹å‘é‡è´¨é‡:")
        cases_vectors = self.vector_data['cases']['vectors']
        
        quality['cases'] = self._analyze_vector_quality(cases_vectors, "æ¡ˆä¾‹")
        
        # æ£€æŸ¥å‘é‡é—´ç›¸ä¼¼åº¦åˆ†å¸ƒ
        print(f"   ğŸ” å‘é‡ç›¸ä¼¼åº¦åˆ†æ:")
        quality['similarity_analysis'] = self._analyze_vector_similarities()
        
        return quality
    
    def _analyze_vector_quality(self, vectors: np.ndarray, data_type: str) -> Dict[str, Any]:
        """åˆ†æå‘é‡è´¨é‡"""
        analysis = {}
        
        # åŸºæœ¬ç»Ÿè®¡
        analysis['shape'] = vectors.shape
        analysis['mean'] = float(np.mean(vectors))
        analysis['std'] = float(np.std(vectors))
        analysis['min'] = float(np.min(vectors))
        analysis['max'] = float(np.max(vectors))
        
        print(f"      ğŸ“ {data_type}å‘é‡å½¢çŠ¶: {vectors.shape}")
        print(f"      ğŸ“Š å‡å€¼: {analysis['mean']:.4f}, æ ‡å‡†å·®: {analysis['std']:.4f}")
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        zero_vectors = np.all(vectors == 0, axis=1).sum()
        analysis['zero_vectors'] = int(zero_vectors)
        
        if zero_vectors > 0:
            print(f"      âš ï¸ å‘ç°{zero_vectors}ä¸ªé›¶å‘é‡")
        else:
            print(f"      âœ… æ— é›¶å‘é‡")
        
        # è®¡ç®—å‘é‡èŒƒæ•°åˆ†å¸ƒ
        norms = np.linalg.norm(vectors, axis=1)
        analysis['norm_stats'] = {
            'mean': float(np.mean(norms)),
            'std': float(np.std(norms)),
            'min': float(np.min(norms)),
            'max': float(np.max(norms))
        }
        
        print(f"      ğŸ“ å‘é‡èŒƒæ•°: å‡å€¼={analysis['norm_stats']['mean']:.4f}, "
              f"æ ‡å‡†å·®={analysis['norm_stats']['std']:.4f}")
        
        # æ£€æŸ¥å‘é‡æ˜¯å¦å½’ä¸€åŒ–
        normalized_check = np.allclose(norms, 1.0, atol=1e-6)
        analysis['is_normalized'] = normalized_check
        
        if normalized_check:
            print(f"      âœ… å‘é‡å·²å½’ä¸€åŒ–")
        else:
            print(f"      âš ï¸ å‘é‡æœªå½’ä¸€åŒ–")
        
        return analysis
    
    def _analyze_vector_similarities(self) -> Dict[str, Any]:
        """åˆ†æå‘é‡ç›¸ä¼¼åº¦"""
        analysis = {}
        
        try:
            # è®¡ç®—æ³•æ¡å‘é‡å†…éƒ¨ç›¸ä¼¼åº¦ï¼ˆéšæœºé‡‡æ ·ä»¥èŠ‚çœè®¡ç®—ï¼‰
            articles_vectors = self.vector_data['articles']['vectors']
            if len(articles_vectors) > 50:
                # éšæœºé‡‡æ ·50ä¸ªå‘é‡è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—
                import random
                indices = random.sample(range(len(articles_vectors)), 50)
                sample_vectors = articles_vectors[indices]
            else:
                sample_vectors = articles_vectors
            
            similarities = cosine_similarity(sample_vectors)
            
            # è·å–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
            upper_triangle = similarities[np.triu_indices_from(similarities, k=1)]
            
            analysis['articles_internal'] = {
                'mean_similarity': float(np.mean(upper_triangle)),
                'std_similarity': float(np.std(upper_triangle)),
                'min_similarity': float(np.min(upper_triangle)),
                'max_similarity': float(np.max(upper_triangle)),
                'sample_size': len(sample_vectors)
            }
            
            print(f"      ğŸ“Š æ³•æ¡å†…éƒ¨ç›¸ä¼¼åº¦ (æ ·æœ¬{len(sample_vectors)}ä¸ª):")
            print(f"         å‡å€¼: {analysis['articles_internal']['mean_similarity']:.4f}")
            print(f"         èŒƒå›´: [{analysis['articles_internal']['min_similarity']:.4f}, "
                  f"{analysis['articles_internal']['max_similarity']:.4f}]")
            
            # ç®€å•çš„æ³•æ¡-æ¡ˆä¾‹äº¤å‰ç›¸ä¼¼åº¦æ£€æŸ¥ï¼ˆå°æ ·æœ¬ï¼‰
            if len(articles_vectors) > 0 and len(self.vector_data['cases']['vectors']) > 0:
                # å–å‰10ä¸ªæ³•æ¡å’Œå‰10ä¸ªæ¡ˆä¾‹
                articles_sample = articles_vectors[:min(10, len(articles_vectors))]
                cases_sample = self.vector_data['cases']['vectors'][:min(10, len(self.vector_data['cases']['vectors']))]
                
                cross_similarities = cosine_similarity(articles_sample, cases_sample)
                
                analysis['cross_similarity'] = {
                    'mean_similarity': float(np.mean(cross_similarities)),
                    'std_similarity': float(np.std(cross_similarities)),
                    'min_similarity': float(np.min(cross_similarities)),
                    'max_similarity': float(np.max(cross_similarities))
                }
                
                print(f"      ğŸ“Š æ³•æ¡-æ¡ˆä¾‹äº¤å‰ç›¸ä¼¼åº¦ (10x10æ ·æœ¬):")
                print(f"         å‡å€¼: {analysis['cross_similarity']['mean_similarity']:.4f}")
                print(f"         èŒƒå›´: [{analysis['cross_similarity']['min_similarity']:.4f}, "
                      f"{analysis['cross_similarity']['max_similarity']:.4f}]")
            
        except Exception as e:
            print(f"      âŒ ç›¸ä¼¼åº¦åˆ†æå¤±è´¥: {e}")
            analysis['error'] = str(e)
        
        return analysis
    
    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        print(f"\n" + "=" * 60)
        print("ğŸ“‹ æ•°æ®å®Œæ•´æ€§éªŒè¯æ€»ç»“")
        print("=" * 60)
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'unknown',
            'critical_issues': [],
            'warnings': [],
            'recommendations': []
        }
        
        # è¯„ä¼°å…³é”®é—®é¢˜
        file_consistency = self.results.get('file_consistency', {})
        id_mapping = self.results.get('id_mapping', {})
        content_integrity = self.results.get('content_integrity', {})
        
        critical_issues = []
        warnings = []
        
        # æ£€æŸ¥æ–‡ä»¶ä¸€è‡´æ€§é—®é¢˜
        if not file_consistency.get('articles', {}).get('count_match', True):
            critical_issues.append("æ³•æ¡å‘é‡å’ŒåŸå§‹æ•°æ®æ•°é‡ä¸ä¸€è‡´")
        
        if not file_consistency.get('cases', {}).get('count_match', True):
            critical_issues.append("æ¡ˆä¾‹å‘é‡å’ŒåŸå§‹æ•°æ®æ•°é‡ä¸ä¸€è‡´")
        
        # æ£€æŸ¥IDæ˜ å°„é—®é¢˜
        if not id_mapping.get('articles', {}).get('perfect_match', True):
            critical_issues.append("æ³•æ¡IDæ˜ å°„ä¸å®Œæ•´")
        
        if not id_mapping.get('cases', {}).get('perfect_match', True):
            critical_issues.append("æ¡ˆä¾‹IDæ˜ å°„ä¸å®Œæ•´")
        
        # æ£€æŸ¥å†…å®¹å®Œæ•´æ€§é—®é¢˜
        if not content_integrity.get('articles', {}).get('all_have_content', True):
            warnings.append(f"éƒ¨åˆ†æ³•æ¡å†…å®¹ä¸ºç©º ({content_integrity.get('articles', {}).get('empty_content_count', 0)}ä¸ª)")
        
        if not content_integrity.get('cases', {}).get('all_have_content', True):
            warnings.append(f"éƒ¨åˆ†æ¡ˆä¾‹å†…å®¹ä¸ºç©º ({content_integrity.get('cases', {}).get('empty_content_count', 0)}ä¸ª)")
        
        # æ£€æŸ¥å‘é‡è´¨é‡é—®é¢˜
        vector_quality = self.results.get('vector_quality', {})
        
        if vector_quality.get('articles', {}).get('zero_vectors', 0) > 0:
            warnings.append(f"æ³•æ¡æ•°æ®åŒ…å«é›¶å‘é‡ ({vector_quality['articles']['zero_vectors']}ä¸ª)")
        
        if vector_quality.get('cases', {}).get('zero_vectors', 0) > 0:
            warnings.append(f"æ¡ˆä¾‹æ•°æ®åŒ…å«é›¶å‘é‡ ({vector_quality['cases']['zero_vectors']}ä¸ª)")
        
        summary['critical_issues'] = critical_issues
        summary['warnings'] = warnings
        
        # ç¡®å®šæ€»ä½“çŠ¶æ€
        if critical_issues:
            summary['overall_status'] = 'critical'
            print("ğŸš¨ å‘ç°å…³é”®é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤")
        elif warnings:
            summary['overall_status'] = 'warning'
            print("âš ï¸ å‘ç°ä¸€äº›è­¦å‘Šï¼Œå»ºè®®ä¼˜åŒ–")
        else:
            summary['overall_status'] = 'healthy'
            print("âœ… æ•°æ®å®Œæ•´æ€§è‰¯å¥½")
        
        # æ˜¾ç¤ºé—®é¢˜è¯¦æƒ…
        if critical_issues:
            print(f"\nğŸš¨ å…³é”®é—®é¢˜:")
            for issue in critical_issues:
                print(f"   - {issue}")
        
        if warnings:
            print(f"\nâš ï¸ è­¦å‘Š:")
            for warning in warnings:
                print(f"   - {warning}")
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        if critical_issues:
            if any("æ•°é‡ä¸ä¸€è‡´" in issue for issue in critical_issues):
                recommendations.append("é‡æ–°ç”Ÿæˆå‘é‡æ•°æ®ï¼Œç¡®ä¿ä¸åŸå§‹æ•°æ®æ•°é‡åŒ¹é…")
            
            if any("IDæ˜ å°„" in issue for issue in critical_issues):
                recommendations.append("æ£€æŸ¥IDç”Ÿæˆé€»è¾‘ï¼Œç¡®ä¿å‘é‡å’ŒåŸå§‹æ•°æ®IDå¯¹åº”")
        
        if warnings:
            if any("å†…å®¹ä¸ºç©º" in warning for warning in warnings):
                recommendations.append("æ£€æŸ¥æ•°æ®é¢„å¤„ç†è¿‡ç¨‹ï¼Œç¡®ä¿æ‰€æœ‰æ–‡æ¡£éƒ½æœ‰æœ‰æ•ˆå†…å®¹")
            
            if any("é›¶å‘é‡" in warning for warning in warnings):
                recommendations.append("æ£€æŸ¥å‘é‡ç”Ÿæˆè¿‡ç¨‹ï¼Œé‡æ–°å¤„ç†å¯¼è‡´é›¶å‘é‡çš„æ–‡æ¡£")
        
        if not critical_issues and not warnings:
            recommendations.append("æ•°æ®çŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨")
        
        summary['recommendations'] = recommendations
        
        if recommendations:
            print(f"\nğŸ’¡ å»ºè®®:")
            for rec in recommendations:
                print(f"   - {rec}")
        
        return summary


def main():
    """ä¸»å‡½æ•°"""
    try:
        checker = DataIntegrityChecker()
        results = checker.check_all_integrity()
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        output_file = project_root / "tools" / "data_validation" / f"integrity_check_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            import json
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
        except Exception as e:
            print(f"\nâš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        
        return results
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æ€§æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}")
        logger.exception("Data integrity checking failed")
        return {'error': str(e)}


if __name__ == "__main__":
    main()