#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœç´¢æœåŠ¡ (Search Service)
å°è£…æœç´¢ä¸šåŠ¡é€»è¾‘ï¼Œè§£è€¦APIå±‚å’ŒåŸºç¡€è®¾æ–½å±‚
"""

import time
import json
from typing import List, Optional, Dict, Any, Tuple
import logging
import uuid

from ..domains.entities import LegalDocument, Article, Case
from ..domains.value_objects import SearchQuery, SearchResult, SearchContext
from ..domains.repositories import ILegalDocumentRepository
from ..types.debug_interfaces import (
    ModuleTrace, SearchPipelineTrace, ModuleStatus,
    create_module_trace, create_search_pipeline_trace
)
from ..config.settings import settings
from ..infrastructure.llm.llm_client import LLMClient

logger = logging.getLogger(__name__)


class SearchService:
    """æœç´¢ä¸šåŠ¡æœåŠ¡ç±» - ä¸šåŠ¡é€»è¾‘æ ¸å¿ƒ"""

    def __init__(self, repository: ILegalDocumentRepository, llm_client: Optional[LLMClient] = None, debug_mode: bool = False):
        """
        åˆå§‹åŒ–æœç´¢æœåŠ¡

        Args:
            repository: æ³•å¾‹æ–‡æ¡£å­˜å‚¨åº“å®ç°
            llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹
            debug_mode: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        self.repository = repository
        self.llm_client = llm_client
        self.debug_mode = debug_mode
        self.websocket_manager = None  # æ–°å¢ï¼šWebSocketç®¡ç†å™¨
        
    def set_websocket_manager(self, websocket_manager):
        """è®¾ç½®WebSocketç®¡ç†å™¨ç”¨äºå®æ—¶æ¨é€"""
        self.websocket_manager = websocket_manager
        logger.info(f"ğŸ”— WebSocketç®¡ç†å™¨å·²è®¾ç½®ï¼Œè°ƒè¯•æ¨¡å¼: {self.debug_mode}, è¿æ¥æ•°: {len(websocket_manager.active_connections) if websocket_manager else 0}")

    async def search_documents_intelligent_debug(self, query_text: str, debug: bool = True) -> Dict[str, Any]:
        """
        AIé©±åŠ¨çš„æ™ºèƒ½æœç´¢ - å®Œæ•´è°ƒè¯•æ¨¡å¼ (å¢å¼ºç‰ˆï¼šå¸¦å®æ—¶é˜¶æ®µæ¨é€)
        å®ç°5é˜¶æ®µæœç´¢ç®¡é“ï¼šLLMåˆ†ç±»â†’ä¿¡æ¯æå–â†’æ™ºèƒ½è·¯ç”±â†’å¤šè·¯æœç´¢â†’èåˆ

        Args:
            query_text: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼

        Returns:
            å®Œæ•´çš„traceæ•°æ®å’Œæœç´¢ç»“æœ
        """
        request_id = str(uuid.uuid4())
        pipeline_trace = create_search_pipeline_trace(request_id, query_text)
        pipeline_start = time.time()

        async def _broadcast_stage_completion(stage_number: int, stage_name: str, trace_data: ModuleTrace):
            logger.info(f"ğŸš€ å°è¯•å¹¿æ’­é˜¶æ®µ{stage_number}å®Œæˆï¼Œè°ƒè¯•æ¨¡å¼: {self.debug_mode}, WebSocketç®¡ç†å™¨: {self.websocket_manager is not None}")

            if self.debug_mode and self.websocket_manager:
                try:
                    # è½¬æ¢trace_dataä¸ºå¯JSONåºåˆ—åŒ–çš„å­—å…¸
                    trace_dict = trace_data.dict()

                    # å¤„ç†datetimeå¯¹è±¡
                    if 'timestamp' in trace_dict and trace_dict['timestamp']:
                        trace_dict['timestamp'] = trace_dict['timestamp'].isoformat()

                    message = {
                        "type": "stage_completed",
                        "stage_number": stage_number,
                        "stage_name": stage_name,
                        "status": trace_data.status,  # ç§»é™¤.valueï¼Œå› ä¸ºModuleStatuså·²ç»æ˜¯å­—ç¬¦ä¸²
                        "processing_time_ms": trace_data.processing_time_ms,
                        "trace_data": trace_dict,
                        "timestamp": int(time.time() * 1000)
                    }

                    logger.info(f"ğŸ“¡ å‡†å¤‡å‘é€WebSocketæ¶ˆæ¯: {message['type']}, é˜¶æ®µ: {stage_number}, è¿æ¥æ•°: {len(self.websocket_manager.active_connections)}")

                    await self.websocket_manager.broadcast(message)
                    logger.info(f"âœ… [å®æ—¶æ¨é€] é˜¶æ®µ{stage_number} ({stage_name}) å®Œæˆ: {trace_data.processing_time_ms:.0f}ms")
                except Exception as e:
                    logger.error(f"âŒ WebSocketæ¨é€é˜¶æ®µ{stage_number}å®ŒæˆçŠ¶æ€å¤±è´¥: {e}")
            else:
                logger.warning(f"âš ï¸ è·³è¿‡WebSocketæ¨é€ - è°ƒè¯•æ¨¡å¼: {self.debug_mode}, WebSocketç®¡ç†å™¨: {self.websocket_manager is not None}")

        try:
            # é˜¶æ®µ1: LLMé—®é¢˜åˆ†ç±»
            classification_trace, is_criminal = await self._llm_classification_stage(query_text, pipeline_trace)
            await _broadcast_stage_completion(1, "classification", classification_trace)

            if is_criminal:
                # é˜¶æ®µ2: ç»“æ„åŒ–ä¿¡æ¯æå–
                extraction_trace, structured_data = await self._structured_extraction_stage(query_text, pipeline_trace)
                await _broadcast_stage_completion(2, "extraction", extraction_trace)

                # é˜¶æ®µ3: æ™ºèƒ½è·¯ç”±å†³ç­–
                routing_trace, selected_paths = await self._intelligent_routing_stage(structured_data, pipeline_trace)
                await _broadcast_stage_completion(3, "routing", routing_trace)

                # é˜¶æ®µ4: å¤šè·¯æœç´¢æ‰§è¡Œ
                search_results = await self._multi_path_search_stage(query_text, structured_data, selected_paths, pipeline_trace)
                # Stage 4 broadcasts are handled inside _multi_path_search_stage for each module

                # é˜¶æ®µ5: ç»“æœèåˆä¸å›ç­”ç”Ÿæˆ
                fusion_trace, final_result = await self._fusion_and_generation_stage(search_results, query_text, pipeline_trace)
                await _broadcast_stage_completion(5, "fusion", fusion_trace)

                processing_mode = "criminal_law"
            else:
                # éåˆ‘æ³•é—®é¢˜ï¼Œä½¿ç”¨é€šç”¨AIæ¨¡å¼
                final_result = await self._general_ai_mode(query_text, pipeline_trace)
                processing_mode = "general_ai"

            pipeline_trace.total_duration_ms = (time.time() - pipeline_start) * 1000
            pipeline_trace.processing_mode = processing_mode
            pipeline_trace.summary.update({
                "stages_completed": 5 if is_criminal else 1,
                "total_modules_used": len(pipeline_trace.searches) + (1 if is_criminal else 0),
                "successful_modules": len([s for s in pipeline_trace.searches.values() if s.status == ModuleStatus.SUCCESS]),
                "highest_confidence": max([s.confidence_score or 0 for s in pipeline_trace.searches.values()] + [0])
            })

            if debug or self.debug_mode:
                return {
                    'success': True,
                    'request_id': request_id,
                    'trace': pipeline_trace.dict(),
                    'final_result': final_result,
                    'debug_enabled': True
                }
            else:
                return final_result

        except Exception as e:
            logger.error(f"Intelligent search failed: {str(e)}")
            error_trace = create_module_trace(
                module_name="intelligent_search_pipeline",
                stage="error_handling",
                input_data={"query": query_text},
                success=False,
                error_message=str(e)
            )
            pipeline_trace.searches["error"] = error_trace
            return self._create_error_response(f"æ™ºèƒ½æœç´¢å¤±è´¥: {str(e)}")

    async def _llm_classification_stage(self, query_text: str, pipeline_trace: SearchPipelineTrace) -> Tuple[ModuleTrace, bool]:
        """é˜¶æ®µ1: LLMé—®é¢˜åˆ†ç±» - ä»…ä½¿ç”¨LLMè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        stage_start = time.time()

        try:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æç¤ºè¯æ¨¡æ¿ï¼ŒåŠ¨æ€åŠ è½½çŸ¥è¯†å›¾è°±ç½ªå
            try:
                crimes_list = settings.load_knowledge_graph_crimes()
                classification_prompt = settings.CLASSIFICATION_PROMPT_TEMPLATE.format(
                    query=query_text,
                    crimes_list=crimes_list
                )
            except Exception as e:
                logger.error(f"åŠ è½½çŸ¥è¯†å›¾è°±ç½ªåå¤±è´¥: {e}")
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæç¤ºè¯
                classification_prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ä¸­å›½æ³•å¾‹AIåˆ†ç±»å™¨ï¼Œåˆ¤æ–­æŸ¥è¯¢æ˜¯å¦å±äºåˆ‘äº‹æ³•å¾‹èŒƒç•´ã€‚

æŸ¥è¯¢ï¼š"{query_text}"

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{{
    "is_criminal_law": true/false,
    "confidence": 0.0-1.0,
    "reasoning": "åˆ†ææ¨ç†è¿‡ç¨‹",
    "identified_crimes": [],
    "query2doc_generated": "",
    "hyde_answer": "",
    "bm25_keywords": []
}}"""

            # å¿…é¡»æœ‰LLMå®¢æˆ·ç«¯æ‰èƒ½è¿›è¡Œåˆ†ç±»
            if not self.llm_client:
                error_trace = create_module_trace(
                    module_name="llm_classifier",
                    stage="é—®é¢˜åˆ†ç±»",
                    input_data={"query": query_text},
                    success=False,
                    processing_time_ms=(time.time() - stage_start) * 1000,
                    error_message="LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œé—®é¢˜åˆ†ç±»"
                )
                pipeline_trace.classification = error_trace
                return error_trace, False

            # è°ƒç”¨LLMåˆ†ç±»
            llm_start = time.time()
            response_text = await self.llm_client.generate_text(
                prompt=classification_prompt,
                max_tokens=settings.CLASSIFICATION_MAX_TOKENS,
                temperature=settings.CLASSIFICATION_TEMPERATURE
            )
            llm_time = (time.time() - llm_start) * 1000

            if not response_text:
                error_trace = create_module_trace(
                    module_name="llm_classifier",
                    stage="é—®é¢˜åˆ†ç±»",
                    input_data={"query": query_text},
                    success=False,
                    processing_time_ms=(time.time() - stage_start) * 1000,
                    error_message="LLMè¿”å›ç©ºå“åº”"
                )
                pipeline_trace.classification = error_trace
                return error_trace, False

            # è§£æLLMè¿”å›çš„JSON
            try:
                classification_data = json.loads(response_text.strip())
                is_criminal = classification_data.get("is_criminal_law", False)
                confidence = classification_data.get("confidence", 0.5)

                # æ„å»ºè¾“å‡ºæ•°æ® - é›†æˆæ‰€æœ‰AIå¢å¼ºæŠ€æœ¯åˆ°é˜¶æ®µ1
                classification_output = {
                    "is_criminal_law": is_criminal,
                    "confidence": confidence,
                    "reasoning": classification_data.get("reasoning", "LLMè¯­ä¹‰åˆ†æç»“æœ"),
                    "classification_method": "llm_semantic_analysis",
                    "identified_crimes": classification_data.get("identified_crimes", []),
                    # Query2docå’ŒHyDEå¢å¼ºå†…å®¹
                    "query2doc_generated": classification_data.get("query2doc_generated", ""),
                    "hyde_answer": classification_data.get("hyde_answer", ""),
                    # BM25å…³é”®è¯ç”Ÿæˆ
                    "bm25_keywords": classification_data.get("bm25_keywords", [])
                }

            except json.JSONDecodeError as e:
                error_trace = create_module_trace(
                    module_name="llm_classifier",
                    stage="é—®é¢˜åˆ†ç±»",
                    input_data={"query": query_text},
                    success=False,
                    processing_time_ms=(time.time() - stage_start) * 1000,
                    error_message=f"LLMè¿”å›JSONæ ¼å¼é”™è¯¯: {e}"
                )
                pipeline_trace.classification = error_trace
                return error_trace, False

            # åˆ›å»ºæˆåŠŸçš„åˆ†ç±»trace - å¢å¼ºç‰ˆï¼ŒåŒ…å«å®Œæ•´è¾“å…¥è¾“å‡º
            classification_trace = create_module_trace(
                module_name="llm_classifier",
                stage="é—®é¢˜åˆ†ç±»",
                input_data={
                    "query": query_text,
                    "prompt_template": "CLASSIFICATION_PROMPT_TEMPLATE",
                    "full_prompt_preview": classification_prompt[:500] + "..." if len(classification_prompt) > 500 else classification_prompt
                },
                output_data={
                    **classification_output,
                    "raw_llm_response": response_text,  # æ·»åŠ åŸå§‹LLMå“åº”
                    "parsed_successfully": True
                },
                processing_time_ms=(time.time() - stage_start) * 1000,
                confidence_score=confidence,
                debug_info={
                    "method_used": "llm_semantic_analysis",
                    "llm_model": self.llm_client.get_current_service(),
                    "llm_response_time_ms": llm_time,
                    "prompt_tokens": len(classification_prompt.split()),
                    "response_tokens": len(response_text.split()),
                    "config_max_tokens": settings.CLASSIFICATION_MAX_TOKENS,
                    "config_temperature": settings.CLASSIFICATION_TEMPERATURE,
                    "config_timeout": settings.CLASSIFICATION_TIMEOUT,
                    "crimes_detected": len(classification_data.get("identified_crimes", [])),
                    "query2doc_generated": bool(classification_data.get("query2doc_generated")),
                    "hyde_generated": bool(classification_data.get("hyde_answer")),
                    "bm25_keywords_count": len(classification_data.get("bm25_keywords", []))
                }
            )

            pipeline_trace.classification = classification_trace
            return classification_trace, is_criminal

        except Exception as e:
            logger.error(f"LLMåˆ†ç±»é˜¶æ®µå‡ºé”™: {e}", exc_info=True)
            error_trace = create_module_trace(
                module_name="llm_classifier",
                stage="é—®é¢˜åˆ†ç±»",
                input_data={"query": query_text},
                success=False,
                processing_time_ms=(time.time() - stage_start) * 1000,
                error_message=str(e)
            )
            pipeline_trace.classification = error_trace
            return error_trace, False

    async def _structured_extraction_stage(self, query_text: str, pipeline_trace: SearchPipelineTrace) -> Tuple[ModuleTrace, Dict]:
        """é˜¶æ®µ2: ç»“æ„åŒ–ä¿¡æ¯æå–ï¼ˆä»é˜¶æ®µ1 LLM JSONä¸­æå–å¹¶å±•ç¤ºï¼‰"""
        stage_start = time.time()

        try:
            classification_output = {}
            if pipeline_trace and pipeline_trace.classification and isinstance(pipeline_trace.classification.output_data, dict):
                classification_output = pipeline_trace.classification.output_data or {}

            identified_crimes = classification_output.get("identified_crimes") or []
            q2d = (classification_output.get("query2doc_generated") or "").strip()
            hyde = (classification_output.get("hyde_answer") or "").strip()
            bm25_keywords = classification_output.get("bm25_keywords") or []

            if q2d:
                query2doc_enhanced = f"{query_text} [SEP] {q2d}"
            else:
                query2doc_enhanced = query_text

            crimes_list_version = "unknown"
            try:
                crimes_text = settings.load_knowledge_graph_crimes()
                lines = crimes_text.split("\n") if crimes_text else []
                head = lines[0] if lines else ""
                tail = lines[-1] if lines else ""
                crimes_list_version = f"lines={len(lines)}; head={head[:8]}; tail={tail[:8]}"
            except Exception:
                crimes_list_version = "unavailable"

            structured_data = {
                "identified_crimes": identified_crimes,
                "query2doc_enhanced": query2doc_enhanced,
                "hyde_hypothetical": hyde,
                "bm25_keywords": bm25_keywords,
                "crimes_list_version": crimes_list_version,
            }

            extraction_trace = create_module_trace(
                module_name="structured_extractor",
                stage="ç»“æ„åŒ–æå–",
                input_data={"query": query_text},
                output_data=structured_data,
                processing_time_ms=(time.time() - stage_start) * 1000,
                confidence_score=classification_output.get("confidence"),
                debug_info={
                    "source": "stage1_llm_json",
                    "has_query2doc": bool(q2d),
                    "has_hyde": bool(hyde),
                    "bm25_keywords_count": len(bm25_keywords),
                }
            )

            pipeline_trace.extraction = extraction_trace
            return extraction_trace, structured_data

        except Exception as e:
            error_trace = create_module_trace(
                module_name="structured_extractor",
                stage="ç»“æ„åŒ–æå–",
                input_data={"query": query_text},
                success=False,
                processing_time_ms=(time.time() - stage_start) * 1000,
                error_message=str(e)
            )
            pipeline_trace.extraction = error_trace
            return error_trace, {}
    async def _extract_crimes_from_knowledge_graph(self, query_text: str) -> List[Dict]:
        """ä»çŸ¥è¯†å›¾è°±ä¸­æå–ç½ªåä¿¡æ¯"""
        try:
            # å°è¯•ä»çŸ¥è¯†å›¾è°±è·å–ç½ªå
            if hasattr(self.repository, 'data_loader') and self.repository.data_loader:
                knowledge_graph = self.repository.data_loader.get_knowledge_graph()
                if knowledge_graph:
                    return knowledge_graph.extract_crimes_from_query(query_text)
            
            # é™çº§åˆ°æœ¬åœ°ç½ªåè¯å…¸åŒ¹é…
            return self._local_crime_matching(query_text)
            
        except Exception as e:
            logger.warning(f"çŸ¥è¯†å›¾è°±ç½ªåæå–å¤±è´¥: {e}")
            return self._local_crime_matching(query_text)
    
    def _local_crime_matching(self, query_text: str) -> List[Dict]:
        """æœ¬åœ°ç½ªååŒ¹é… - åŸºäºå…³é”®è¯"""
        # æ‰©å±•çš„ç½ªåè¯å…¸
        crimes_map = {
            "æ•…æ„ä¼¤å®³": {"crime_name": "æ•…æ„ä¼¤å®³ç½ª", "confidence": 0.9, "article_number": 234},
            "æ•…æ„æ€äºº": {"crime_name": "æ•…æ„æ€äººç½ª", "confidence": 0.95, "article_number": 232},
            "ç›—çªƒ": {"crime_name": "ç›—çªƒç½ª", "confidence": 0.85, "article_number": 264},
            "æŠ¢åŠ«": {"crime_name": "æŠ¢åŠ«ç½ª", "confidence": 0.9, "article_number": 263},
            "è¯ˆéª—": {"crime_name": "è¯ˆéª—ç½ª", "confidence": 0.88, "article_number": 266},
            "å¼ºå¥¸": {"crime_name": "å¼ºå¥¸ç½ª", "confidence": 0.95, "article_number": 236},
            "ç»‘æ¶": {"crime_name": "ç»‘æ¶ç½ª", "confidence": 0.92, "article_number": 239},
            "æ•²è¯ˆ": {"crime_name": "æ•²è¯ˆå‹’ç´¢ç½ª", "confidence": 0.9, "article_number": 274},
            "è´ªæ±¡": {"crime_name": "è´ªæ±¡ç½ª", "confidence": 0.93, "article_number": 382},
            "å—è´¿": {"crime_name": "å—è´¿ç½ª", "confidence": 0.93, "article_number": 385},
            "äº¤é€šè‚‡äº‹": {"crime_name": "äº¤é€šè‚‡äº‹ç½ª", "confidence": 0.85, "article_number": 133},
            "å±é™©é©¾é©¶": {"crime_name": "å±é™©é©¾é©¶ç½ª", "confidence": 0.88, "article_number": 133},
            "é†‰é©¾": {"crime_name": "å±é™©é©¾é©¶ç½ª", "confidence": 0.9, "article_number": 133},
            "æ¯’å“": {"crime_name": "éæ³•æŒæœ‰æ¯’å“ç½ª", "confidence": 0.8, "article_number": 348},
            "è´©æ¯’": {"crime_name": "è´©å–æ¯’å“ç½ª", "confidence": 0.95, "article_number": 347},
        }
        
        detected_crimes = []
        query_lower = query_text.lower()
        
        for keyword, crime_info in crimes_map.items():
            if keyword in query_lower:
                detected_crimes.append(crime_info)
        
        return detected_crimes

    async def _fallback_structured_extraction(self, query_text: str) -> Dict:
        """é™çº§çš„ç»“æ„åŒ–æå–æ–¹æ³•"""
        try:
            # 1. æœ¬åœ°ç½ªåè¯†åˆ«
            identified_crimes = self._local_crime_matching(query_text)
            
            # 2. ç”ŸæˆQuery2docï¼ˆç®€åŒ–ç‰ˆï¼‰
            query2doc_enhanced = self._generate_simple_query2doc(query_text, identified_crimes)
            
            # 3. ç”ŸæˆHyDEï¼ˆç®€åŒ–ç‰ˆï¼‰
            hyde_hypothetical = self._generate_simple_hyde(query_text, identified_crimes)
            
            # 4. æå–BM25å…³é”®è¯
            bm25_keywords = self._extract_bm25_keywords(query_text, identified_crimes)
            
            return {
                "identified_crimes": identified_crimes,
                "query2doc_enhanced": query2doc_enhanced,
                "hyde_hypothetical": hyde_hypothetical,
                "bm25_keywords": bm25_keywords
            }
            
        except Exception as e:
            logger.error(f"é™çº§ç»“æ„åŒ–æå–å¤±è´¥: {e}")
            return {
                "identified_crimes": [],
                "query2doc_enhanced": "",
                "hyde_hypothetical": "",
                "bm25_keywords": []
            }

    def _generate_simple_query2doc(self, query_text: str, crimes: List[Dict]) -> str:
        """ç”Ÿæˆç®€å•çš„Query2docå¢å¼ºæŸ¥è¯¢"""
        if crimes:
            crime_names = [crime.get("crime_name", "") for crime in crimes]
            articles = [f"ç¬¬{crime.get('article_number', '')}æ¡" for crime in crimes if crime.get('article_number')]
            
            enhanced = f"æ ¹æ®ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ç›¸å…³è§„å®šï¼Œæ¶‰åŠ{', '.join(crime_names)}ç­‰ç½ªåã€‚"
            if articles:
                enhanced += f"ç›¸å…³æ³•æ¡åŒ…æ‹¬{', '.join(articles)}ã€‚"
            enhanced += f"é’ˆå¯¹'{query_text}'çš„æ³•å¾‹åˆ†æå’Œå¤„ç†ä¾æ®ã€‚"
            return enhanced
        else:
            return f"å…³äº'{query_text}'çš„æ³•å¾‹æ¡æ–‡å’Œç›¸å…³æ¡ˆä¾‹åˆ†æã€‚"

    def _generate_simple_hyde(self, query_text: str, crimes: List[Dict]) -> str:
        """ç”Ÿæˆç®€å•çš„HyDEå‡è®¾ç­”æ¡ˆ"""
        if crimes:
            crime_name = crimes[0].get("crime_name", "ç›¸å…³çŠ¯ç½ª")
            article_num = crimes[0].get("article_number", "")
            
            hyde = f"æ ¹æ®æ³•å¾‹è§„å®šï¼Œ{query_text}å¯èƒ½æ¶‰åŠ{crime_name}ã€‚"
            if article_num:
                hyde += f"ä¾æ®ã€Šåˆ‘æ³•ã€‹ç¬¬{article_num}æ¡ï¼Œ"
            hyde += f"æ­¤ç±»è¡Œä¸ºçš„æ³•å¾‹åæœåŒ…æ‹¬åˆ‘äº‹è´£ä»»å’Œç›¸åº”çš„åˆ‘ç½šæªæ–½ã€‚å…·ä½“é‡åˆ‘éœ€è¦æ ¹æ®æ¡ˆä»¶çš„å…·ä½“æƒ…å†µã€çŠ¯ç½ªæƒ…èŠ‚ã€ç¤¾ä¼šå±å®³ç¨‹åº¦ç­‰å› ç´ ç»¼åˆåˆ¤å®šã€‚"
            return hyde
        else:
            return f"å…³äº{query_text}çš„æ³•å¾‹é—®é¢˜ï¼Œéœ€è¦æ ¹æ®å…·ä½“æƒ…å†µåˆ†æé€‚ç”¨çš„æ³•å¾‹æ¡æ–‡å’Œç›¸å…³åˆ¤ä¾‹ï¼Œä»¥ç¡®å®šç›¸åº”çš„æ³•å¾‹è´£ä»»å’Œå¤„ç†æ–¹å¼ã€‚"

    def _extract_bm25_keywords(self, query_text: str, crimes: List[Dict]) -> List[Dict]:
        """æå–å¸¦æƒé‡çš„BM25æœç´¢å…³é”®è¯"""
        keywords = []
        
        # é¿å…çš„é€šç”¨è¯æ±‡
        common_words = {
            "åˆ‘äº‹", "çŠ¯ç½ª", "å—å®³è€…", "è¢«å‘Šäºº", "æ³•å¾‹", "å¤„ç½š", "è´£ä»»", "è¡Œä¸º", 
            "æ„æˆ", "ä¾æ®", "è§„å®š", "æ¡æ–‡", "æ¡ˆä»¶", "äº‹å®", "æƒ…èŠ‚", "ç¤¾ä¼š", 
            "å±å®³", "åæœ", "æ€ä¹ˆ", "å¦‚ä½•", "ä»€ä¹ˆ", "å“ªä¸ª", "åˆ¤åˆ‘", "å®šç½ª"
        }
        
        # 1. æ·»åŠ æ³•æ¡ç¼–å·ï¼ˆæœ€é«˜æƒé‡ï¼‰
        for crime in crimes:
            article_num = crime.get("article_number")
            if article_num:
                keywords.append({"keyword": f"ç¬¬{article_num}æ¡", "weight": 0.95})
                keywords.append({"keyword": str(article_num), "weight": 0.9})
        
        # 2. æ·»åŠ å…·ä½“ç½ªåï¼ˆé«˜æƒé‡ï¼‰
        for crime in crimes:
            crime_name = crime.get("crime_name", "")
            if crime_name and crime_name not in common_words:
                keywords.append({"keyword": crime_name, "weight": 0.85})
                # æ·»åŠ ç½ªåçš„æ ¸å¿ƒè¯
                core_word = crime_name.replace("ç½ª", "")
                if core_word != crime_name and core_word not in common_words:
                    keywords.append({"keyword": core_word, "weight": 0.8})
        
        # 3. æ·»åŠ æŸ¥è¯¢ä¸­çš„ä¸“ä¸šæœ¯è¯­ï¼ˆä¸­ç­‰æƒé‡ï¼‰
        import re
        # æ”¹è¿›çš„ä¸­æ–‡åˆ†è¯ - æå–æœ‰æ„ä¹‰çš„è¯æ±‡
        query_words = []
        
        # ä¸“ä¸šæœ¯è¯­è¯å…¸ï¼ˆåŒ…å«å®Œæ•´è¯æ±‡ï¼‰
        professional_terms = {
            "è½»ä¼¤": 0.75, "é‡ä¼¤": 0.8, "æ­»äº¡": 0.85,
            "æ•°é¢è¾ƒå¤§": 0.7, "æ•°é¢å·¨å¤§": 0.75, "æ•°é¢ç‰¹åˆ«å·¨å¤§": 0.8,
            "æƒ…èŠ‚ä¸¥é‡": 0.7, "æƒ…èŠ‚ç‰¹åˆ«ä¸¥é‡": 0.75,
            "æ•…æ„": 0.65, "è¿‡å¤±": 0.65, "æš´åŠ›": 0.7,
            "ç§˜å¯†çªƒå–": 0.7, "å…¬ç„¶æŠ¢å¤º": 0.7, "å…¥æˆ·": 0.65,
            "æŒæœ‰": 0.6, "è´©å–": 0.7, "è¿è¾“": 0.65,
            "ä¼¤å®³": 0.6, "æ€äºº": 0.75, "ç›—çªƒ": 0.7, "æŠ¢åŠ«": 0.75,
            "é†‰é…’": 0.7, "é©¾é©¶": 0.65, "æœºåŠ¨è½¦": 0.6,
            "è¯ˆéª—": 0.75, "æ•²è¯ˆ": 0.7, "å‹’ç´¢": 0.7,
            "ç»‘æ¶": 0.8, "å¼ºå¥¸": 0.8, "æŠ¢å¤º": 0.7
        }
        
        # å…ˆæ£€æŸ¥ä¸“ä¸šæœ¯è¯­
        for term, weight in professional_terms.items():
            if term in query_text and term not in common_words:
                keywords.append({"keyword": term, "weight": weight})
        
        # å†æå–å…¶ä»–ä¸­æ–‡è¯æ±‡
        other_words = re.findall(r'[\u4e00-\u9fff]{2,}', query_text)
        for word in other_words:
            if (word not in common_words and 
                not any(kw["keyword"] == word for kw in keywords) and
                not any(term in word for term in professional_terms.keys())):
                # æ ¹æ®è¯æ±‡é•¿åº¦å’Œç‰¹å¾ç»™äºˆä¸åŒæƒé‡
                if len(word) >= 4:
                    weight = 0.55  # è¾ƒé•¿è¯æ±‡æƒé‡ç¨é«˜
                else:
                    weight = 0.45
                keywords.append({"keyword": word, "weight": weight})
        
        # æŒ‰æƒé‡æ’åºå¹¶å»é‡
        keywords.sort(key=lambda x: x["weight"], reverse=True)
        
        # å»é‡ï¼ˆä¿ç•™æƒé‡æœ€é«˜çš„ï¼‰
        seen = set()
        unique_keywords = []
        for kw in keywords:
            if kw["keyword"] not in seen:
                seen.add(kw["keyword"])
                unique_keywords.append(kw)
        
        # è¿”å›å‰8ä¸ªæœ€é‡è¦çš„å…³é”®è¯
        return unique_keywords[:8]

    def _validate_and_clean_structured_data(self, structured_data: Dict, query_text: str) -> Dict:
        """éªŒè¯å’Œæ¸…ç†ç»“æ„åŒ–æ•°æ®"""
        # ç¡®ä¿æ‰€æœ‰å­—æ®µå­˜åœ¨
        cleaned_data = {
            "identified_crimes": structured_data.get("identified_crimes", []),
            "query2doc_enhanced": structured_data.get("query2doc_enhanced", ""),
            "hyde_hypothetical": structured_data.get("hyde_hypothetical", ""),
            "bm25_keywords": structured_data.get("bm25_keywords", [])
        }
        
        # éªŒè¯ç½ªåæ ¼å¼
        valid_crimes = []
        for crime in cleaned_data["identified_crimes"]:
            if isinstance(crime, dict) and crime.get("crime_name"):
                valid_crimes.append(crime)
        cleaned_data["identified_crimes"] = valid_crimes
        
        # ç¡®ä¿Query2docå’ŒHyDEä¸ä¸ºç©º
        if not cleaned_data["query2doc_enhanced"]:
            cleaned_data["query2doc_enhanced"] = self._generate_simple_query2doc(query_text, valid_crimes)
        
        if not cleaned_data["hyde_hypothetical"]:
            cleaned_data["hyde_hypothetical"] = self._generate_simple_hyde(query_text, valid_crimes)
        
        # ç¡®ä¿BM25å…³é”®è¯ä¸ä¸ºç©º
        if not cleaned_data["bm25_keywords"]:
            cleaned_data["bm25_keywords"] = self._extract_bm25_keywords(query_text, valid_crimes)
        
        return cleaned_data

    async def _intelligent_routing_stage(self, structured_data: Dict, pipeline_trace: SearchPipelineTrace) -> Tuple[ModuleTrace, List[str]]:
        """é˜¶æ®µ3: æ™ºèƒ½è·¯ç”±å†³ç­–"""
        stage_start = time.time()

        try:
            selected_paths = []
            routing_reasoning = []

            # æ ¹æ®æå–çš„ä¿¡æ¯å†³å®šæœç´¢è·¯å¾„
            if structured_data.get("identified_crimes"):
                selected_paths.append("knowledge_graph")
                routing_reasoning.append("æ£€æµ‹åˆ°æ˜ç¡®ç½ªåï¼Œå¯ç”¨çŸ¥è¯†å›¾è°±æœç´¢")

            if structured_data.get("query2doc_enhanced"):
                selected_paths.append("query2doc")
                routing_reasoning.append("Query2docå¢å¼ºå¯ç”¨ï¼Œå¯ç”¨Query2docæœç´¢")

            if structured_data.get("hyde_hypothetical"):
                selected_paths.append("hyde")
                routing_reasoning.append("HyDEå‡è®¾ç­”æ¡ˆå¯ç”¨ï¼Œå¯ç”¨HyDEæœç´¢")

            if structured_data.get("bm25_keywords"):
                selected_paths.append("bm25_hybrid")
                routing_reasoning.append("å…³é”®è¯æå–æˆåŠŸï¼Œå¯ç”¨BM25æ··åˆæœç´¢")

            # å§‹ç»ˆæ·»åŠ åŸºç¡€è¯­ä¹‰æœç´¢ä½œä¸ºæ ¸å¿ƒè·¯å¾„ä¹‹ä¸€
            selected_paths.append("basic_semantic")
            routing_reasoning.append("æ·»åŠ åŸºç¡€è¯­ä¹‰æœç´¢ï¼ˆçº¯å‘é‡ï¼Œç›´æ¥ç”¨æˆ·è¾“å…¥ï¼‰")

            routing_output = {
                "selected_paths": selected_paths,
                "routing_reasoning": "; ".join(routing_reasoning),
                "parallel_execution": True,
                "path_priorities": {path: 1.0 / (i + 1) for i, path in enumerate(selected_paths)},
                "execution_plan": {
                    "mode": "concurrent_async",
                    "timeout_ms": 30000,  # 30ç§’è¶…æ—¶
                    "minimum_engines": max(1, len(selected_paths) // 2),  # è‡³å°‘ä¸€åŠçš„å¼•æ“æˆåŠŸ
                    "fallback_strategy": "graceful_degradation"
                },
                "routing_confidence": 0.9 if len(selected_paths) >= 3 else 0.7,
                "path_details": [
                    {
                        "engine": path,
                        "priority": 1.0 / (i + 1),
                        "expected_contribution": 0.8 if i == 0 else 0.6 - (i * 0.1),
                        "reason": reasoning
                    } for i, (path, reasoning) in enumerate(zip(selected_paths, routing_reasoning))
                ]
            }

            routing_trace = create_module_trace(
                module_name="intelligent_router",
                stage="è·¯ç”±å†³ç­–",
                input_data=structured_data,
                output_data=routing_output,
                processing_time_ms=(time.time() - stage_start) * 1000,
                confidence_score=0.9,
                debug_info={
                    "decision_factors": list(structured_data.keys()),
                    "routing_algorithm": "rule_based_v1"
                }
            )

            pipeline_trace.routing = routing_trace
            return routing_trace, selected_paths

        except Exception as e:
            error_trace = create_module_trace(
                module_name="intelligent_router",
                stage="è·¯ç”±å†³ç­–",
                input_data=structured_data,
                success=False,
                processing_time_ms=(time.time() - stage_start) * 1000,
                error_message=str(e)
            )
            pipeline_trace.routing = error_trace
            return error_trace, ["basic_semantic"]  # é”™è¯¯æ—¶é»˜è®¤åŸºç¡€æœç´¢

    async def _multi_path_search_stage(self, query_text: str, structured_data: Dict,
                                 selected_paths: List[str], pipeline_trace: SearchPipelineTrace) -> Dict:
        """é˜¶æ®µ4: å¤šè·¯æœç´¢æ‰§è¡Œï¼ˆä¼˜åŒ–ç‰ˆï¼šå¹¶è¡Œæ‰§è¡Œ + å®æ—¶æ¨é€ï¼‰"""
        import asyncio
        all_results: Dict[str, Any] = {}
        
        logger.info(f"å¼€å§‹å¹¶è¡Œæ‰§è¡Œ{len(selected_paths)}ä¸ªæœç´¢æ¨¡å—")
        
        async def run_path(path: str):
            start = time.time()
            try:
                logger.info(f"æ¨¡å—{path}å¼€å§‹æ‰§è¡Œ...")
                
                # æ ¹æ®è·¯å¾„æ‰§è¡Œç›¸åº”æœç´¢
                if path == "knowledge_graph":
                    res = await self._execute_knowledge_graph_search(query_text, structured_data)
                elif path == "query2doc":
                    res = await self._execute_query2doc_search(query_text, structured_data)
                elif path == "hyde":
                    res = await self._execute_hyde_search(query_text, structured_data)
                elif path == "bm25_hybrid":
                    res = await self._execute_bm25_hybrid_search(query_text, structured_data)
                elif path == "basic_semantic":
                    res = await self._execute_basic_semantic_search(query_text)
                else:
                    res = {"success": False, "error": f"æœªçŸ¥è·¯å¾„: {path}"}
                
                processing_time = (time.time() - start) * 1000
                status = ModuleStatus.SUCCESS if (isinstance(res, dict) and res.get('success')) else ModuleStatus.ERROR
                
                # åˆ›å»ºæ¨¡å—trace
                trace = create_module_trace(
                    module_name=f"{path}_search",
                    stage=f"{path}æœç´¢",
                    input_data={"query": query_text, "path": path},
                    output_data=res,
                    processing_time_ms=processing_time,
                    success=(status == ModuleStatus.SUCCESS),
                    confidence_score=res.get('confidence_score') if isinstance(res, dict) else None,
                    debug_info={"concurrent": True, "execution_order": len(pipeline_trace.searches) + 1}
                )
                
                # ç«‹å³æ·»åŠ åˆ°pipeline_trace
                pipeline_trace.searches[path] = trace
                
                # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šæ¯ä¸ªæ¨¡å—å®Œæˆåç«‹å³æ¨é€ç»™å‰ç«¯
                if self.debug_mode and self.websocket_manager:
                    try:
                        # è½¬æ¢traceä¸ºå¯JSONåºåˆ—åŒ–çš„å­—å…¸
                        trace_dict = trace.dict()

                        # å¤„ç†datetimeå¯¹è±¡
                        if 'timestamp' in trace_dict and trace_dict['timestamp']:
                            trace_dict['timestamp'] = trace_dict['timestamp'].isoformat()

                        # å®æ—¶WebSocketå¹¿æ’­
                        await self.websocket_manager.broadcast({
                            "type": "module_completed",
                            "module_name": path,
                            "status": status,  # ç§»é™¤.valueï¼Œå› ä¸ºModuleStatuså·²ç»æ˜¯å­—ç¬¦ä¸²
                            "processing_time_ms": processing_time,
                            "results_count": len(res.get('articles', [])) + len(res.get('cases', [])) if isinstance(res, dict) else 0,
                            "trace_data": trace_dict,  # å…³é”®æ–°å¢ï¼šåŒ…å«å®Œæ•´çš„æ¨¡å—trace
                            "timestamp": int(time.time() * 1000)
                        })
                        logger.info(f"[å®æ—¶æ¨é€] æ¨¡å—{path}å®Œæˆ: {processing_time:.0f}ms, çŠ¶æ€: {status}")
                    except Exception as ws_error:
                        logger.warning(f"WebSocketæ¨é€å¤±è´¥: {ws_error}")
                elif self.debug_mode:
                    # fallbackæ—¥å¿—
                    logger.info(f"[è°ƒè¯•æ¨¡å¼] æ¨¡å—{path}å®Œæˆ: {processing_time:.0f}ms, çŠ¶æ€: {status}")
                
                logger.info(f"æ¨¡å—{path}æ‰§è¡Œå®Œæˆ: {processing_time:.0f}ms, ç»“æœ: {len(res.get('articles', [])) if isinstance(res, dict) else 0}æ³•æ¡ + {len(res.get('cases', [])) if isinstance(res, dict) else 0}æ¡ˆä¾‹")
                return path, res
                
            except Exception as e:
                processing_time = (time.time() - start) * 1000
                logger.error(f"æ¨¡å—{path}æ‰§è¡Œå¤±è´¥: {e}")
                
                error_trace = create_module_trace(
                    module_name=f"{path}_search",
                    stage=f"{path}æœç´¢",
                    input_data={"query": query_text, "path": path},
                    success=False,
                    processing_time_ms=processing_time,
                    error_message=str(e)
                )
                pipeline_trace.searches[path] = error_trace
                return path, {"success": False, "error": str(e)}

        # å»é‡å¹¶ä¿æŒåŸæœ‰é¡ºåº
        seen = set()
        ordered_paths = [p for p in selected_paths if not (p in seen or seen.add(p))]
        
        # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨asyncio.as_completedå®ç°çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œå’Œå®æ—¶å¤„ç†
        tasks = [run_path(p) for p in ordered_paths]
        completed_count = 0
        
        for coro in asyncio.as_completed(tasks):
            try:
                path, res = await coro
                all_results[path] = res
                completed_count += 1
                
                # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                logger.info(f"âœ… æ¨¡å—{path}å®Œæˆ ({completed_count}/{len(ordered_paths)}) - å‰©ä½™{len(ordered_paths) - completed_count}ä¸ªæ¨¡å—")
                
                # è¿™é‡Œå¯ä»¥ç«‹å³å¤„ç†å•ä¸ªæ¨¡å—çš„ç»“æœï¼Œè€Œä¸ç­‰å¾…æ‰€æœ‰æ¨¡å—å®Œæˆ
                # ä¾‹å¦‚ï¼šç«‹å³å‘é€ç»™å‰ç«¯æ˜¾ç¤ºè¿™ä¸ªæ¨¡å—çš„ç»“æœ
                
            except Exception as e:
                logger.error(f"å¤„ç†å¹¶è¡Œæœç´¢ç»“æœå¤±è´¥: {e}")
                completed_count += 1
        
        logger.info(f"ğŸ‰ æ‰€æœ‰{len(ordered_paths)}ä¸ªæœç´¢æ¨¡å—å·²å®Œæˆ")
        return all_results

    async def _execute_knowledge_graph_search(self, query_text: str, structured_data: Dict) -> Dict:
        """æ‰§è¡ŒçŸ¥è¯†å›¾è°±æœç´¢"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰çŸ¥è¯†å›¾è°±æ”¯æŒ
            if not hasattr(self.repository, 'data_loader') or not self.repository.data_loader:
                logger.warning("æ•°æ®åŠ è½½å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æœç´¢")
                return {"success": False, "error": "æ•°æ®åŠ è½½å™¨æœªåˆå§‹åŒ–"}

            knowledge_graph = self.repository.data_loader.get_knowledge_graph()
            if not knowledge_graph:
                logger.warning("çŸ¥è¯†å›¾è°±æœªåˆå§‹åŒ–ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±æœç´¢")
                return {"success": False, "error": "çŸ¥è¯†å›¾è°±æœªåˆå§‹åŒ–"}

            # ä»structured_dataè·å–è¯†åˆ«çš„ç½ªå
            crimes = structured_data.get("identified_crimes", [])
            crime_names = [crime.get("crime_name", "") for crime in crimes if crime.get("crime_name")]
            
            if crime_names:
                # ä½¿ç”¨è¯†åˆ«çš„ç½ªåè¿›è¡ŒçŸ¥è¯†å›¾è°±æœç´¢
                crime_based_query = " ".join(crime_names)
                logger.info(f"æ‰§è¡ŒçŸ¥è¯†å›¾è°±æœç´¢: åŸæŸ¥è¯¢'{query_text}' -> ç½ªåæŸ¥è¯¢'{crime_based_query}'")
                search_query = crime_based_query
            else:
                logger.info(f"æ‰§è¡ŒçŸ¥è¯†å›¾è°±æœç´¢: ä½¿ç”¨åŸæŸ¥è¯¢'{query_text}'")
                search_query = query_text

            # ä½¿ç”¨çŸ¥è¯†å›¾è°±æ‰©å±•æŸ¥è¯¢
            expansion_results = knowledge_graph.expand_query_with_relations(search_query)

            # ğŸ“Š è½¬æ¢çŸ¥è¯†å›¾è°±ç»“æœä¸ºæ ‡å‡†æœç´¢æ ¼å¼
            # å¦‚æœçŸ¥è¯†å›¾è°±è¿”å›äº†ç›¸å…³æ³•æ¡å’Œæ¡ˆä¾‹ï¼Œéœ€è¦è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            articles = []
            cases = []
            
            # å¤„ç†ç›¸å…³æ³•æ¡
            related_articles = expansion_results.get('related_articles', [])
            for article_info in related_articles[:5]:  # æœ€å¤š5æ¡
                article_number = article_info.get('article_number')
                article_content = ''
                
                # ğŸ”§ ä¿®å¤ï¼šä»DataLoaderè·å–å®Œæ•´æ³•æ¡å†…å®¹
                if article_number and hasattr(self.repository, 'data_loader') and self.repository.data_loader:
                    try:
                        # é€šè¿‡article_idæˆ–article_numberè·å–å®Œæ•´å†…å®¹
                        article_id = f"article_{article_number}"
                        article_content = self.repository.data_loader.get_article_content(article_id)
                        if not article_content:
                            # å¦‚æœé€šè¿‡IDè·å–å¤±è´¥ï¼Œç›´æ¥é€šè¿‡ç¼–å·è·å–
                            article_content = self.repository.data_loader._get_article_by_number(article_number)
                        logger.debug(f"çŸ¥è¯†å›¾è°±æ³•æ¡{article_number}å†…å®¹é•¿åº¦: {len(article_content) if article_content else 0}")
                    except Exception as e:
                        logger.warning(f"è·å–æ³•æ¡{article_number}å®Œæ•´å†…å®¹å¤±è´¥: {e}")
                        article_content = article_info.get('content', 'æš‚æ— å†…å®¹')
                else:
                    article_content = article_info.get('content', 'æš‚æ— å†…å®¹')
                
                # ğŸ¯ å¢å¼ºï¼šæ·»åŠ çŸ¥è¯†å›¾è°±æƒé‡å’Œæ¥æºä¿¡æ¯
                kg_total_score = article_info.get('kg_total_score', article_info.get('confidence', 0.8))
                kg_sources = article_info.get('kg_sources', ['knowledge_graph'])
                
                articles.append({
                    'id': f"article_{article_number or 'unknown'}",
                    'type': 'article',
                    'title': f'ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³• ç¬¬{article_number}æ¡' if article_number else 'æœªçŸ¥æ³•æ¡',  # ğŸ”§ ç»Ÿä¸€æ ‡é¢˜æ ¼å¼ï¼Œä¸å…¶ä»–æ¨¡å—ä¿æŒä¸€è‡´
                    'content': article_content,
                    'article_number': article_number,  # ğŸ”§ æ·»åŠ article_numberå­—æ®µ
                    'chapter': '',  # ğŸ”§ æ·»åŠ ç« èŠ‚å­—æ®µä¿æŒæ ¼å¼ä¸€è‡´
                    'similarity': article_info.get('confidence', 0.8),  # çŸ¥è¯†å›¾è°±ç½®ä¿¡åº¦
                    'score': article_info.get('confidence', 0.8),  # ğŸ”§ æ·»åŠ scoreå­—æ®µï¼Œå‰ç«¯ä½¿ç”¨æ­¤å­—æ®µæ˜¾ç¤ºç›¸å…³åº¦
                    'kg_total_score': kg_total_score,  # ğŸ¯ çŸ¥è¯†å›¾è°±æƒé‡
                    'kg_sources': kg_sources,  # ğŸ¯ çŸ¥è¯†å›¾è°±æ¥æº
                    'source': 'æ•°æ®é›†',  # ğŸ”§ ç»Ÿä¸€æ¥æºæ˜¾ç¤º
                    'search_method': 'knowledge_graph'  # æœç´¢æ–¹æ³•ä½œä¸ºæŠ€æœ¯ä¿¡æ¯
                })
            
            # å¤„ç†ç›¸å…³æ¡ˆä¾‹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            related_cases = expansion_results.get('related_cases', [])
            for case_info in related_cases[:5]:  # æœ€å¤š5ä¸ªæ¡ˆä¾‹
                case_id = case_info.get('case_id', 'unknown')
                case_content = ''
                
                # ğŸ”§ ä¿®å¤ï¼šä»DataLoaderè·å–å®Œæ•´æ¡ˆä¾‹å†…å®¹
                if case_id != 'unknown' and hasattr(self.repository, 'data_loader') and self.repository.data_loader:
                    try:
                        # é€šè¿‡case_idè·å–å®Œæ•´å†…å®¹
                        case_content = self.repository.data_loader.get_case_content(case_id)
                        if not case_content:
                            case_content = case_info.get('content', 'æš‚æ— å†…å®¹')
                        logger.debug(f"çŸ¥è¯†å›¾è°±æ¡ˆä¾‹{case_id}å†…å®¹é•¿åº¦: {len(case_content) if case_content else 0}")
                    except Exception as e:
                        logger.warning(f"è·å–æ¡ˆä¾‹{case_id}å®Œæ•´å†…å®¹å¤±è´¥: {e}")
                        case_content = case_info.get('content', 'æš‚æ— å†…å®¹')
                else:
                    case_content = case_info.get('content', 'æš‚æ— å†…å®¹')
                
                # ğŸ¯ å¢å¼ºï¼šæ·»åŠ çŸ¥è¯†å›¾è°±æƒé‡å’Œæ¥æºä¿¡æ¯
                kg_total_score = case_info.get('kg_total_score', case_info.get('confidence', 0.8))
                kg_sources = case_info.get('kg_sources', ['knowledge_graph'])
                
                cases.append({
                    'id': f"case_{case_id}",
                    'type': 'case', 
                    'title': case_info.get('case_title', f'æ¡ˆä¾‹{case_id}'),
                    'content': case_content,
                    'fact': case_content,  # ğŸ”§ æ·»åŠ factå­—æ®µï¼Œå‰ç«¯éœ€è¦æ­¤å­—æ®µæ˜¾ç¤ºæ¡ˆæƒ…æè¿°
                    'case_id': case_id,  # ğŸ”§ æ·»åŠ case_idå­—æ®µ
                    'accusations': case_info.get('accusations', []),  # ğŸ”§ æ·»åŠ ç½ªåå­—æ®µ
                    'similarity': case_info.get('confidence', 0.8),
                    'score': case_info.get('confidence', 0.8),  # ğŸ”§ æ·»åŠ scoreå­—æ®µï¼Œå‰ç«¯ä½¿ç”¨æ­¤å­—æ®µæ˜¾ç¤ºç›¸å…³åº¦
                    'kg_total_score': kg_total_score,  # ğŸ¯ çŸ¥è¯†å›¾è°±æƒé‡
                    'kg_sources': kg_sources,  # ğŸ¯ çŸ¥è¯†å›¾è°±æ¥æº
                    'source': 'æ•°æ®é›†',  # ğŸ”§ ç»Ÿä¸€æ¥æºæ˜¾ç¤º
                    'search_method': 'knowledge_graph'  # æœç´¢æ–¹æ³•ä½œä¸ºæŠ€æœ¯ä¿¡æ¯
                })

            # æ„å»ºæ ‡å‡†æ ¼å¼çš„è¿”å›ç»“æœ
            result = {
                'success': True,
                'articles': articles,
                'cases': cases,
                'total': len(articles) + len(cases),
                'search_meta': {
                    'method': 'knowledge_graph',
                    'search_type': 'entity_expansion',
                    'detected_crimes': len(expansion_results.get('detected_entities', {}).get('crimes', [])),
                    'expanded_keywords': expansion_results.get('expanded_keywords', []),
                    'query': query_text
                },
                'knowledge_expansion': expansion_results  # ä¿ç•™åŸå§‹çŸ¥è¯†å›¾è°±æ•°æ®
            }

            logger.info(f"çŸ¥è¯†å›¾è°±æœç´¢å®Œæˆ: æ£€æµ‹åˆ°{len(expansion_results.get('detected_entities', {}).get('crimes', []))}ä¸ªç½ªå, {len(expansion_results.get('related_articles', []))}ä¸ªç›¸å…³æ³•æ¡")
            return result

        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æœç´¢å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    async def _execute_query2doc_search(self, query_text: str, structured_data: Dict) -> Dict:
        """æ‰§è¡ŒQuery2docå¢å¼ºæœç´¢"""
        try:
            # ä»structured_dataè·å–Query2docå¢å¼ºå†…å®¹
            query2doc_enhanced = structured_data.get("query2doc_enhanced", "")
            
            if not query2doc_enhanced:
                logger.warning("æ²¡æœ‰Query2docå¢å¼ºå†…å®¹ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                enhanced_query = query_text
            else:
                enhanced_query = query2doc_enhanced
                logger.info(f"æ‰§è¡ŒQuery2docæœç´¢: åŸæŸ¥è¯¢'{query_text}' -> å¢å¼ºæŸ¥è¯¢'{enhanced_query[:50]}...'")

            # ä½¿ç”¨å¢å¼ºæŸ¥è¯¢è¿›è¡Œå‘é‡æœç´¢
            result = await self.search_documents_hybrid(enhanced_query, 5, 5)
            
            # æ·»åŠ æœç´¢å…ƒä¿¡æ¯
            if result.get("success"):
                result["search_meta"] = {
                    "method": "query2doc",
                    "used_enhancement": bool(query2doc_enhanced),
                    "enhanced_query": enhanced_query[:100] + "..." if len(enhanced_query) > 100 else enhanced_query
                }
                
                # ğŸ”§ ç»Ÿä¸€æ¥æºæ˜¾ç¤º
                for article in result.get('articles', []):
                    article['source'] = 'æ•°æ®é›†'
                    article['search_method'] = 'query2doc'
                for case in result.get('cases', []):
                    case['source'] = 'æ•°æ®é›†' 
                    case['search_method'] = 'query2doc'
            
            return result

        except Exception as e:
            logger.error(f"Query2docæœç´¢å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "articles": [], "cases": []}

    async def _execute_hyde_search(self, query_text: str, structured_data: Dict) -> Dict:
        """æ‰§è¡ŒHyDEå¢å¼ºæœç´¢"""
        try:
            # ä»structured_dataè·å–HyDEå‡è®¾ç­”æ¡ˆ
            hyde_hypothetical = structured_data.get("hyde_hypothetical", "")
            
            if not hyde_hypothetical:
                logger.warning("æ²¡æœ‰HyDEå‡è®¾ç­”æ¡ˆï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                enhanced_query = query_text
            else:
                enhanced_query = hyde_hypothetical
                logger.info(f"æ‰§è¡ŒHyDEæœç´¢: åŸæŸ¥è¯¢'{query_text}' -> å‡è®¾ç­”æ¡ˆ'{enhanced_query[:50]}...'")

            # ä½¿ç”¨å‡è®¾ç­”æ¡ˆè¿›è¡Œå‘é‡æœç´¢
            result = await self.search_documents_hybrid(enhanced_query, 5, 5)
            
            # æ·»åŠ æœç´¢å…ƒä¿¡æ¯
            if result.get("success"):
                result["search_meta"] = {
                    "method": "hyde",
                    "used_enhancement": bool(hyde_hypothetical),
                    "hypothetical_answer": enhanced_query[:100] + "..." if len(enhanced_query) > 100 else enhanced_query
                }
                
                # ğŸ”§ ç»Ÿä¸€æ¥æºæ˜¾ç¤º
                for article in result.get('articles', []):
                    article['source'] = 'æ•°æ®é›†'
                    article['search_method'] = 'hyde'
                for case in result.get('cases', []):
                    case['source'] = 'æ•°æ®é›†'
                    case['search_method'] = 'hyde'
            
            return result

        except Exception as e:
            logger.error(f"HyDEæœç´¢å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "articles": [], "cases": []}

    async def _execute_bm25_hybrid_search(self, query_text: str, structured_data: Dict) -> Dict:
        """æ‰§è¡ŒBM25æ··åˆæœç´¢ - ä½¿ç”¨æå–çš„å…³é”®è¯"""
        try:
            # ä»structured_dataè·å–BM25å…³é”®è¯
            bm25_keywords = structured_data.get("bm25_keywords", [])
            
            if bm25_keywords:
                # æ„å»ºå…³é”®è¯æŸ¥è¯¢
                if isinstance(bm25_keywords[0], dict):
                    # æ–°æ ¼å¼ï¼šå¸¦æƒé‡çš„å…³é”®è¯å¯¹è±¡
                    keyword_query = " ".join([kw.get("keyword", "") for kw in bm25_keywords[:5]])
                else:
                    # æ—§æ ¼å¼ï¼šç®€å•å­—ç¬¦ä¸²åˆ—è¡¨
                    keyword_query = " ".join(bm25_keywords[:5])
                
                logger.info(f"æ‰§è¡ŒBM25æ··åˆæœç´¢: åŸæŸ¥è¯¢'{query_text}' -> å…³é”®è¯æŸ¥è¯¢'{keyword_query}'")
                enhanced_query = f"{query_text} {keyword_query}"
            else:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„BM25å…³é”®è¯ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢")
                enhanced_query = query_text

            # ä½¿ç”¨å¢å¼ºæŸ¥è¯¢è¿›è¡Œæ··åˆæœç´¢
            result = await self.search_documents_hybrid(enhanced_query, 5, 5)
            
            # æ·»åŠ æœç´¢å…ƒä¿¡æ¯
            if result.get("success"):
                result["search_meta"] = {
                    "method": "bm25_hybrid",
                    "used_keywords": bool(bm25_keywords),
                    "keywords_count": len(bm25_keywords),
                    "enhanced_query": enhanced_query[:100] + "..." if len(enhanced_query) > 100 else enhanced_query
                }
                
                # ğŸ”§ ç»Ÿä¸€æ¥æºæ˜¾ç¤º
                for article in result.get('articles', []):
                    article['source'] = 'æ•°æ®é›†'
                    article['search_method'] = 'bm25_hybrid'
                for case in result.get('cases', []):
                    case['source'] = 'æ•°æ®é›†'
                    case['search_method'] = 'bm25_hybrid'
            
            return result

        except Exception as e:
            logger.error(f"BM25æ··åˆæœç´¢å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "articles": [], "cases": []}

    async def _execute_llm_enhanced_search(self, query_text: str, structured_data: Dict) -> Dict:
        """æ‰§è¡ŒLLMå¢å¼ºæœç´¢ - åŒæ—¶ä½¿ç”¨Query2docå’ŒHyDE"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šè·¯å¬å›å¼•æ“
            if not hasattr(self.repository, 'multi_retrieval_engine') or not self.repository.multi_retrieval_engine:
                logger.warning("å¤šè·¯å¬å›å¼•æ“æœªåˆå§‹åŒ–ï¼Œé™çº§åˆ°æ··åˆæœç´¢")
                return await self.search_documents_hybrid(query_text, 5, 5)

            logger.info(f"æ‰§è¡ŒLLMå¢å¼ºæœç´¢ï¼ˆä¸‰è·¯å¬å›ï¼‰: '{query_text}'")
            
            # ä½¿ç”¨ä¸‰è·¯å¬å›å¼•æ“è¿›è¡Œæœç´¢
            raw_results = await self.repository.multi_retrieval_engine.three_way_retrieval(
                query_text, top_k=10
            )
            
            if not raw_results:
                return {"success": False, "error": "LLMå¢å¼ºæœç´¢æ— ç»“æœ", "articles": [], "cases": []}
            
            # åˆ†ç¦»æ³•æ¡å’Œæ¡ˆä¾‹
            articles = [r for r in raw_results if r.get('type') == 'article'][:5]
            cases = [r for r in raw_results if r.get('type') == 'case'][:5]
            
            # æ·»åŠ æœç´¢å…ƒä¿¡æ¯
            result = {
                "success": True,
                "articles": articles,
                "cases": cases,
                "total": len(articles) + len(cases),
                "search_meta": {
                    "method": "llm_enhanced",
                    "search_type": "three_way_fusion",
                    "used_query2doc": bool(structured_data.get("query2doc_enhanced")),
                    "used_hyde": bool(structured_data.get("hyde_hypothetical")),
                    "query": query_text
                }
            }
            
            logger.info(f"LLMå¢å¼ºæœç´¢å®Œæˆ: {len(articles)}æ¡æ³•æ¡, {len(cases)}ä¸ªæ¡ˆä¾‹")
            return result

        except Exception as e:
            logger.error(f"LLMå¢å¼ºæœç´¢å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "articles": [], "cases": []}

    async def _execute_basic_semantic_search(self, query_text: str) -> Dict:
        """æ‰§è¡ŒåŸºç¡€è¯­ä¹‰æœç´¢ - çº¯å‘é‡æœç´¢ï¼Œä¸ä½¿ç”¨BM25"""
        try:
            logger.info(f"æ‰§è¡ŒåŸºç¡€è¯­ä¹‰æœç´¢ï¼ˆçº¯å‘é‡ï¼‰: '{query_text}'")
            
            # ğŸ”§ æ£€æŸ¥æœç´¢å¼•æ“çŠ¶æ€
            if not hasattr(self.repository, 'search_engine') or not self.repository.search_engine:
                error_msg = "æœç´¢å¼•æ“æœªåˆå§‹åŒ–"
                logger.error(error_msg)
                return {"success": False, "error": error_msg, "articles": [], "cases": []}
            
            # æ£€æŸ¥æœç´¢å¼•æ“å¥åº·çŠ¶æ€
            try:
                health_check = self.repository.search_engine.health_check()
                logger.debug(f"æœç´¢å¼•æ“å¥åº·æ£€æŸ¥: {health_check}")
                if not health_check.get('healthy', False):
                    error_msg = f"æœç´¢å¼•æ“ä¸å¥åº·: {health_check.get('error', 'æœªçŸ¥é”™è¯¯')}"
                    logger.warning(error_msg)
                    # ä¸ç«‹å³è¿”å›é”™è¯¯ï¼Œå°è¯•ç»§ç»­æœç´¢
            except Exception as health_error:
                logger.warning(f"å¥åº·æ£€æŸ¥å¤±è´¥: {health_error}")
            
            # ç›´æ¥ä½¿ç”¨å‘é‡æœç´¢ï¼Œä¸æ··åˆBM25
            search_result = self.repository.search_engine.search(
                query_text, 
                top_k=10, 
                include_content=True
            )
            
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ­£ç¡®çš„å­—æ®µåï¼ŒSearchCoordinatorè¿”å›çš„æ˜¯successå­—æ®µï¼Œä¸æ˜¯statuså­—æ®µ
            if not search_result or not search_result.get('success', False):
                error_msg = search_result.get('error', 'åŸºç¡€è¯­ä¹‰æœç´¢æ— ç»“æœ') if search_result else 'æœç´¢å¼•æ“æœªè¿”å›ç»“æœ'
                logger.warning(f"åŸºç¡€è¯­ä¹‰æœç´¢å¤±è´¥: {error_msg}")
                return {"success": False, "error": error_msg, "articles": [], "cases": []}
            
            # åˆ†ç¦»æ³•æ¡å’Œæ¡ˆä¾‹
            all_results = search_result.get('articles', []) + search_result.get('cases', [])
            articles = [r for r in all_results if r.get('type') == 'article'][:5]
            cases = [r for r in all_results if r.get('type') == 'case'][:5]
            
            # æ·»åŠ æœç´¢å…ƒä¿¡æ¯
            result = {
                "success": True,
                "articles": articles,
                "cases": cases,
                "total": len(articles) + len(cases),
                "search_meta": {
                    "method": "basic_semantic",
                    "search_type": "pure_vector",
                    "no_bm25": True,
                    "query": query_text
                }
            }
            
            # ğŸ”§ ç»Ÿä¸€æ¥æºæ˜¾ç¤º
            for article in articles:
                article['source'] = 'æ•°æ®é›†'
                article['search_method'] = 'basic_semantic'
            for case in cases:
                case['source'] = 'æ•°æ®é›†'
                case['search_method'] = 'basic_semantic'
            
            logger.info(f"åŸºç¡€è¯­ä¹‰æœç´¢å®Œæˆ: {len(articles)}æ¡æ³•æ¡, {len(cases)}ä¸ªæ¡ˆä¾‹")
            return result

        except Exception as e:
            logger.error(f"åŸºç¡€è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return {"success": False, "error": str(e), "articles": [], "cases": []}

    async def _fusion_and_generation_stage(self, search_results: Dict, query_text: str,
                                         pipeline_trace: SearchPipelineTrace) -> Tuple[ModuleTrace, Dict]:
        """é˜¶æ®µ5: ç»“æœèåˆä¸AIå›ç­”ç”Ÿæˆ"""
        stage_start = time.time()

        try:
            # ğŸ† æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„æ³•æ¡å’Œæ¡ˆä¾‹ç»“æœ
            all_articles = []
            all_cases = []
            path_confidence_scores = {}
            
            logger.info(f"å¼€å§‹èåˆ{len(search_results)}ä¸ªæœç´¢æ¨¡å—çš„ç»“æœ")
            
            for path, results in search_results.items():
                if isinstance(results, dict) and results.get('success'):
                    path_articles = results.get('articles', [])
                    path_cases = results.get('cases', [])
                    
                    # æ·»åŠ è·¯å¾„æ ‡è¯†
                    for article in path_articles:
                        article['search_path'] = path
                        article['path_priority'] = len(all_articles) + 1
                        article['source'] = 'æ•°æ®é›†'  # ğŸ”§ ç»Ÿä¸€æ¥æº
                        article['search_method'] = path  # æœç´¢æ–¹æ³•
                        all_articles.append(article)
                    
                    for case in path_cases:
                        case['search_path'] = path
                        case['path_priority'] = len(all_cases) + 1
                        case['source'] = 'æ•°æ®é›†'  # ğŸ”§ ç»Ÿä¸€æ¥æº
                        case['search_method'] = path  # æœç´¢æ–¹æ³•
                        all_cases.append(case)
                    
                    # è®¡ç®—è¯¥è·¯å¾„çš„å¹³å‡ç½®ä¿¡åº¦
                    all_path_results = path_articles + path_cases
                    if all_path_results:
                        path_confidence = sum(r.get('similarity', 0) for r in all_path_results) / len(all_path_results)
                        path_confidence_scores[path] = path_confidence
                        logger.info(f"è·¯å¾„ {path}: {len(path_articles)}æ¡æ³•æ¡, {len(path_cases)}ä¸ªæ¡ˆä¾‹, å¹³å‡ç½®ä¿¡åº¦: {path_confidence:.3f}")
                    else:
                        path_confidence_scores[path] = 0.0
                        logger.warning(f"è·¯å¾„ {path}: æ— æœ‰æ•ˆç»“æœ")
                else:
                    logger.warning(f"è·¯å¾„ {path}: æœç´¢å¤±è´¥æˆ–æ— ç»“æœ")
            
            # ğŸ¯ ä½¿ç”¨å¢å¼ºRRFç®—æ³•èåˆç»“æœ
            final_articles = self._apply_enhanced_rrf_fusion(all_articles, target_count=5)
            final_cases = self._apply_enhanced_rrf_fusion(all_cases, target_count=5)
            
            logger.info(f"RRFèåˆå®Œæˆ: æœ€ç»ˆè·å¾—{len(final_articles)}æ¡æ³•æ¡, {len(final_cases)}ä¸ªæ¡ˆä¾‹")
            
            # ğŸ¤– è°ƒç”¨LLMç”Ÿæˆæ™ºèƒ½å›ç­”
            llm_start = time.time()
            ai_answer, final_confidence = await self._generate_ai_answer(
                query_text, final_articles, final_cases, path_confidence_scores
            )
            llm_time = (time.time() - llm_start) * 1000
            
            # æ„å»ºèåˆè¾“å‡ºæ•°æ®
            fusion_output = {
                "fusion_algorithm": "Enhanced_RRF",
                "input_sources_count": len([k for k, v in search_results.items() if v.get('success')]),
                "total_input_results": len(all_articles) + len(all_cases),
                "final_results_count": len(final_articles) + len(final_cases),
                "final_articles": final_articles,
                "final_cases": final_cases,
                "final_answer": ai_answer,
                "final_confidence": final_confidence,
                "path_confidence_scores": path_confidence_scores,
                "llm_generation_time_ms": llm_time,
                "fusion_details": {
                    "articles_before_fusion": len(all_articles),
                    "articles_after_fusion": len(final_articles),
                    "cases_before_fusion": len(all_cases),
                    "cases_after_fusion": len(final_cases),
                    "rrf_algorithm": "weighted_with_diversity",
                    "diversity_penalty": 0.1
                }
            }

            fusion_trace = create_module_trace(
                module_name="fusion_and_generation_engine",
                stage="ç»“æœèåˆä¸AIç”Ÿæˆ",
                input_data={
                    "query": query_text,
                    "search_modules": list(search_results.keys()),
                    "total_input_articles": len(all_articles),
                    "total_input_cases": len(all_cases)
                },
                output_data=fusion_output,
                processing_time_ms=(time.time() - stage_start) * 1000,
                confidence_score=final_confidence,
                debug_info={
                    "fusion_method": "enhanced_rrf_with_diversity",
                    "llm_generation_enabled": bool(self.llm_client),
                    "llm_generation_time_ms": llm_time,
                    "path_contributions": path_confidence_scores,
                    "answer_length": len(ai_answer) if ai_answer else 0,
                    "sources_used": len([k for k, v in search_results.items() if v.get('success')])
                }
            )

            pipeline_trace.fusion = fusion_trace

            # æ„å»ºæœ€ç»ˆç»“æœ
            final_result = {
                'success': True,
                'articles': final_articles,
                'cases': final_cases,
                'total_articles': len(final_articles),
                'total_cases': len(final_cases),
                'query': query_text,
                'ai_analysis': ai_answer,
                'final_confidence': final_confidence,
                'search_context': {
                    'fusion_applied': True,
                    'sources_used': len([k for k, v in search_results.items() if v.get('success')]),
                    'path_confidence_scores': path_confidence_scores,
                    'fusion_algorithm': 'enhanced_rrf_with_llm',
                    'processing_time_ms': (time.time() - stage_start) * 1000
                }
            }

            return fusion_trace, final_result

        except Exception as e:
            logger.error(f"é˜¶æ®µ5èåˆä¸ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            error_trace = create_module_trace(
                module_name="fusion_and_generation_engine",
                stage="ç»“æœèåˆä¸AIç”Ÿæˆ",
                input_data={"search_results": list(search_results.keys()) if search_results else []},
                success=False,
                processing_time_ms=(time.time() - stage_start) * 1000,
                error_message=str(e)
            )
            pipeline_trace.fusion = error_trace

            # è¿”å›åŸºç¡€ç»“æœ
            return error_trace, self._create_error_response(f"ç»“æœèåˆä¸AIç”Ÿæˆå¤±è´¥: {str(e)}")

    def _apply_enhanced_rrf_fusion(self, results: List[Dict], target_count: int = 5) -> List[Dict]:
        """
        åº”ç”¨å¢å¼ºç‰ˆRRFï¼ˆReciprocal Rank Fusionï¼‰ç®—æ³•èåˆç»“æœ
        åŒ…å«å¤šæ ·æ€§æƒ©ç½šå’Œè·¯å¾„æƒé‡è°ƒæ•´
        
        Args:
            results: ç»“æœåˆ—è¡¨
            target_count: ç›®æ ‡æ•°é‡
            
        Returns:
            èåˆåçš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return []
        
        logger.info(f"å¼€å§‹å¢å¼ºRRFèåˆï¼Œè¾“å…¥{len(results)}ä¸ªç»“æœï¼Œç›®æ ‡{target_count}ä¸ª")
        
        # 1. æŒ‰æœç´¢è·¯å¾„åˆ†ç»„
        path_groups = {}
        for result in results:
            path = result.get('search_path', 'unknown')
            if path not in path_groups:
                path_groups[path] = []
            path_groups[path].append(result)
        
        # 2. ä¼˜åŒ–è·¯å¾„æƒé‡è®¾ç½®ï¼ˆæé«˜åŸºç¡€æƒé‡ï¼‰
        path_weights = {
            'knowledge_graph': 2.5,  # çŸ¥è¯†å›¾è°±æƒé‡æœ€é«˜
            'query2doc': 2.2,        # Query2docå¢å¼ºæƒé‡é«˜
            'hyde': 2.2,             # HyDEå¢å¼ºæƒé‡é«˜
            'bm25_hybrid': 2.0,      # BM25æ··åˆæ ‡å‡†æƒé‡
            'basic_semantic': 1.8    # åŸºç¡€è¯­ä¹‰æƒé‡é€‚ä¸­
        }
        
        # 3. è®¡ç®—æ¯ä¸ªç»“æœçš„å¢å¼ºRRFåˆ†æ•°
        enhanced_scores = {}
        diversity_seen_content = set()  # ç”¨äºå¤šæ ·æ€§æ£€æŸ¥
        
        for path, path_results in path_groups.items():
            path_weight = path_weights.get(path, 2.0)  # é»˜è®¤æƒé‡æå‡åˆ°2.0
            
            # å¯¹æ¯ä¸ªè·¯å¾„å†…çš„ç»“æœæŒ‰ç›¸ä¼¼åº¦æ’åº
            sorted_path_results = sorted(path_results, 
                                       key=lambda x: x.get('similarity', 0), 
                                       reverse=True)
            
            for rank, result in enumerate(sorted_path_results):
                doc_id = result.get('id', f"doc_{len(enhanced_scores)}")
                
                # RRFåŸºç¡€åˆ†æ•°ï¼ˆè°ƒæ•´kå‚æ•°ï¼Œé™ä½åˆ†æ¯ï¼‰
                k = 20  # ä»60é™ä½åˆ°20ï¼Œæé«˜RRFåˆ†æ•°
                base_rrf_score = 1.0 / (k + rank + 1)
                
                # ç›¸ä¼¼åº¦å¢å¼ºï¼ˆç›¸ä¼¼åº¦è¶Šé«˜ï¼Œå¢å¼ºè¶Šå¤šï¼‰
                similarity = result.get('similarity', 0)
                similarity_boost = 1.0 + (similarity * 1.5)  # æœ€é«˜å¯æå‡2.5å€
                
                # è·¯å¾„æƒé‡è°ƒæ•´
                weighted_score = base_rrf_score * path_weight * similarity_boost
                
                # å¤šæ ·æ€§æƒ©ç½šï¼ˆè½»å¾®è°ƒæ•´ï¼‰
                content_snippet = (result.get('content', '') or '')[:100].lower()
                diversity_penalty = 1.0
                if content_snippet in diversity_seen_content:
                    diversity_penalty = 0.85  # è½»å¾®æƒ©ç½šï¼Œä»0.7æå‡åˆ°0.85
                else:
                    diversity_seen_content.add(content_snippet)
                
                # æœ€ç»ˆåˆ†æ•°
                final_score = weighted_score * diversity_penalty
                
                if doc_id in enhanced_scores:
                    # å¦‚æœåŒä¸€æ–‡æ¡£åœ¨å¤šä¸ªè·¯å¾„ä¸­å‡ºç°ï¼Œç´¯åŠ åˆ†æ•°
                    enhanced_scores[doc_id]['score'] += final_score
                    enhanced_scores[doc_id]['path_count'] += 1
                    enhanced_scores[doc_id]['paths'].append(path)
                else:
                    enhanced_scores[doc_id] = {
                        'score': final_score,
                        'data': result,
                        'path_count': 1,
                        'paths': [path]
                    }
                    # æ›´æ–°ç»“æœæ•°æ®
                    result['rrf_score'] = base_rrf_score
                    result['path_weight'] = path_weight
                    result['similarity_boost'] = similarity_boost
                    result['diversity_penalty'] = diversity_penalty
        
        # 4. å¤šè·¯å¾„ä¸€è‡´æ€§å¥–åŠ±ï¼ˆå¢å¼ºç‰ˆï¼‰
        for doc_id, score_data in enhanced_scores.items():
            if score_data['path_count'] > 1:
                # å¤šä¸ªè·¯å¾„éƒ½æ‰¾åˆ°äº†åŒä¸€æ–‡æ¡£ï¼Œç»™äºˆæ›´å¤§çš„ä¸€è‡´æ€§å¥–åŠ±
                consistency_bonus = 0.3 * (score_data['path_count'] - 1)  # ä»0.1æå‡åˆ°0.3
                score_data['score'] *= (1 + consistency_bonus)
                score_data['data']['consistency_bonus'] = consistency_bonus
        
        # 5. æŒ‰æœ€ç»ˆåˆ†æ•°æ’åºå¹¶è¿”å›ç»“æœ
        sorted_results = sorted(enhanced_scores.items(), 
                              key=lambda x: x[1]['score'], 
                              reverse=True)
        
        final_results = []
        for i, (doc_id, score_data) in enumerate(sorted_results[:target_count]):
            result_data = score_data['data']
            # å°†åˆ†æ•°è½¬æ¢ä¸ºç™¾åˆ†æ¯”æ˜¾ç¤ºï¼ˆå¤§å¹…æå‡æ˜¾ç¤ºæ•ˆæœï¼‰
            # ç¡®ä¿åˆ†æ•°è‡³å°‘æœ‰ä¸€å®šçš„åŸºç¡€å€¼ï¼Œé¿å…æ˜¾ç¤º0.0%
            base_score = max(score_data['score'], 0.001)  # è®¾ç½®æœ€å°åŸºç¡€åˆ†æ•°
            display_score = min(base_score * 100, 100)  # ä¹˜ä»¥100æå‡æ˜¾ç¤ºæ•ˆæœ
            result_data['final_fusion_score'] = round(display_score, 1)
            result_data['fusion_rank'] = i + 1
            result_data['path_count'] = score_data['path_count']
            result_data['contributing_paths'] = score_data['paths']
            # ğŸ”§ ç»Ÿä¸€æ¥æºå’Œæ–¹æ³•æ˜¾ç¤º
            result_data['source'] = 'æ•°æ®é›†'
            result_data['search_method'] = result_data.get('search_path', 'unknown')
            final_results.append(result_data)
        
        logger.info(f"å¢å¼ºRRFèåˆå®Œæˆ: ä»{len(results)}ä¸ªç»“æœä¸­é€‰å‡º{len(final_results)}ä¸ªæœ€ä½³ç»“æœ")
        logger.info(f"èåˆåˆ†æ•°èŒƒå›´: {final_results[0]['final_fusion_score']:.1f}% - {final_results[-1]['final_fusion_score']:.1f}%")
        return final_results

    async def _generate_ai_answer(self, query_text: str, final_articles: List[Dict], 
                                 final_cases: List[Dict], path_confidence_scores: Dict) -> Tuple[str, float]:
        """
        ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½æ³•å¾‹å›ç­”
        
        Args:
            query_text: ç”¨æˆ·æŸ¥è¯¢
            final_articles: èåˆåçš„æ³•æ¡
            final_cases: èåˆåçš„æ¡ˆä¾‹
            path_confidence_scores: è·¯å¾„ç½®ä¿¡åº¦åˆ†æ•°
            
        Returns:
            (AIå›ç­”æ–‡æœ¬, æœ€ç»ˆç½®ä¿¡åº¦)
        """
        try:
            if not self.llm_client:
                logger.warning("LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œä½¿ç”¨é»˜è®¤å›ç­”æ¨¡æ¿")
                return self._generate_fallback_answer(query_text, final_articles, final_cases), 0.6
            
            # æ„å»ºé«˜è´¨é‡çš„æç¤ºè¯
            prompt = self._build_legal_analysis_prompt(
                query_text, final_articles, final_cases, path_confidence_scores
            )
            
            logger.info(f"è°ƒç”¨LLMç”Ÿæˆå›ç­”ï¼Œæç¤ºè¯é•¿åº¦: {len(prompt)}å­—ç¬¦")
            
            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            ai_response = await self.llm_client.generate_text(
                prompt=prompt,
                max_tokens=getattr(settings, 'ANSWER_GENERATION_MAX_TOKENS', 1500),
                temperature=getattr(settings, 'ANSWER_GENERATION_TEMPERATURE', 0.3)
            )
            
            if not ai_response or len(ai_response.strip()) < 50:
                logger.warning("LLMè¿”å›å›ç­”è¿‡çŸ­æˆ–ä¸ºç©ºï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                return self._generate_fallback_answer(query_text, final_articles, final_cases), 0.5
            
            # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
            final_confidence = self._calculate_final_confidence(
                path_confidence_scores, len(final_articles), len(final_cases)
            )
            
            logger.info(f"LLMå›ç­”ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(ai_response)}å­—ç¬¦ï¼Œç½®ä¿¡åº¦: {final_confidence:.3f}")
            return ai_response.strip(), final_confidence
            
        except Exception as e:
            logger.error(f"LLMå›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_answer(query_text, final_articles, final_cases), 0.4
    
    def _build_legal_analysis_prompt(self, query_text: str, articles: List[Dict], 
                                   cases: List[Dict], confidence_scores: Dict) -> str:
        """
        æ„å»ºæ³•å¾‹åˆ†ææç¤ºè¯
        
        Args:
            query_text: ç”¨æˆ·æŸ¥è¯¢
            articles: ç›¸å…³æ³•æ¡
            cases: ç›¸å…³æ¡ˆä¾‹
            confidence_scores: ç½®ä¿¡åº¦åˆ†æ•°
            
        Returns:
            å®Œæ•´çš„æç¤ºè¯
        """
        # åŸºç¡€æç¤ºè¯æ¨¡æ¿
        base_prompt = """ä½ æ˜¯ä¸“ä¸šçš„ä¸­å›½åˆ‘æ³•å¾‹å¸ˆå’Œæ³•å®˜ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„å¸æ³•å®è·µç»éªŒã€‚è¯·åŸºäºæä¾›çš„æ³•æ¡å’Œæ¡ˆä¾‹ï¼Œå¯¹ç”¨æˆ·çš„æ³•å¾‹é—®é¢˜è¿›è¡Œä¸“ä¸šã€å‡†ç¡®ã€å…¨é¢çš„åˆ†æå›ç­”ã€‚

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

ã€ç›¸å…³æ³•æ¡ã€‘
{articles_section}

ã€ç›¸å…³æ¡ˆä¾‹ã€‘
{cases_section}

ã€å›ç­”è¦æ±‚ã€‘
1. é¦–å…ˆæ˜ç¡®å›ç­”ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜
2. å¼•ç”¨å…·ä½“çš„æ³•æ¡æ¡æ–‡è¿›è¡Œæ³•ç†åˆ†æ
3. ç»“åˆç›¸å…³æ¡ˆä¾‹è¯´æ˜å¸æ³•å®è·µ
4. åˆ†æå¯èƒ½çš„æ³•å¾‹åæœå’Œå¤„ç½šæ ‡å‡†
5. æä¾›å®ç”¨çš„æ³•å¾‹å»ºè®®
6. è¯­è¨€ä¸“ä¸šä½†é€šä¿—æ˜“æ‡‚ï¼Œæ¡ç†æ¸…æ™°

ã€ä¸“ä¸šåˆ†æã€‘"""
        
        # æ„å»ºæ³•æ¡éƒ¨åˆ†
        articles_section = ""
        if articles:
            articles_section = "æ ¹æ®æ£€ç´¢ç³»ç»Ÿæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³æ³•æ¡ï¼š\n\n"
            for i, article in enumerate(articles[:5], 1):
                title = article.get('title', f'ç¬¬{i}æ¡')
                content = article.get('content', 'æš‚æ— å†…å®¹')[:500]  # é™åˆ¶é•¿åº¦
                similarity = article.get('similarity', 0)
                
                articles_section += f"{i}. {title} (ç›¸å…³åº¦: {similarity:.1%})\n"
                articles_section += f"   {content}\n\n"
        else:
            articles_section = "æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„æ³•æ¡ï¼Œè¯·åŸºäºåˆ‘æ³•ä¸€èˆ¬åŸåˆ™è¿›è¡Œåˆ†æã€‚\n\n"
        
        # æ„å»ºæ¡ˆä¾‹éƒ¨åˆ†
        cases_section = ""
        if cases:
            cases_section = "ç›¸å…³å¸æ³•æ¡ˆä¾‹å‚è€ƒï¼š\n\n"
            for i, case in enumerate(cases[:5], 1):
                title = case.get('title', f'æ¡ˆä¾‹{i}')
                content = case.get('content', 'æš‚æ— å†…å®¹')[:400]  # é™åˆ¶é•¿åº¦
                similarity = case.get('similarity', 0)
                
                cases_section += f"{i}. {title} (ç›¸å…³åº¦: {similarity:.1%})\n"
                cases_section += f"   {content}\n\n"
        else:
            cases_section = "æœªæ‰¾åˆ°ç›´æ¥ç›¸å…³çš„æ¡ˆä¾‹ï¼Œè¯·åŸºäºæ³•ç†å’Œä¸€èˆ¬å¸æ³•å®è·µè¿›è¡Œåˆ†æã€‚\n\n"
        
        # ç»„è£…å®Œæ•´æç¤ºè¯
        full_prompt = base_prompt.format(
            query=query_text,
            articles_section=articles_section,
            cases_section=cases_section
        )
        
        return full_prompt
    
    def _generate_fallback_answer(self, query_text: str, articles: List[Dict], cases: List[Dict]) -> str:
        """
        ç”Ÿæˆå¤‡ç”¨å›ç­”ï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            query_text: ç”¨æˆ·æŸ¥è¯¢
            articles: ç›¸å…³æ³•æ¡
            cases: ç›¸å…³æ¡ˆä¾‹
            
        Returns:
            å¤‡ç”¨å›ç­”æ–‡æœ¬
        """
        fallback_answer = f"é’ˆå¯¹æ‚¨å’¨è¯¢çš„é—®é¢˜'{query_text}'ï¼ŒåŸºäºæ£€ç´¢åˆ°çš„æ³•å¾‹èµ„æºï¼Œç°æä¾›ä»¥ä¸‹åˆ†æï¼š\n\n"
        
        if articles:
            fallback_answer += f"ã€ç›¸å…³æ³•æ¡ã€‘\næ ¹æ®æ£€ç´¢ç»“æœï¼Œæ‰¾åˆ°{len(articles)}æ¡ç›¸å…³æ³•æ¡ï¼š\n"
            for i, article in enumerate(articles[:3], 1):
                title = article.get('title', f'ç¬¬{i}æ¡')
                fallback_answer += f"{i}. {title}\n"
            fallback_answer += "\n"
        
        if cases:
            fallback_answer += f"ã€ç›¸å…³æ¡ˆä¾‹ã€‘\nç³»ç»Ÿæ£€ç´¢åˆ°{len(cases)}ä¸ªç›¸å…³æ¡ˆä¾‹å¯ä¾›å‚è€ƒã€‚\n\n"
        
        fallback_answer += "ã€å»ºè®®ã€‘\nå»ºè®®æ‚¨ï¼š\n"
        fallback_answer += "1. è¯¦ç»†äº†è§£ä¸Šè¿°æ³•æ¡çš„å…·ä½“å†…å®¹\n"
        fallback_answer += "2. å‚è€ƒç›¸å…³æ¡ˆä¾‹çš„å¤„ç†æ–¹å¼\n"
        fallback_answer += "3. å¦‚éœ€å…·ä½“æ³•å¾‹æ„è§ï¼Œè¯·å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆ\n\n"
        fallback_answer += "*æœ¬å›ç­”åŸºäºAIæ£€ç´¢ç³»ç»Ÿç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæ­£å¼æ³•å¾‹æ„è§ã€‚"
        
        return fallback_answer
    
    def _calculate_final_confidence(self, path_scores: Dict, articles_count: int, cases_count: int) -> float:
        """
        è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        
        Args:
            path_scores: å„è·¯å¾„ç½®ä¿¡åº¦åˆ†æ•°
            articles_count: æ³•æ¡æ•°é‡
            cases_count: æ¡ˆä¾‹æ•°é‡
            
        Returns:
            æœ€ç»ˆç½®ä¿¡åº¦ (0-1)
        """
        try:
            # åŸºç¡€ç½®ä¿¡åº¦ï¼šå„è·¯å¾„å¹³å‡åˆ†
            if path_scores:
                base_confidence = sum(path_scores.values()) / len(path_scores)
            else:
                base_confidence = 0.3
            
            # ç»“æœæ•°é‡å¥–åŠ±
            result_bonus = 0.0
            if articles_count >= 3 and cases_count >= 3:
                result_bonus = 0.15  # æ³•æ¡å’Œæ¡ˆä¾‹éƒ½å……è¶³
            elif articles_count >= 2 or cases_count >= 2:
                result_bonus = 0.1   # å…¶ä¸­ä¸€ç±»ç»“æœå……è¶³
            elif articles_count >= 1 or cases_count >= 1:
                result_bonus = 0.05  # è‡³å°‘æœ‰ä¸€äº›ç»“æœ
            
            # å¤šè·¯å¾„ä¸€è‡´æ€§å¥–åŠ±
            consistency_bonus = 0.0
            if len(path_scores) >= 3:
                consistency_bonus = 0.1  # å¤šä¸ªæœç´¢è·¯å¾„éƒ½æœ‰ç»“æœ
            elif len(path_scores) >= 2:
                consistency_bonus = 0.05
            
            # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
            final_confidence = min(base_confidence + result_bonus + consistency_bonus, 0.95)
            final_confidence = max(final_confidence, 0.1)  # æœ€ä½ç½®ä¿¡åº¦ä¿æŠ¤
            
            return final_confidence
            
        except Exception as e:
            logger.warning(f"ç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦

    async def _general_ai_mode(self, query_text: str, pipeline_trace: SearchPipelineTrace) -> Dict:
        """é€šç”¨AIåŠ©æ‰‹æ¨¡å¼ï¼ˆéåˆ‘æ³•é—®é¢˜ï¼‰"""
        stage_start = time.time()

        try:
            # é€šç”¨AIå›ç­”ç”Ÿæˆ
            general_response = {
                'success': True,
                'query': query_text,
                'ai_response': f"è¿™æ˜¯ä¸€ä¸ªé€šç”¨é—®é¢˜ï¼š{query_text}ã€‚å»ºè®®å’¨è¯¢ç›¸å…³ä¸“ä¸šäººå£«è·å–å‡†ç¡®ä¿¡æ¯ã€‚",
                'mode': 'general_ai',
                'search_context': {
                    'duration_ms': (time.time() - stage_start) * 1000,
                    'specialized_search': False
                }
            }

            # è®°å½•é€šç”¨AI trace
            general_trace = create_module_trace(
                module_name="general_ai_assistant",
                stage="é€šç”¨AIå›ç­”",
                input_data={"query": query_text},
                output_data=general_response,
                processing_time_ms=(time.time() - stage_start) * 1000,
                confidence_score=0.6,
                debug_info={
                    "response_type": "general_assistance",
                    "specialized_search_skipped": True
                }
            )

            pipeline_trace.searches["general_ai"] = general_trace
            return general_response

        except Exception as e:
            logger.error(f"é€šç”¨AIæ¨¡å¼å¤±è´¥: {e}")
            return self._create_error_response(f"é€šç”¨AIå›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    async def search_documents(self, query_text: str, max_results: int = 10, 
                             document_types: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        æœç´¢æ³•å¾‹æ–‡æ¡£ - å®Œæ•´ä¸šåŠ¡æµç¨‹
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            max_results: æœ€å¤§ç»“æœæ•°
            document_types: æ–‡æ¡£ç±»å‹è¿‡æ»¤
            
        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œå…ƒä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        
        try:
            # 1. åˆ›å»ºæœç´¢æŸ¥è¯¢å€¼å¯¹è±¡
            search_query = SearchQuery(
                query_text=query_text,
                max_results=max_results,
                document_types=document_types
            )
            
            # 2. éªŒè¯æŸ¥è¯¢æœ‰æ•ˆæ€§
            if not search_query.is_valid():
                return self._create_error_response("æ— æ•ˆçš„æŸ¥è¯¢æ–‡æœ¬")
            
            # 3. æ‰§è¡Œæœç´¢
            results, context = await self.repository.search_documents(search_query)
            
            # 4. è½¬æ¢ä¸ºAPIå“åº”æ ¼å¼
            api_results = []
            for result in results:
                api_result = self._convert_domain_result_to_api(result)
                api_results.append(api_result)
            
            # 5. æ„å»ºå“åº”
            end_time = time.time()
            response = {
                'success': True,
                'results': api_results,
                'total': len(api_results),
                'query': query_text,
                'search_context': {
                    'duration_ms': round((end_time - start_time) * 1000, 2),
                    'total_documents_searched': context.total_documents_searched,
                    'performance_info': context.get_performance_info()
                }
            }
            
            return response
            
        except Exception as e:
            logger.error(f"Document search error: {str(e)}")
            return self._create_error_response(f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    async def search_documents_mixed(self, query_text: str, articles_count: int = 5, 
                                   cases_count: int = 5) -> Dict[str, Any]:
        """
        æ··åˆæœç´¢ - åˆ†åˆ«è¿”å›æ³•æ¡å’Œæ¡ˆä¾‹
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            articles_count: æ³•æ¡æ•°é‡
            cases_count: æ¡ˆä¾‹æ•°é‡
            
        Returns:
            åŒ…å«åˆ†ç±»ç»“æœçš„å­—å…¸
        """
        start_time = time.time()
        
        try:
            # 1. åˆ›å»ºæœç´¢æŸ¥è¯¢
            search_query = SearchQuery(
                query_text=query_text,
                max_results=articles_count + cases_count,
                document_types=None
            )
            
            # 2. éªŒè¯æŸ¥è¯¢
            if not search_query.is_valid():
                return self._create_error_response("æ— æ•ˆçš„æŸ¥è¯¢æ–‡æœ¬")
            
            # 3. æ‰§è¡Œæ··åˆæœç´¢
            mixed_results = await self.repository.search_documents_mixed(
                search_query, articles_count, cases_count
            )
            
            # 4. è½¬æ¢ç»“æœæ ¼å¼
            api_articles = []
            for result in mixed_results.get('articles', []):
                api_result = self._convert_domain_result_to_api(result)
                api_articles.append(api_result)
            
            api_cases = []
            for result in mixed_results.get('cases', []):
                api_result = self._convert_domain_result_to_api(result)
                api_cases.append(api_result)
            
            # 5. æ„å»ºå“åº”
            end_time = time.time()
            return {
                'success': True,
                'articles': api_articles,
                'cases': api_cases,
                'total_articles': len(api_articles),
                'total_cases': len(api_cases),
                'query': query_text,
                'search_context': {
                    'duration_ms': round((end_time - start_time) * 1000, 2),
                    'articles_requested': articles_count,
                    'cases_requested': cases_count
                }
            }
            
        except Exception as e:
            logger.error(f"Mixed search error: {str(e)}")
            return self._create_error_response(f"æ··åˆæœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    async def search_documents_hybrid(self, query_text: str, articles_count: int = 5,
                                    cases_count: int = 5) -> Dict[str, Any]:
        """
        å¢å¼ºç‰ˆæ··åˆæœç´¢ - ä½¿ç”¨BM25+è¯­ä¹‰æœç´¢èåˆ

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            articles_count: æ³•æ¡æ•°é‡
            cases_count: æ¡ˆä¾‹æ•°é‡

        Returns:
            åŒ…å«åˆ†ç±»ç»“æœçš„å­—å…¸
        """
        start_time = time.time()

        try:
            # 1. åˆ›å»ºæœç´¢æŸ¥è¯¢
            search_query = SearchQuery(
                query_text=query_text,
                max_results=articles_count + cases_count,
                document_types=None
            )

            # 2. éªŒè¯æŸ¥è¯¢
            if not search_query.is_valid():
                return self._create_error_response("æ— æ•ˆçš„æŸ¥è¯¢æ–‡æœ¬")

            # 3. ä½¿ç”¨æ··åˆæœç´¢å¼•æ“
            raw_results = []  # ğŸ”§ ä¿®å¤ï¼šåˆå§‹åŒ–raw_resultså˜é‡
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ··åˆæœç´¢åŠŸèƒ½
                if hasattr(self.repository.search_engine, 'hybrid_search'):
                    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨execute_mixed_searchç¡®ä¿è¿”å›æ­£ç¡®æ•°é‡çš„æ³•æ¡å’Œæ¡ˆä¾‹
                    search_coordinator = self.repository.search_engine._get_search_coordinator()
                    if search_coordinator and hasattr(search_coordinator, 'execute_mixed_search'):
                        logger.info(f"ä½¿ç”¨execute_mixed_searchï¼Œä¿è¯è¿”å›{articles_count}æ¡æ³•æ¡+{cases_count}ä¸ªæ¡ˆä¾‹")
                        mixed_results = search_coordinator.execute_mixed_search(
                            query_text, articles_count, cases_count, include_content=True
                        )
                        
                        if mixed_results.get('success'):
                            # ç›´æ¥ä½¿ç”¨åˆ†ç¦»å¥½çš„ç»“æœ
                            domain_articles = mixed_results.get('articles', [])
                            domain_cases = mixed_results.get('cases', [])
                        else:
                            logger.warning(f"execute_mixed_searchå¤±è´¥: {mixed_results.get('error')}")
                            # é™çº§åˆ°åŸå§‹hybrid_search
                            raw_results = self.repository.search_engine.hybrid_search(
                                query_text, (articles_count + cases_count) * 2
                            )
                            # åˆ†ç¦»æ³•æ¡å’Œæ¡ˆä¾‹ç»“æœ
                            articles_results = [r for r in raw_results if 'article' in r.get('id', '')][:articles_count]
                            cases_results = [r for r in raw_results if 'case' in r.get('id', '')][:cases_count]
                            
                            # è½¬æ¢ä¸ºé¢†åŸŸå¯¹è±¡
                            domain_articles = await self._convert_to_domain_objects(articles_results)
                            domain_cases = await self._convert_to_domain_objects(cases_results)
                    else:
                        logger.warning("æ— æ³•è·å–SearchCoordinatorï¼Œä½¿ç”¨åŸå§‹hybrid_search")
                        raw_results = self.repository.search_engine.hybrid_search(
                            query_text, (articles_count + cases_count) * 2
                        )
                        # åˆ†ç¦»æ³•æ¡å’Œæ¡ˆä¾‹ç»“æœ
                        articles_results = [r for r in raw_results if 'article' in r.get('id', '')][:articles_count]
                        cases_results = [r for r in raw_results if 'case' in r.get('id', '')][:cases_count]
                        
                        # è½¬æ¢ä¸ºé¢†åŸŸå¯¹è±¡
                        domain_articles = await self._convert_to_domain_objects(articles_results)
                        domain_cases = await self._convert_to_domain_objects(cases_results)
                        
                    logger.info(f"æ··åˆæœç´¢å®Œæˆ: {len(domain_articles)}æ¡æ³•æ¡, {len(domain_cases)}ä¸ªæ¡ˆä¾‹")
                else:
                    # é™çº§åˆ°åŸå§‹æœç´¢
                    logger.warning("æ··åˆæœç´¢åŠŸèƒ½ä¸å¯ç”¨ï¼Œé™çº§åˆ°åŸå§‹æœç´¢")
                    mixed_results = await self.repository.search_documents_mixed(
                        search_query, articles_count, cases_count
                    )
                    # è½¬æ¢ä¸ºé¢†åŸŸå¯¹è±¡
                    domain_articles = await self._convert_to_domain_objects(mixed_results.get('articles', []))
                    domain_cases = await self._convert_to_domain_objects(mixed_results.get('cases', []))

                # 6. æ„å»ºå“åº”
                end_time = time.time()
                return {
                    'success': True,
                    'articles': domain_articles,
                    'cases': domain_cases,
                    'total_articles': len(domain_articles),
                    'total_cases': len(domain_cases),
                    'query': query_text,
                    'search_context': {
                        'duration_ms': round((end_time - start_time) * 1000, 2),
                        'hybrid_search_used': hasattr(self.repository.search_engine, 'hybrid_search'),
                        'fusion_method': 'RRF',
                        'articles_requested': articles_count,
                        'cases_requested': cases_count,
                        'raw_results_count': len(raw_results) if raw_results else 0
                    }
                }

            except Exception as e:
                logger.warning(f"æ··åˆæœç´¢å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæœç´¢: {e}")
                return await self._fallback_search_mixed(query_text, articles_count, cases_count)

        except Exception as e:
            logger.error(f"Hybrid search error: {str(e)}")
            return self._create_error_response(f"æ··åˆæœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")

    async def _convert_to_domain_objects(self, raw_results: List[Dict]) -> List[Dict[str, Any]]:
        """
        å°†åŸå§‹æœç´¢ç»“æœè½¬æ¢ä¸ºAPIæ ¼å¼

        Args:
            raw_results: åŸå§‹æœç´¢ç»“æœ

        Returns:
            APIæ ¼å¼çš„ç»“æœåˆ—è¡¨
        """
        api_results = []

        for result in raw_results:
            try:
                # å¦‚æœå·²ç»æ˜¯é¢†åŸŸå¯¹è±¡æ ¼å¼ï¼Œç›´æ¥è½¬æ¢
                if hasattr(result, 'document') and hasattr(result, 'similarity_score'):
                    api_result = self._convert_domain_result_to_api(result)
                    api_results.append(api_result)
                    continue

                # è½¬æ¢ä¸ºAPIæ ¼å¼
                api_result = {
                    'id': result.get('id', ''),
                    'title': self._normalize_article_title(result.get('title', result.get('id', ''))),  # ç»Ÿä¸€æ ‡é¢˜æ ¼å¼
                    'content': result.get('content', ''),
                    'similarity': result.get('similarity', result.get('fusion_score', 0.0)),
                    'type': self._determine_document_type(result.get('id', '')),
                    'search_method': result.get('search_method', 'hybrid'),
                    'source': 'æ•°æ®é›†'  # ç»Ÿä¸€æ¥æºæ˜¾ç¤º
                }

                # æ·»åŠ ç±»å‹ç‰¹å®šå­—æ®µ
                doc_id = result.get('id', '')
                if 'case_' in doc_id:
                    api_result.update({
                        'case_id': result.get('case_id', doc_id),
                        'accusations': result.get('accusations', []),
                        'relevant_articles': result.get('relevant_articles', []),
                        'imprisonment_months': result.get('imprisonment_months', 0)
                    })
                elif 'article_' in doc_id:
                    api_result.update({
                        'article_number': result.get('article_number', ''),
                        'chapter': result.get('chapter', ''),
                        'law_name': result.get('law_name', 'åˆ‘æ³•')
                    })

                api_results.append(api_result)

            except Exception as e:
                logger.warning(f"è½¬æ¢ç»“æœå¤±è´¥: {e}, è·³è¿‡ç»“æœ: {result.get('id', 'unknown')}")
                continue

        return api_results

    async def _fallback_search_mixed(self, query_text: str, articles_count: int,
                                   cases_count: int) -> Dict[str, Any]:
        """
        é™çº§æœç´¢æ–¹æ³•

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            articles_count: æ³•æ¡æ•°é‡
            cases_count: æ¡ˆä¾‹æ•°é‡

        Returns:
            æœç´¢ç»“æœ
        """
        try:
            search_query = SearchQuery(
                query_text=query_text,
                max_results=articles_count + cases_count,
                document_types=None
            )

            mixed_results = await self.repository.search_documents_mixed(
                search_query, articles_count, cases_count
            )

            # è½¬æ¢ç»“æœæ ¼å¼
            api_articles = []
            for result in mixed_results.get('articles', []):
                api_result = self._convert_domain_result_to_api(result)
                api_articles.append(api_result)

            api_cases = []
            for result in mixed_results.get('cases', []):
                api_result = self._convert_domain_result_to_api(result)
                api_cases.append(api_result)

            return {
                'success': True,
                'articles': api_articles,
                'cases': api_cases,
                'total_articles': len(api_articles),
                'total_cases': len(api_cases),
                'query': query_text,
                'search_context': {
                    'duration_ms': 0,
                    'hybrid_search_used': False,
                    'fallback_reason': 'hybrid_search_unavailable'
                }
            }

        except Exception as e:
            logger.error(f"é™çº§æœç´¢ä¹Ÿå¤±è´¥: {e}")
            return self._create_error_response(f"æ‰€æœ‰æœç´¢æ–¹æ³•éƒ½å¤±è´¥: {str(e)}")

    def _determine_document_type(self, doc_id: str) -> str:
        """
        æ ¹æ®æ–‡æ¡£IDç¡®å®šæ–‡æ¡£ç±»å‹

        Args:
            doc_id: æ–‡æ¡£ID

        Returns:
            æ–‡æ¡£ç±»å‹å­—ç¬¦ä¸²
        """
        if 'case_' in doc_id:
            return 'case'
        elif 'article_' in doc_id:
            return 'article'
        else:
            return 'unknown'
    
    def _normalize_article_title(self, title: str) -> str:
        """
        ç»Ÿä¸€æ³•æ¡æ ‡é¢˜æ ¼å¼
        
        Args:
            title: åŸå§‹æ ‡é¢˜
            
        Returns:
            è§„èŒƒåŒ–åçš„æ ‡é¢˜
        """
        import re
        
        # æå–æ³•æ¡ç¼–å·
        article_match = re.search(r'ç¬¬?(â”€*[0-9]+)æ¡', title)
        if article_match:
            article_num = article_match.group(1).strip('â”€')  # å»é™¤å¯èƒ½çš„ç ´æŠ˜å·
            return f'ç¬¬{article_num}æ¡'
        
        # å¦‚æœåŒ…å«article_å‰ç¼€ï¼Œæå–ç¼–å·
        if 'article_' in title:
            try:
                article_num = title.split('article_')[1].split('_')[0]
                if article_num.isdigit():
                    return f'ç¬¬{article_num}æ¡'
            except:
                pass
        
        # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ ç¬¬ä¸€æ¡æ ¼å¼
        if title.isdigit():
            return f'ç¬¬{title}æ¡'
            
        # å…¶ä»–æƒ…å†µè¿”å›åŸæ ‡é¢˜
        return title
    
    async def search_documents_for_crime_consistency(self, query_text: str, cases_count: int = None) -> Dict[str, Any]:
        """
        ç½ªåä¸€è‡´æ€§ä¸“ç”¨æœç´¢ - 3ä¸ªæ³•æ¡ + 10ä¸ªæ¡ˆä¾‹
        
        Args:
            query_text: æœç´¢æŸ¥è¯¢æ–‡æœ¬
            cases_count: æ¡ˆä¾‹æ•°é‡ï¼Œé»˜è®¤å›ºå®š10ä¸ª
            
        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        # ç½ªåä¸€è‡´æ€§è¯„ä¼°çš„å›ºå®šé…ç½®
        articles_count = 3  # å›ºå®šè¿”å›3ä¸ªæ³•æ¡
        if cases_count is None:
            cases_count = 10  # å›ºå®šè¿”å›10ä¸ªæ¡ˆä¾‹
        
        logger.info(f"ç½ªåä¸€è‡´æ€§æœç´¢: {query_text} - è¯·æ±‚{articles_count}æ¡æ³•æ¡, {cases_count}ä¸ªæ¡ˆä¾‹")
        
        start_time = time.time()
        
        try:
            # 1. åˆ›å»ºæœç´¢æŸ¥è¯¢
            search_query = SearchQuery(
                query_text=query_text,
                max_results=articles_count + cases_count,
                document_types=None
            )
            
            # 2. éªŒè¯æŸ¥è¯¢
            if not search_query.is_valid():
                return self._create_error_response("æ— æ•ˆçš„æŸ¥è¯¢æ–‡æœ¬")
            
            # 3. æ‰§è¡Œæ··åˆæœç´¢
            mixed_results = await self.repository.search_documents_mixed(
                search_query, articles_count, cases_count
            )
            
            # 4. è½¬æ¢ç»“æœæ ¼å¼
            api_articles = []
            for result in mixed_results.get('articles', []):
                api_result = self._convert_domain_result_to_api(result)
                api_articles.append(api_result)
            
            api_cases = []
            for result in mixed_results.get('cases', []):
                api_result = self._convert_domain_result_to_api(result)
                api_cases.append(api_result)
            
            # 5. æ„å»ºå“åº”
            end_time = time.time()
            return {
                'success': True,
                'articles': api_articles,
                'cases': api_cases,
                'total_articles': len(api_articles),
                'total_cases': len(api_cases),
                'query': query_text,
                'search_type': 'crime_consistency',
                'search_context': {
                    'duration_ms': round((end_time - start_time) * 1000, 2),
                    'articles_requested': articles_count,
                    'cases_requested': cases_count,
                    'actual_cases_returned': len(api_cases)
                }
            }
            
        except Exception as e:
            logger.error(f"Crime consistency search error: {str(e)}")
            return self._create_error_response(f"ç½ªåä¸€è‡´æ€§æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    async def load_more_cases(self, query_text: str, offset: int = 0, 
                            limit: int = 5) -> Dict[str, Any]:
        """
        åˆ†é¡µåŠ è½½æ›´å¤šæ¡ˆä¾‹
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            offset: åç§»é‡
            limit: è¿”å›æ•°é‡
            
        Returns:
            åˆ†é¡µæ¡ˆä¾‹ç»“æœ
        """
        start_time = time.time()
        
        try:
            # 1. åˆ›å»ºåˆ†é¡µæŸ¥è¯¢
            search_query = SearchQuery(
                query_text=query_text,
                max_results=limit,
                document_types=['cases']
            )
            
            if not search_query.is_valid():
                return self._create_error_response("æ— æ•ˆçš„æŸ¥è¯¢æ–‡æœ¬")
            
            # 2. æ‰§è¡Œåˆ†é¡µæœç´¢
            paginated_results = await self.repository.load_more_cases(
                search_query, offset, limit
            )
            
            # 3. è½¬æ¢ç»“æœæ ¼å¼
            api_cases = []
            for result in paginated_results.get('cases', []):
                api_result = self._convert_domain_result_to_api(result)
                api_cases.append(api_result)
            
            # 4. æ„å»ºå“åº”
            end_time = time.time()
            return {
                'success': True,
                'cases': api_cases,
                'offset': offset,
                'limit': limit,
                'returned_count': len(api_cases),
                'has_more': paginated_results.get('has_more', False),
                'query': query_text,
                'search_context': {
                    'duration_ms': round((end_time - start_time) * 1000, 2)
                }
            }
            
        except Exception as e:
            logger.error(f"Load more cases error: {str(e)}")
            return self._create_error_response(f"åŠ è½½æ›´å¤šæ¡ˆä¾‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    async def get_document_by_id(self, document_id: str) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®IDè·å–å•ä¸ªæ–‡æ¡£
        
        Args:
            document_id: æ–‡æ¡£ID
            
        Returns:
            æ–‡æ¡£ä¿¡æ¯å­—å…¸æˆ–None
        """
        try:
            document = await self.repository.get_document_by_id(document_id)
            if document is None:
                return None
            
            return self._convert_domain_document_to_api(document)
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£å¤±è´¥ (ID: {document_id}): {str(e)}")
            return None
    
    def get_system_status(self) -> Dict[str, Any]:
        """
        è·å–ç³»ç»ŸçŠ¶æ€
        
        Returns:
            ç³»ç»ŸçŠ¶æ€ä¿¡æ¯
        """
        try:
            is_ready = self.repository.is_ready()
            document_counts = self.repository.get_total_document_count()
            
            return {
                'status': 'ok' if is_ready else 'not_ready',
                'ready': is_ready,
                'total_documents': document_counts.get('total', 0),
                'document_breakdown': document_counts
            }
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                'status': 'error',
                'ready': False,
                'total_documents': 0,
                'error': str(e)
            }
    
    def _convert_domain_result_to_api(self, domain_result: SearchResult) -> Dict[str, Any]:
        """å°†é¢†åŸŸæœç´¢ç»“æœè½¬æ¢ä¸ºAPIæ ¼å¼"""
        document = domain_result.document
        
        base_result = {
            'id': document.id,
            'title': document.get_display_title(),
            'content': document.content,
            'similarity': domain_result.similarity_score,
            'type': document.document_type,
            'confidence_level': domain_result.confidence_level,
            'rank': domain_result.rank
        }
        
        # æ·»åŠ ç±»å‹ç‰¹å®šå­—æ®µ
        if isinstance(document, Case):
            base_result.update({
                'case_id': document.case_id,
                'criminals': document.criminals,
                'accusations': document.accusations,
                'relevant_articles': document.relevant_articles,
                'punish_of_money': document.punish_of_money,
                'death_penalty': document.death_penalty,
                'life_imprisonment': document.life_imprisonment,
                'imprisonment_months': document.imprisonment_months,
                'is_severe_case': document.has_severe_punishment()
            })
        elif isinstance(document, Article):
            base_result.update({
                'article_number': document.article_number,
                'chapter': document.chapter,
                'law_name': document.law_name
            })
        
        return base_result
    
    def _convert_domain_document_to_api(self, document: LegalDocument) -> Dict[str, Any]:
        """å°†é¢†åŸŸæ–‡æ¡£å®ä½“è½¬æ¢ä¸ºAPIæ ¼å¼"""
        base_doc = {
            'id': document.id,
            'title': document.get_display_title(),
            'content': document.content,
            'type': document.document_type,
            'searchable_text': document.get_searchable_text()
        }
        
        # æ·»åŠ ç±»å‹ç‰¹å®šå­—æ®µ
        if isinstance(document, Case):
            base_doc.update({
                'case_id': document.case_id,
                'criminals': document.criminals,
                'accusations': document.accusations,
                'relevant_articles': document.relevant_articles,
                'punish_of_money': document.punish_of_money,
                'death_penalty': document.death_penalty,
                'life_imprisonment': document.life_imprisonment,
                'imprisonment_months': document.imprisonment_months
            })
        elif isinstance(document, Article):
            base_doc.update({
                'article_number': document.article_number,
                'chapter': document.chapter,
                'law_name': document.law_name
            })
        
        return base_doc

    # ==================== LLMå¢å¼ºæœç´¢æ–¹æ³• ====================

    async def search_documents_enhanced(self, query_text: str, articles_count: int = 5,
                                      cases_count: int = 5) -> Dict[str, Any]:
        """
        LLMå¢å¼ºç‰ˆæœç´¢ - ä½¿ç”¨ä¸‰è·¯å¬å›èåˆ

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            articles_count: æ³•æ¡æ•°é‡
            cases_count: æ¡ˆä¾‹æ•°é‡

        Returns:
            åŒ…å«åˆ†ç±»ç»“æœçš„å­—å…¸
        """
        start_time = time.time()

        try:
            # 1. åˆ›å»ºæœç´¢æŸ¥è¯¢
            search_query = SearchQuery(
                query_text=query_text,
                max_results=articles_count + cases_count,
                document_types=None
            )

            # 2. éªŒè¯æŸ¥è¯¢
            if not search_query.is_valid():
                return self._create_error_response("æ— æ•ˆçš„æŸ¥è¯¢æ–‡æœ¬")

            # 3. å°è¯•ä½¿ç”¨ä¸‰è·¯å¬å›å¼•æ“
            try:
                if hasattr(self.repository, 'multi_retrieval_engine') and self.repository.multi_retrieval_engine:
                    logger.info("ä½¿ç”¨ä¸‰è·¯å¬å›å¢å¼ºæœç´¢")
                    raw_results = await self.repository.multi_retrieval_engine.three_way_retrieval(
                        query_text,
                        top_k=(articles_count + cases_count) * 2
                    )

                    # åˆ†ç¦»å’Œè½¬æ¢ç»“æœ
                    articles_results = [r for r in raw_results if 'article' in r.get('id', '')][:articles_count]
                    cases_results = [r for r in raw_results if 'case' in r.get('id', '')][:cases_count]

                    domain_articles = await self._convert_to_domain_objects_enhanced(articles_results, 'article')
                    domain_cases = await self._convert_to_domain_objects_enhanced(cases_results, 'case')

                    end_time = time.time()
                    return {
                        'success': True,
                        'articles': domain_articles,
                        'cases': domain_cases,
                        'total_articles': len(domain_articles),
                        'total_cases': len(domain_cases),
                        'query': query_text,
                        'search_context': {
                            'duration_ms': round((end_time - start_time) * 1000, 2),
                            'llm_enhanced': True,
                            'retrieval_paths': ['hybrid', 'query2doc', 'hyde'],
                            'fusion_method': 'weighted_multi_path'
                        }
                    }
                else:
                    logger.warning("ä¸‰è·¯å¬å›å¼•æ“ä¸å¯ç”¨ï¼Œé™çº§åˆ°æ··åˆæœç´¢")
                    return await self._progressive_fallback_search(query_text, articles_count, cases_count)

            except Exception as e:
                logger.error(f"LLMå¢å¼ºæœç´¢å¤±è´¥: {e}")
                return await self._progressive_fallback_search(query_text, articles_count, cases_count)

        except Exception as e:
            logger.error(f"å¢å¼ºæœç´¢æœåŠ¡é”™è¯¯: {e}")
            return self._create_error_response(f"å¢å¼ºæœç´¢å¤±è´¥: {str(e)}")

    async def _progressive_fallback_search(self, query_text: str, articles_count: int,
                                         cases_count: int) -> Dict[str, Any]:
        """
        æ¸è¿›å¼é™çº§æœç´¢ç­–ç•¥

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            articles_count: æ³•æ¡æ•°é‡
            cases_count: æ¡ˆä¾‹æ•°é‡

        Returns:
            æœç´¢ç»“æœ
        """
        try:
            # å°è¯•æ··åˆæœç´¢
            logger.info("å°è¯•æ··åˆæœç´¢ä½œä¸ºé™çº§æ–¹æ¡ˆ")
            return await self.search_documents_hybrid(query_text, articles_count, cases_count)

        except Exception as e:
            logger.warning(f"æ··åˆæœç´¢ä¹Ÿå¤±è´¥: {e}")

            try:
                # æœ€åé™çº§åˆ°åŸå§‹æœç´¢
                logger.info("æœ€åé™çº§åˆ°åŸå§‹æœç´¢")
                return await self.search_documents_mixed(query_text, articles_count, cases_count)

            except Exception as e2:
                logger.error(f"æ‰€æœ‰æœç´¢æ–¹æ³•éƒ½å¤±è´¥: {e2}")
                return self._create_error_response(f"æ‰€æœ‰æœç´¢æ–¹æ³•éƒ½å¤±è´¥: {str(e2)}")

    async def _convert_to_domain_objects_enhanced(self, results: List[Dict], doc_type: str) -> List[Dict[str, Any]]:
        """
        å°†æœç´¢ç»“æœè½¬æ¢ä¸ºé¢†åŸŸå¯¹è±¡æ ¼å¼

        Args:
            results: åŸå§‹æœç´¢ç»“æœ
            doc_type: æ–‡æ¡£ç±»å‹

        Returns:
            è½¬æ¢åçš„é¢†åŸŸå¯¹è±¡åˆ—è¡¨
        """
        converted_results = []

        for result in results:
            try:
                # è·å–å®Œæ•´æ–‡æ¡£å†…å®¹
                doc_id = result.get('id')
                if not doc_id:
                    continue

                full_document = await self.repository.get_document_by_id(doc_id)
                if full_document:
                    api_result = self._convert_domain_document_to_api(full_document)

                    # æ·»åŠ æœç´¢ç›¸å…³çš„å…ƒä¿¡æ¯
                    api_result.update({
                        'similarity': result.get('similarity', 0),
                        'fusion_score': result.get('fusion_score', 0),
                        'confidence': result.get('confidence', 0),
                        'sources': result.get('sources', []),
                        'search_meta': result.get('search_meta', {})
                    })

                    converted_results.append(api_result)

            except Exception as e:
                logger.warning(f"è½¬æ¢æ–‡æ¡£ {result.get('id')} å¤±è´¥: {e}")
                continue

        return converted_results

    def get_llm_enhancement_status(self) -> Dict[str, Any]:
        """
        è·å–LLMå¢å¼ºåŠŸèƒ½çŠ¶æ€

        Returns:
            LLMå¢å¼ºåŠŸèƒ½çŠ¶æ€ä¿¡æ¯
        """
        try:
            has_multi_retrieval = (hasattr(self.repository, 'multi_retrieval_engine') and
                                 self.repository.multi_retrieval_engine is not None)

            if has_multi_retrieval:
                stats = self.repository.multi_retrieval_engine.get_retrieval_stats()
            else:
                stats = None

            return {
                'llm_enhancement_available': has_multi_retrieval,
                'multi_retrieval_engine_status': stats,
                'fallback_available': True,
                'supported_methods': ['three_way_fusion', 'hybrid', 'mixed'] if has_multi_retrieval else ['hybrid', 'mixed']
            }

        except Exception as e:
            logger.error(f"è·å–LLMå¢å¼ºçŠ¶æ€å¤±è´¥: {e}")
            return {
                'llm_enhancement_available': False,
                'error': str(e),
                'fallback_available': True,
                'supported_methods': ['hybrid', 'mixed']
            }

    # ==================== çŸ¥è¯†å›¾è°±å¢å¼ºæœç´¢æ–¹æ³• ====================

    async def search_documents_kg_enhanced(self, query_text: str, articles_count: int = 5,
                                         cases_count: int = 5) -> Dict[str, Any]:
        """
        çŸ¥è¯†å›¾è°±å¢å¼ºç‰ˆæœç´¢ - æœ€é«˜çº§çš„æœç´¢åŠŸèƒ½

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            articles_count: æ³•æ¡æ•°é‡
            cases_count: æ¡ˆä¾‹æ•°é‡

        Returns:
            åŒ…å«åˆ†ç±»ç»“æœçš„å­—å…¸
        """
        start_time = time.time()

        try:
            # 1. åˆ›å»ºæœç´¢æŸ¥è¯¢
            search_query = SearchQuery(
                query_text=query_text,
                max_results=articles_count + cases_count,
                document_types=None
            )

            # 2. éªŒè¯æŸ¥è¯¢
            if not search_query.is_valid():
                return self._create_error_response("æ— æ•ˆçš„æŸ¥è¯¢æ–‡æœ¬")

            # 3. å°è¯•ä½¿ç”¨çŸ¥è¯†å›¾è°±å¢å¼ºæœç´¢
            try:
                if hasattr(self.repository, 'kg_enhanced_engine') and self.repository.kg_enhanced_engine:
                    logger.info("ä½¿ç”¨çŸ¥è¯†å›¾è°±å¢å¼ºæœç´¢")
                    raw_results = await self.repository.kg_enhanced_engine.knowledge_enhanced_search(
                        query_text,
                        top_k=(articles_count + cases_count) * 2
                    )

                    # åˆ†ç¦»å’Œè½¬æ¢ç»“æœ
                    articles_results = [r for r in raw_results if 'article' in r.get('id', '')][:articles_count]
                    cases_results = [r for r in raw_results if 'case' in r.get('id', '')][:cases_count]

                    domain_articles = await self._convert_to_domain_objects_enhanced(articles_results, 'article')
                    domain_cases = await self._convert_to_domain_objects_enhanced(cases_results, 'case')

                    end_time = time.time()
                    return {
                        'success': True,
                        'articles': domain_articles,
                        'cases': domain_cases,
                        'total_articles': len(domain_articles),
                        'total_cases': len(domain_cases),
                        'query': query_text,
                        'search_context': {
                            'duration_ms': round((end_time - start_time) * 1000, 2),
                            'kg_enhanced': True,
                            'knowledge_expansion': raw_results[0].get('knowledge_expansion', {}) if raw_results else {},
                            'fusion_method': 'knowledge_graph_enhanced'
                        }
                    }

                else:
                    logger.info("çŸ¥è¯†å›¾è°±å¢å¼ºå¼•æ“ä¸å¯ç”¨ï¼Œé™çº§åˆ°LLMå¢å¼ºæœç´¢")
                    return await self.search_documents_enhanced(query_text, articles_count, cases_count)

            except Exception as e:
                logger.error(f"çŸ¥è¯†å›¾è°±å¢å¼ºæœç´¢å¤±è´¥: {e}")
                # é™çº§åˆ°LLMå¢å¼ºæœç´¢
                return await self.search_documents_enhanced(query_text, articles_count, cases_count)

        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±å¢å¼ºæœç´¢å®Œå…¨å¤±è´¥: {e}")
            return await self._fallback_search_mixed(query_text, articles_count, cases_count)

    async def explain_search_reasoning(self, query_text: str) -> Dict[str, Any]:
        """
        è§£é‡Šæœç´¢æ¨ç†è¿‡ç¨‹

        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬

        Returns:
            æœç´¢æ¨ç†è§£é‡Š
        """
        try:
            if hasattr(self.repository, 'kg_enhanced_engine') and self.repository.kg_enhanced_engine:
                explanation = await self.repository.kg_enhanced_engine.explain_search_reasoning(query_text)
                return {
                    'success': True,
                    'explanation': explanation
                }
            else:
                return {
                    'success': False,
                    'error': 'çŸ¥è¯†å›¾è°±å¢å¼ºå¼•æ“ä¸å¯ç”¨',
                    'fallback': 'ä½¿ç”¨æ ‡å‡†æœç´¢æ¨ç†'
                }

        except Exception as e:
            logger.error(f"æœç´¢æ¨ç†è§£é‡Šå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def get_kg_enhanced_status(self) -> Dict[str, Any]:
        """
        è·å–çŸ¥è¯†å›¾è°±å¢å¼ºæœç´¢çŠ¶æ€

        Returns:
            çŠ¶æ€ä¿¡æ¯
        """
        try:
            has_kg_engine = (hasattr(self.repository, 'kg_enhanced_engine') and
                           self.repository.kg_enhanced_engine is not None)

            if has_kg_engine:
                stats = self.repository.kg_enhanced_engine.get_search_statistics()
            else:
                stats = None

            knowledge_graph = None
            if hasattr(self.repository, 'data_loader'):
                knowledge_graph = self.repository.data_loader.get_knowledge_graph()

            kg_stats = None
            if knowledge_graph:
                kg_stats = knowledge_graph.get_graph_statistics()

            return {
                'kg_enhancement_available': has_kg_engine,
                'kg_enhanced_engine_status': stats,
                'knowledge_graph_stats': kg_stats,
                'fallback_available': True,
                'supported_methods': ['kg_enhanced', 'three_way_fusion', 'hybrid', 'mixed'] if has_kg_engine else ['hybrid', 'mixed']
            }

        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†å›¾è°±å¢å¼ºçŠ¶æ€å¤±è´¥: {e}")
            return {
                'kg_enhancement_available': False,
                'error': str(e),
                'fallback_available': True,
                'supported_methods': ['hybrid', 'mixed']
            }

    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            'success': False,
            'results': [],
            'total': 0,
            'query': '',
            'error': error_message
        }

